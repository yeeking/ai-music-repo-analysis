,Unnamed: 0.5,Unnamed: 0.4,Unnamed: 0.3,Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,title,url,paper_id,citations,paper_url,git_urls,Repository,Stars,Forks,Downloads,Watchers,Open Issues,Network Count,Commit Count,Age (days),Languages,Language percs,Code total,Main language,Main language lines,Type
0,0,0.0,0.0,0.0,0.0,0.0,Generative music with stochastic diffusion search,https://scholar.google.com/scholar?q=Generative%20music%20with%20stochastic%20diffusion%20search,0,9.0,http://gala.gre.ac.uk/id/eprint/21009/1/21009_AL%20RIFAIE_Generative_music_with_stochastic_diffusion_search.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
1,1,1.0,1.0,1.0,1.0,1.0,Gene expression synthesis,https://scholar.google.com/scholar?q=Gene%20expression%20synthesis,1,100.0,https://pubs.rsc.org/en/content/getauthorversionpdf/c5mb00310e,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
2,2,2.0,2.0,2.0,2.0,2.0,Real-time musical interaction between musician and multi-agent system,https://scholar.google.com/scholar?q=Real-time%20musical%20interaction%20between%20musician%20and%20multi-agent%20system,2,3.0,https://cir.nii.ac.jp/crid/1570572700569443584,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
3,3,3.0,3.0,3.0,3.0,3.0,Deep machine learning – a new frontier in artificial intelligence research,https://scholar.google.com/scholar?q=Deep%20machine%20learning%20%E2%80%93%20a%20new%20frontier%20in%20artificial%20intelligence%20research,3,1642.0,https://www.researchgate.net/profile/Thomas-Karnowski/publication/220438386_Deep_Machine_Learning_-_A_New_Frontier_in_Artificial_Intelligence_Research_Research_Frontier/links/570bb11b08ae8883a1ffd3e1/Deep-Machine-Learning-A-New-Frontier-in-Artificial-Intelligence-Research-Research-Frontier.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
4,4,4.0,4.0,4.0,4.0,4.0,Evaluation of musical creativity and musical metacreation systems,https://scholar.google.com/scholar?q=Evaluation%20of%20musical%20creativity%20and%20musical%20metacreation%20systems,4,68.0,https://dl.acm.org/doi/pdf/10.1145/2967506?casa_token=d-NU3YTadPgAAAAA:BWQ_nHPpVw9OX1X2Npvh5NIo2wdEQxCpGUR2nLWi_p_8TG8rl80qU8dR3ugVnzI0Cw7CMJZA_fAb,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
5,5,5.0,5.0,5.0,5.0,5.0,Automatic construction of interactive machine improvisation scenarios from audio recordings,https://scholar.google.com/scholar?q=Automatic%20construction%20of%20interactive%20machine%20improvisation%20scenarios%20from%20audio%20recordings,5,5.0,https://hal.science/hal-01336825/document,https://github.com/fpom/snakesibisc,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
6,6,6.0,6.0,6.0,6.0,6.0,Omax brothers: A dynamic topology of agents for improvization learning,https://scholar.google.com/scholar?q=Omax%20brothers%3A%20A%20dynamic%20topology%20of%20agents%20for%20improvization%20learning,6,197.0,https://dl.acm.org/doi/pdf/10.1145/1178723.1178742?casa_token=1X6j-EJHq5UAAAAA:ymsck-WDuf2-vCH_ne4aXGga-rUspL37GMA9pImcscHGQ2uQaJP6FJ5GDWjSHbfg5VbRVYAQf2L0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
7,7,7.0,7.0,7.0,7.0,7.0,Guessing the composer’s mind: Applying universal prediction to musical style,https://scholar.google.com/scholar?q=Guessing%20the%20composer%E2%80%99s%20mind%3A%20Applying%20universal%20prediction%20to%20musical%20style,7,107.0,http://dub.ucsd.edu/Papers/GuessingICMC99.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
8,8,8.0,8.0,8.0,8.0,8.0,Artificial evolution of tuning systems,https://scholar.google.com/scholar?q=Artificial%20evolution%20of%20tuning%20systems,8,51.0,https://www.sciencedirect.com/science/article/pii/S0965997816300515?casa_token=WgOShEm10_QAAAAA:_Ho2TOcWqFkNtErBDKWO8Qclnn12Cp4asGuJGAGWx7DXJfpBow0UYq3dhtnN3t9LAFM__Tsm9Q,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
9,9,9.0,9.0,9.0,9.0,9.0,Ringomatic: A realtime interactive drummer using constraint-satisfaction and drum sound descriptors,https://scholar.google.com/scholar?q=Ringomatic%3A%20A%20realtime%20interactive%20drummer%20using%20constraint-satisfaction%20and%20drum%20sound%20descriptors,9,20.0,https://www.academia.edu/download/77079872/1057.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
10,10,10.0,10.0,10.0,10.0,10.0,Continuous realtime gesture following and recognition,https://scholar.google.com/scholar?q=Continuous%20realtime%20gesture%20following%20and%20recognition,10,279.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=7be03a3a6771da0ac898fc6c76e6ba0be6a2f82a,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
11,11,11.0,11.0,11.0,11.0,11.0,On-line development of man-machine relationships: Motivation-driven musical interaction,https://scholar.google.com/scholar?q=On-line%20development%20of%20man-machine%20relationships%3A%20Motivation-driven%20musical%20interaction,11,3.0,http://www.generativeart.com/on/cic/papersGA2008/1.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
12,12,12.0,12.0,12.0,12.0,12.0,Structural coupling in a society of musical agents,https://scholar.google.com/scholar?q=Structural%20coupling%20in%20a%20society%20of%20musical%20agents,12,34.0,https://www.researchgate.net/profile/Peter-Beyls-2/publication/228846032_Interaction_and_Self-organisation_in_a_Society_of_Musical_Agents/links/55940b7c08ae5af2b0ecfb7e/Interaction-and-Self-organisation-in-a-Society-of-Musical-Agents.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
13,13,13.0,13.0,13.0,13.0,13.0,"Autonomy, influence and emergence in an audiovisual ecosystem",https://scholar.google.com/scholar?q=Autonomy%2C%20influence%20and%20emergence%20in%20an%20audiovisual%20ecosystem,13,6.0,https://metacreation.net/ktatar/musical_agents/Beyls_2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
14,14,14.0,14.0,14.0,14.0,14.0,GenJam: A genetic algorithm for generating jazz solos,https://scholar.google.com/scholar?q=GenJam%3A%20A%20genetic%20algorithm%20for%20generating%20jazz%20solos,14,850.0,https://www.researchgate.net/profile/John-Biles-2/publication/2342018_GenJam_A_Genetic_Algorithm_for_Generating_Jazz_Solos/links/54debcb80cf2510fcee4b529/GenJam-A-Genetic-Algorithm-for-Generating-Jazz-Solos.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
15,15,15.0,15.0,15.0,15.0,15.0,Performing with technology: Lessons learned from the GenJam project,https://scholar.google.com/scholar?q=Performing%20with%20technology%3A%20Lessons%20learned%20from%20the%20GenJam%20project,15,11.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12642/12490,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
16,16,16.0,16.0,16.0,16.0,16.0,"Generating affect: Applying valence and arousal values to unified video, music, and sound generation system",https://scholar.google.com/scholar?q=Generating%20affect%3A%20Applying%20valence%20and%20arousal%20values%20to%20unified%20video%2C%20music%2C%20and%20sound%20generation%20system,16,7.0,https://www.researchgate.net/profile/Arne-Eigenfeldt/publication/295772363_Applying_Valence_and_Arousal_Values_to_a_Unified_Video_Music_and_Sound_Generative_Multimedia_Work/links/56cd53c908aeb52500c0a51f/Applying-Valence-and-Arousal-Values-to-a-Unified-Video-Music-and-Sound-Generative-Multimedia-Work.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
17,17,17.0,17.0,17.0,17.0,17.0,Live algorithms: Towards autonomous computer improvisers,https://scholar.google.com/scholar?q=Live%20algorithms%3A%20Towards%20autonomous%20computer%20improvisers,17,44.0,https://metacreation.net/ktatar/musical_agents/Blackwell_2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
18,18,18.0,18.0,18.0,18.0,18.0,"Self-organised music Organised Sound, 9(02), 123–136 Bloch, G., Dubnov, S., & Assayag, G [DATE]",https://scholar.google.com/scholar?q=Self-organised%20music%20Organised%20Sound%2C%209%2802%29%2C%20123%E2%80%93136%20Bloch%2C%20G.%2C%20Dubnov%2C%20S.%2C%20%26%20Assayag%2C%20G%20%5BDATE%5D,18,26.0,https://www.tandfonline.com/doi/pdf/10.1080/09298215.2013.860465?casa_token=1xhNUMK5yc4AAAAA:Xorr8npEW3gj5FzbDXYNMF8qHQPKqd_jkmH8wgPdrLX0GLNq_ipMPVExFoItsV0FWhuJUcOYBC-w,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
19,19,19.0,19.0,19.0,19.0,19.0,"Musical metacreation: Past, present, and future",https://scholar.google.com/scholar?q=Musical%20metacreation%3A%20Past%2C%20present%2C%20and%20future,19,9.0,https://musicalmetacreation.org/mume2018/proceedings/Bodily-2.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
20,20,20.0,20.0,20.0,20.0,20.0,Manifesto for a musebot ensemble: A platform for live interactive performance between multiple autonomous musical agents,https://scholar.google.com/scholar?q=Manifesto%20for%20a%20musebot%20ensemble%3A%20A%20platform%20for%20live%20interactive%20performance%20between%20multiple%20autonomous%20musical%20agents,20,29.0,https://www.researchgate.net/profile/Arne-Eigenfeldt/publication/281441670_Manifesto_for_a_Musebot_Ensemble_A_platform_for_live_interactive_performance_between_multiple_autonomous_musical_agents/links/55e71ee708ae65b638994fbf/Manifesto-for-a-Musebot-Ensemble-A-platform-for-live-interactive-performance-between-multiple-autonomous-musical-agents.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
21,21,21.0,21.0,21.0,21.0,21.0,Backgammon: Process-based musical explorations using the agent designer,https://scholar.google.com/scholar?q=Backgammon%3A%20Process-based%20musical%20explorations%20using%20the%20agent%20designer,21,1.0,https://dl.acm.org/doi/pdf/10.1145/2466627.2481236?casa_token=Bkwg9s8lrVMAAAAA:qcxYAn59TwKEQiJY2iKr7QFjXn8ojjEfkJ5kVQHaGRGe7UC-93JVyl1PNNNeTyZJEwdFG1hqecAY,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
22,22,22.0,22.0,22.0,22.0,22.0,Ecosystemic methods for creative domains: Niche construction and boundary formation,https://scholar.google.com/scholar?q=Ecosystemic%20methods%20for%20creative%20domains%3A%20Niche%20construction%20and%20boundary%20formation,22,7.0,https://www.csse.monash.edu/~jonmc/research/Papers/bown_mccormack_kowaliw_ieee-alife11.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
23,23,23.0,23.0,23.0,23.0,23.0,Linear and non-linear composition systems: User experience in nodal and pro tools,https://scholar.google.com/scholar?q=Linear%20and%20non-linear%20composition%20systems%3A%20User%20experience%20in%20nodal%20and%20pro%20tools,23,3.0,https://www.researchgate.net/profile/Liam-Bray-2/publication/325412037_Linear_and_Non-linear_Composition_Systems_User_Experience_in_Nodal_and_Pro_Tools/links/5b0caf980f7e9b1ed7fbc013/Linear-and-Non-linear-Composition-Systems-User-Experience-in-Nodal-and-Pro-Tools.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
24,24,24.0,24.0,24.0,24.0,24.0,Applying core interaction design principles to computational creativity,https://scholar.google.com/scholar?q=Applying%20core%20interaction%20design%20principles%20to%20computational%20creativity,24,11.0,https://www.computationalcreativity.net/iccc2016/wp-content/uploads/2016/01/Applying-Core-Interaction-Design-Principles-to-Computational-Creativity.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
25,25,25.0,25.0,25.0,25.0,25.0,"A robust layered control system for a mobile robot IEEE Journal of Robotics and Automation, 2(1), 14–23 Brooks, R A [DATE]",https://scholar.google.com/scholar?q=A%20robust%20layered%20control%20system%20for%20a%20mobile%20robot%20IEEE%20Journal%20of%20Robotics%20and%20Automation%2C%202%281%29%2C%2014%E2%80%9323%20Brooks%2C%20R%20A%20%5BDATE%5D,25,13064.0,https://dspace.mit.edu/bitstream/handle/1721.1/6432/AIM-864.pdf?sequence=2&origin=publication_detail,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
26,26,26.0,26.0,26.0,26.0,26.0,The reactive accompanist: Adaptation and behavior decomposition in a music system,https://scholar.google.com/scholar?q=The%20reactive%20accompanist%3A%20Adaptation%20and%20behavior%20decomposition%20in%20a%20music%20system,26,21.0,https://metacreation.net/ktatar/musical_agents/Bryson_1995.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
27,27,27.0,27.0,27.0,27.0,27.0,Magenta: An architecture for real time automatic composition of background music,https://scholar.google.com/scholar?q=Magenta%3A%20An%20architecture%20for%20real%20time%20automatic%20composition%20of%20background%20music,27,65.0,https://metacreation.net/ktatar/musical_agents/Casella_2001.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
28,28,28.0,28.0,28.0,28.0,28.0,Drumtrack: Beat induction from an acoustic drum kit with synchronised scheduling,https://scholar.google.com/scholar?q=Drumtrack%3A%20Beat%20induction%20from%20an%20acoustic%20drum%20kit%20with%20synchronised%20scheduling,28,15.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8152e210b5e0370d889d12e77483a90167ad967e,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
29,29,29.0,29.0,29.0,29.0,29.0,Reinforcement learning for live musical agents,https://scholar.google.com/scholar?q=Reinforcement%20learning%20for%20live%20musical%20agents,29,38.0,http://composerprogrammer.com/research/rlforlivemusicalagents.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
30,30,30.0,30.0,30.0,30.0,30.0,Creativity versus the perception of creativity in computational systems,https://scholar.google.com/scholar?q=Creativity%20versus%20the%20perception%20of%20creativity%20in%20computational%20systems,30,332.0,https://cdn.aaai.org/Symposia/Spring/2008/SS-08-03/SS08-03-003.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
31,31,31.0,31.0,31.0,31.0,31.0,Anticipatory model of musical style imitation using collaborative and competitive reinforcement learning,https://scholar.google.com/scholar?q=Anticipatory%20model%20of%20musical%20style%20imitation%20using%20collaborative%20and%20competitive%20reinforcement%20learning,31,52.0,https://inria.hal.science/docs/00/83/90/73/PDF/ArshiaCont_ABIALS06_Chapter.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
32,32,32.0,32.0,32.0,32.0,32.0,"Living melodies: Coevolution of sonic communication Leonardo, 34(3), 243–248 Delgado, M., Fajardo, W., & Molina-Solana, M [DATE]",https://scholar.google.com/scholar?q=Living%20melodies%3A%20Coevolution%20of%20sonic%20communication%20Leonardo%2C%2034%283%29%2C%20243%E2%80%93248%20Delgado%2C%20M.%2C%20Fajardo%2C%20W.%2C%20%26%20Molina-Solana%2C%20M%20%5BDATE%5D,32,71.0,https://www.jstor.org/stable/pdf/1576943.pdf?casa_token=0Py_La3eU2AAAAAA:kjGMx5GnpAD9QK2MNXcMAxmRPJNc46vKYPvWSh9fYKzhWCrhy0KIVKUSwrIayttBtttuTY6MOzuMhsiPlic89h-tpK91TJTjITTreVp8SGWVc_b4BPA,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
33,33,33.0,33.0,33.0,33.0,33.0,Machine improvisation with formal specifications,https://scholar.google.com/scholar?q=Machine%20improvisation%20with%20formal%20specifications,33,36.0,https://www.icmc14-smc14.net/images/proceedings/OS14-B09-MachineImprovisation.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
34,34,34.0,34.0,34.0,34.0,34.0,Improvisation planning and jam session design using concepts of sequence variation and flow experience,https://scholar.google.com/scholar?q=Improvisation%20planning%20and%20jam%20session%20design%20using%20concepts%20of%20sequence%20variation%20and%20flow%20experience,34,28.0,https://hal.science/hal-01161335/file/index.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
35,35,35.0,35.0,35.0,35.0,35.0,Audio oracle: A new algorithm for fast learning of audio structures,https://scholar.google.com/scholar?q=Audio%20oracle%3A%20A%20new%20algorithm%20for%20fast%20learning%20of%20audio%20structures,35,79.0,https://inria.hal.science/hal-00839072/document,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
36,36,36.0,36.0,36.0,36.0,36.0,Audio oracle analysis of musical information rate,https://scholar.google.com/scholar?q=Audio%20oracle%20analysis%20of%20musical%20information%20rate,36,51.0,https://inria.hal.science/docs/00/83/90/65/PDF/Dubnov2011a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
37,37,37.0,37.0,37.0,37.0,37.0,Universal classification applied to musical sequences,https://scholar.google.com/scholar?q=Universal%20classification%20applied%20to%20musical%20sequences,37,74.0,https://inria.hal.science/hal-01161365/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
38,38,38.0,38.0,38.0,38.0,38.0,Emergent rhythms through multi-agency in Max/MSP,https://scholar.google.com/scholar?q=Emergent%20rhythms%20through%20multi-agency%20in%20Max/MSP,38,23.0,http://www.lma.cnrs-mrs.fr/~kronland/Sense_of_Sound/49690368.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
39,39,39.0,39.0,39.0,39.0,39.0,The evolution of evolutionary software: Intelligent rhythm generation in kinetic engine,https://scholar.google.com/scholar?q=The%20evolution%20of%20evolutionary%20software%3A%20Intelligent%20rhythm%20generation%20in%20kinetic%20engine,39,36.0,https://metacreation.net/ktatar/musical_agents/Eigenfeldt_2009b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
40,40,40.0,40.0,40.0,40.0,40.0,Coming together: Composition by negotiation,https://scholar.google.com/scholar?q=Coming%20together%3A%20Composition%20by%20negotiation,40,8.0,https://dl.acm.org/doi/pdf/10.1145/1873951.1874237?casa_token=DA9EfeTVxTkAAAAA:dT5TmcSLW8odYfJ4bAjZCpND0AUnCeGPOJY9O2Mmeuc10hNXjtrMlQWK-vHqBoSemKKop-pSX6P2,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
41,41,41.0,41.0,41.0,41.0,41.0,Multi-agent modeling of complex rhythmic interactions in realtime performance,https://scholar.google.com/scholar?q=Multi-agent%20modeling%20of%20complex%20rhythmic%20interactions%20in%20realtime%20performance,41,6.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
42,42,42.0,42.0,42.0,42.0,42.0,Generating structure-towards large-scale formal generation,https://scholar.google.com/scholar?q=Generating%20structure-towards%20large-scale%20formal%20generation,42,11.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12764/12612,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
43,43,43.0,43.0,43.0,43.0,43.0,Collaborative composition with creative systems: Reflections on the first musebot ensemble,https://scholar.google.com/scholar?q=Collaborative%20composition%20with%20creative%20systems%3A%20Reflections%20on%20the%20first%20musebot%20ensemble,43,32.0,https://axon.cs.byu.edu/ICCC2015proceedings/6.2Eigenfeldt.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
44,44,44.0,44.0,44.0,44.0,44.0,"A realtime generative music system using autonomous melody, harmony, and rhythm agents",https://scholar.google.com/scholar?q=A%20realtime%20generative%20music%20system%20using%20autonomous%20melody%2C%20harmony%2C%20and%20rhythm%20agents,44,25.0,https://generativeart.com/on/cic/GA2009Papers/p7.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
45,45,45.0,45.0,45.0,45.0,45.0,"Creative agents, curatorial agents, and human-agent interaction in coming together",https://scholar.google.com/scholar?q=Creative%20agents%2C%20curatorial%20agents%2C%20and%20human-agent%20interaction%20in%20coming%20together,45,17.0,http://www.sfu.ca/~eigenfel/Eigenfeldt-SMC12-CreativeAgents.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
46,46,46.0,46.0,46.0,46.0,46.0,"What is agency American Journal of Sociology, 103(4), 962–1023 Fan, J., Tatar, K., Thorogood, M., & Pasquier, P [DATE] Ranking-based emotion recognition for experimental music",https://scholar.google.com/scholar?q=What%20is%20agency%20American%20Journal%20of%20Sociology%2C%20103%284%29%2C%20962%E2%80%931023%20Fan%2C%20J.%2C%20Tatar%2C%20K.%2C%20Thorogood%2C%20M.%2C%20%26%20Pasquier%2C%20P%20%5BDATE%5D%20Ranking-based%20emotion%20recognition%20for%20experimental%20music,46,0.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
47,47,47.0,47.0,47.0,47.0,47.0,From agents to organizations: An organizational view of multi-agent systems,https://scholar.google.com/scholar?q=From%20agents%20to%20organizations%3A%20An%20organizational%20view%20of%20multi-agent%20systems,47,1073.0,https://hal-lirmm.ccsd.cnrs.fr/lirmm-00269681/document,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
48,48,48.0,48.0,48.0,48.0,48.0,Mimi4x: An interactive audio-visual installation for high-level structural improvisation,https://scholar.google.com/scholar?q=Mimi4x%3A%20An%20interactive%20audio-visual%20installation%20for%20high-level%20structural%20improvisation,48,23.0,https://kclpure.kcl.ac.uk/portal/files/230294404/Mimi4x_accepted.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
49,49,49.0,49.0,49.0,49.0,49.0,What is generative art complexity theory as a context for art theory,https://scholar.google.com/scholar?q=What%20is%20generative%20art%20complexity%20theory%20as%20a%20context%20for%20art%20theory,49,82.0,https://oss.adm.ntu.edu.sg/20s1-dm3008-tut-g01/wp-content/uploads/sites/10799/2020/08/Philip-Galanter-2016-Generative-Art-Theory-From-Christiane-Paul-A-Companion-to-Digital-Art.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
50,50,50.0,50.0,50.0,50.0,50.0,Appropriate and complementary rhythmic improvisation in an interactive music system,https://scholar.google.com/scholar?q=Appropriate%20and%20complementary%20rhythmic%20improvisation%20in%20an%20interactive%20music%20system,50,8.0,https://link.springer.com/chapter/10.1007/978-1-4471-2990-5_16,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
51,51,51.0,51.0,51.0,51.0,51.0,Anticipatory timing in algorithmic rhythm generation,https://scholar.google.com/scholar?q=Anticipatory%20timing%20in%20algorithmic%20rhythm%20generation,51,3.0,https://eprints.qut.edu.au/33281/1/c33281.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
52,52,52.0,52.0,52.0,52.0,52.0,An ontomemetic approach to musical intelligence,https://scholar.google.com/scholar?q=An%20ontomemetic%20approach%20to%20musical%20intelligence,52,14.0,https://www.researchgate.net/profile/Marcelo-Gimenes/publication/318431828_An_Ontomemetic_Approach_to_Musical_Intelligence/links/596912faaca2728ca67c0061/An-Ontomemetic-Approach-to-Musical-Intelligence.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
53,53,53.0,53.0,53.0,53.0,53.0,Towards an intelligent rhythmic generator based on given examples: A memetic approach,https://scholar.google.com/scholar?q=Towards%20an%20intelligent%20rhythmic%20generator%20based%20on%20given%20examples%3A%20A%20memetic%20approach,53,0.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
54,54,54.0,54.0,54.0,54.0,54.0,Musicianship for robots with style,https://scholar.google.com/scholar?q=Musicianship%20for%20robots%20with%20style,54,5.0,https://dl.acm.org/doi/pdf/10.1145/1279740.1279778?casa_token=-VfiA2SYYckAAAAA:A_e9RDBzT33_Ximzo0S1gYL12aWwteuwpq1Ty_yj_Cb7tjIE3GEWsZTp2-JvG1PO7hFKtM8TgkID,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
55,55,55.0,55.0,55.0,55.0,55.0,Cartesian genetic programming for image processing,https://scholar.google.com/scholar?q=Cartesian%20genetic%20programming%20for%20image%20processing,55,74.0,https://link.springer.com/chapter/10.1007/978-1-4614-6846-2_3,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
56,56,56.0,56.0,56.0,56.0,56.0,All together now: Introducing the virtual human toolkit,https://scholar.google.com/scholar?q=All%20together%20now%3A%20Introducing%20the%20virtual%20human%20toolkit,56,237.0,https://apps.dtic.mil/sti/pdfs/AD1157790.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
57,57,57.0,57.0,57.0,57.0,57.0,A biophysically constrained multi-agent systems approach to algorithmic composition with expressive performance,https://scholar.google.com/scholar?q=A%20biophysically%20constrained%20multi-agent%20systems%20approach%20to%20algorithmic%20composition%20with%20expressive%20performance,57,2.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
58,58,58.0,58.0,58.0,58.0,58.0,"A simple, high-yield method for assessing structural novelity",https://scholar.google.com/scholar?q=A%20simple%2C%20high-yield%20method%20for%20assessing%20structural%20novelity,58,16.0,"https://jyx.jyu.fi/bitstream/handle/123456789/41611/Olivier%20Lartillot%20-%20A%20Simple,%20High-Yield%20Method%20For%20Assessing%20Structural%20Novelty.pdf?sequence=1",,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
59,59,59.0,59.0,59.0,59.0,59.0,A heuristic for computing repeats with a factor oracle: Application to biological sequences,https://scholar.google.com/scholar?q=A%20heuristic%20for%20computing%20repeats%20with%20a%20factor%20oracle%3A%20Application%20to%20biological%20sequences,59,7.0,https://www.tandfonline.com/doi/abs/10.1080/00207160214653,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
60,60,60.0,60.0,60.0,60.0,60.0,BeatBender: Subsumption architecture for autonomous rhythm generation,https://scholar.google.com/scholar?q=BeatBender%3A%20Subsumption%20architecture%20for%20autonomous%20rhythm%20generation,60,11.0,https://dl.acm.org/doi/pdf/10.1145/1501750.1501762?casa_token=pzSoMvZcrGsAAAAA:Teq5SR7AkjLEab8xtPNI7HwaKnbdWv7-meYWZqceY77r4pqflAYwyft6UA9f6dvir4x0T-k5gQ4E,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
61,61,61.0,61.0,61.0,61.0,61.0,OMaxist dialectics,https://scholar.google.com/scholar?q=OMaxist%20dialectics,61,39.0,https://hal.science/hal-00706662/file/OMaxist_Dialectics.v2.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
62,62,62.0,62.0,62.0,62.0,62.0,"Motivation, microdrives and microgoals in mockingbird",https://scholar.google.com/scholar?q=Motivation%2C%20microdrives%20and%20microgoals%20in%20mockingbird,62,3.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12770/12618,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
63,63,63.0,63.0,63.0,63.0,63.0,Automatic design of sound synthesizers as pure data patches using coevolutionary mixed-typed cartesian genetic programming,https://scholar.google.com/scholar?q=Automatic%20design%20of%20sound%20synthesizers%20as%20pure%20data%20patches%20using%20coevolutionary%20mixed-typed%20cartesian%20genetic%20programming,63,26.0,https://dl.acm.org/doi/pdf/10.1145/2576768.2598303?casa_token=P4Oc204fE1QAAAAA:XUx2xdT65V0LZZsXEZQf9Yj7vIlScnE-ZNeyS_5Vg9uY9Hj3wzKkO7RdDDf3Afms9cnRkpptLDch,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
64,64,64.0,64.0,64.0,64.0,64.0,The agent designer toolkit,https://scholar.google.com/scholar?q=The%20agent%20designer%20toolkit,64,171.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=2592b2d9dbcda8dda2d1d5bce38cba89d37e5759,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
65,65,65.0,65.0,65.0,65.0,65.0,A toolkit for designing interactive musical agents,https://scholar.google.com/scholar?q=A%20toolkit%20for%20designing%20interactive%20musical%20agents,65,25.0,https://dl.acm.org/doi/pdf/10.1145/2071536.2071567?casa_token=YYVYm6opPRAAAAAA:V4eYlTS3gFO8_J6rjnPnPT0VH0Yqz3FgeaXswhMR_R4KaBrOwbcAE7MImtRJ9Pl84_iUk4BYj49_,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
66,66,66.0,66.0,66.0,66.0,66.0,Implementation of a real-time musical decision-maker,https://scholar.google.com/scholar?q=Implementation%20of%20a%20real-time%20musical%20decision-maker,66,14.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=ba499ca6f999221a6dd0ba94a7d75699ad932765,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
67,67,67.0,67.0,67.0,67.0,67.0,Creative experiments using a system for learning high-level performance structure in ableton live,https://scholar.google.com/scholar?q=Creative%20experiments%20using%20a%20system%20for%20learning%20high-level%20performance%20structure%20in%20ableton%20live,67,12.0,https://www.academia.edu/download/34141692/Martin__Jin__Carey_and_Bown_-_ADTK_SMC_Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
68,68,68.0,68.0,68.0,68.0,68.0,A connectionist architecture for the evolution of rhythms,https://scholar.google.com/scholar?q=A%20connectionist%20architecture%20for%20the%20evolution%20of%20rhythms,68,18.0,https://joaomartins.eu/pdf/Evomusart06_Joao.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
69,69,69.0,69.0,69.0,69.0,69.0,Emergent rhythmic phrases in an A-Life environment,https://scholar.google.com/scholar?q=Emergent%20rhythmic%20phrases%20in%20an%20A-Life%20environment,69,17.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=7189537a5e521d33813ab84b9b030e7efb289912,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
70,70,70.0,70.0,70.0,70.0,70.0,Breeding rhythms with artificial life,https://scholar.google.com/scholar?q=Breeding%20rhythms%20with%20artificial%20life,70,7.0,http://smc.afim-asso.org/smc08/images/proceedings/session1_number2_paper6.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
71,71,71.0,71.0,71.0,71.0,71.0,MusiCOG: A cognitive architecture for music learning and generation,https://scholar.google.com/scholar?q=MusiCOG%3A%20A%20cognitive%20architecture%20for%20music%20learning%20and%20generation,71,23.0,https://metacreation.net/ktatar/musical_agents/Maxwell_2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
72,72,72.0,72.0,72.0,72.0,72.0,Hierarchical sequential memory for music: A cognitive model,https://scholar.google.com/scholar?q=Hierarchical%20sequential%20memory%20for%20music%3A%20A%20cognitive%20model,72,18.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e5cd26ebb1e25fd4f8a9a3b2b67401a7ab8d934c,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
73,73,73.0,73.0,73.0,73.0,73.0,Life’s what you make: Niche construction and evolutionary art,https://scholar.google.com/scholar?q=Life%E2%80%99s%20what%20you%20make%3A%20Niche%20construction%20and%20evolutionary%20art,73,39.0,https://www.csse.monash.edu/~jonmc/research/Papers/McCormack_EvoMUSART09.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
74,74,74.0,74.0,74.0,74.0,74.0,Virtualband: Interacting with stylistically consistent agents,https://scholar.google.com/scholar?q=Virtualband%3A%20Interacting%20with%20stylistically%20consistent%20agents,74,27.0,https://archives.ismir.net/ismir2013/paper/000277.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
75,75,75.0,75.0,75.0,75.0,75.0,VirtuaLatin – towards a musical multi-agent system,https://scholar.google.com/scholar?q=VirtuaLatin%20%E2%80%93%20towards%20a%20musical%20multi-agent%20system,75,9.0,https://metacreation.net/ktatar/musical_agents/Murray_2005b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
76,76,76.0,76.0,76.0,76.0,76.0,A musical composition application based on a multiagent system to assist novel composers,https://scholar.google.com/scholar?q=A%20musical%20composition%20application%20based%20on%20a%20multiagent%20system%20to%20assist%20novel%20composers,76,9.0,https://hal.science/hal-02187570/document,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
77,77,77.0,77.0,77.0,77.0,77.0,Guided improvisation as dynamic calls to an offline model,https://scholar.google.com/scholar?q=Guided%20improvisation%20as%20dynamic%20calls%20to%20an%20offline%20model,77,20.0,https://hal.science/hal-01184642/file/smc15-improtek.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
78,78,78.0,78.0,78.0,78.0,78.0,Improtek: integrating harmonic controls into improvisation in the filiation of OMax,https://scholar.google.com/scholar?q=Improtek%3A%20integrating%20harmonic%20controls%20into%20improvisation%20in%20the%20filiation%20of%20OMax,78,35.0,https://hal.science/hal-01059330/file/NIkaChemillier-ICMC2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
79,79,79.0,79.0,79.0,79.0,79.0,"ImproteK: Introducing scenarios into human-computer music improvisation Computers in Entertainment, 14(2), 1–27 Nika, J., Echeveste, J., Chemillier, M., & Giavitto, J.-L [DATE] Planning human-computer improvisation",https://scholar.google.com/scholar?q=ImproteK%3A%20Introducing%20scenarios%20into%20human-computer%20music%20improvisation%20Computers%20in%20Entertainment%2C%2014%282%29%2C%201%E2%80%9327%20Nika%2C%20J.%2C%20Echeveste%2C%20J.%2C%20Chemillier%2C%20M.%2C%20%26%20Giavitto%2C%20J.-L%20%5BDATE%5D%20Planning%20human-computer%20improvisation,79,18.0,https://hal.science/docs/01/05/38/34/PDF/ArticleICMC2014.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
80,80,80.0,80.0,80.0,80.0,80.0,Rhythms as emerging structures,https://scholar.google.com/scholar?q=Rhythms%20as%20emerging%20structures,80,32.0,https://www.francoispachet.fr/wp-content/uploads/2021/01/pachet-00-rhythm.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
81,81,81.0,81.0,81.0,81.0,81.0,On impact and evaluation in computational creativity: A discussion of the Turing test and an alternative proposal,https://scholar.google.com/scholar?q=On%20impact%20and%20evaluation%20in%20computational%20creativity%3A%20A%20discussion%20of%20the%20Turing%20test%20and%20an%20alternative%20proposal,81,125.0,https://www.researchgate.net/profile/Alison-Pease/publication/230855719_On_Impact_and_Evaluation_in_Computational_Creativity_A_Discussion_of_the_Turing_Test_and_an_Alternative_Proposal/links/0c960517533753d6f8000000/On-Impact-and-Evaluation-in-Computational-Creativity-A-Discussion-of-the-Turing-Test-and-an-Alternative-Proposal.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
82,82,82.0,82.0,82.0,82.0,82.0,"Milieus of creativity: The role of places, environments, and spatial",https://scholar.google.com/scholar?q=Milieus%20of%20creativity%3A%20The%20role%20of%20places%2C%20environments%2C%20and%20spatial,82,207.0,https://www.researchgate.net/profile/Peter-Meusburger/publication/225908618_Milieus_of_Creativity_The_Role_of_Places_Environments_and_Spatial_Contexts/links/5762c2ae08aee61395bef546/Milieus-of-Creativity-The-Role-of-Places-Environments-and-Spatial-Contexts.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
83,83,83.0,83.0,83.0,83.0,83.0,Using coevolution in music improvisation,https://scholar.google.com/scholar?q=Using%20coevolution%20in%20music%20improvisation,83,2.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
84,84,84.0,84.0,84.0,84.0,84.0,"Markov decision processes: Discrete stochastic dynamic programming Wiley series in probability and mathematical statistics Applied Probability and Statistics Section New York: John Wiley & Sons Rigau, J., Feixas, M., & Sbert, M [DATE]",https://scholar.google.com/scholar?q=Markov%20decision%20processes%3A%20Discrete%20stochastic%20dynamic%20programming%20Wiley%20series%20in%20probability%20and%20mathematical%20statistics%20Applied%20Probability%20and%20Statistics%20Section%20New%20York%3A%20John%20Wiley%20%26%20Sons%20Rigau%2C%20J.%2C%20Feixas%2C%20M.%2C%20%26%20Sbert%2C%20M%20%5BDATE%5D,84,18120.0,"https://books.google.com/books?hl=en&lr=&id=VvBjBAAAQBAJ&oi=fnd&pg=PT9&dq=Markov+decision+processes:+Discrete+stochastic+dynamic+programming+Wiley+series+in+probability+and+mathematical+statistics+Applied+Probability+and+Statistics+Section+New+York:+John+Wiley+%26+Sons+Rigau,+J.,+Feixas,+M.,+%26+Sbert,+M+%5BDATE%5D&ots=rsoDyNTVMM&sig=dm-R4FtrAMFesqaSGZITzXD9WQ0",,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
85,85,85.0,85.0,85.0,85.0,85.0,"The new science of management decision The Ford Distinguished Lectures, Vol xii New York, NY: Harper & Brothers doi:10.1037/13978-000 Sivanandam, S N., & Deepa, S N [DATE]",https://scholar.google.com/scholar?q=The%20new%20science%20of%20management%20decision%20The%20Ford%20Distinguished%20Lectures%2C%20Vol%20xii%20New%20York%2C%20NY%3A%20Harper%20%26%20Brothers%20doi%3A10.1037/13978-000%20Sivanandam%2C%20S%20N.%2C%20%26%20Deepa%2C%20S%20N%20%5BDATE%5D,85,10377.0,https://psycnet.apa.org/record/2009-05849-000,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
86,86,86.0,86.0,86.0,86.0,86.0,ML.* Machine learning library as a musical partner in the computer-acoustic composition flight,https://scholar.google.com/scholar?q=ML.%2A%20Machine%20learning%20library%20as%20a%20musical%20partner%20in%20the%20computer-acoustic%20composition%20flight,86,4.0,https://scholarworks.iupui.edu/bitstream/handle/1805/15340/bbp2372.2014.197.pdf?sequence=1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
87,87,87.0,87.0,87.0,87.0,87.0,"Reinforcement learning and the creative, automated music improviser",https://scholar.google.com/scholar?q=Reinforcement%20learning%20and%20the%20creative%2C%20automated%20music%20improviser,87,21.0,https://metacreation.net/ktatar/musical_agents/Smith_2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
88,88,88.0,88.0,88.0,88.0,88.0,Feature selection and composition using PyOracle,https://scholar.google.com/scholar?q=Feature%20selection%20and%20composition%20using%20PyOracle,88,48.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12653/12501,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
89,89,89.0,89.0,89.0,89.0,89.0,"Using physically based models and genetic algorithms for functional composition of sound signals, synchronized to animated motion",https://scholar.google.com/scholar?q=Using%20physically%20based%20models%20and%20genetic%20algorithms%20for%20functional%20composition%20of%20sound%20signals%2C%20synchronized%20to%20animated%20motion,89,43.0,https://apps.dtic.mil/sti/pdfs/ADA456431.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
90,90,90.0,90.0,90.0,90.0,90.0,"MASOM: A musical agent architecture based on self organizing maps, affective computing, and variable Markov models",https://scholar.google.com/scholar?q=MASOM%3A%20A%20musical%20agent%20architecture%20based%20on%20self%20organizing%20maps%2C%20affective%20computing%2C%20and%20variable%20Markov%20models,90,24.0,https://www.researchgate.net/profile/Kivanc-Tatar/publication/317357259_MASOM_A_Musical_Agent_Architecture_based_on_Self-Organizing_Maps_Affective_Computing_and_Variable_Markov_Models/links/59363cddaca272fc556b7587/MASOM-A-Musical-Agent-Architecture-based-on-Self-Organizing-Maps-Affective-Computing-and-Variable-Markov-Models.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
91,91,91.0,91.0,91.0,91.0,91.0,"REVIVE: An audiovisual performance with musical and visual AI agents (pp 1–6) New York, NY: ACM Press Thom, B (2000a) BoB: An interactive improvisational music companion",https://scholar.google.com/scholar?q=REVIVE%3A%20An%20audiovisual%20performance%20with%20musical%20and%20visual%20AI%20agents%20%28pp%201%E2%80%936%29%20New%20York%2C%20NY%3A%20ACM%20Press%20Thom%2C%20B%20%282000a%29%20BoB%3A%20An%20interactive%20improvisational%20music%20companion,91,14.0,https://dl.acm.org/doi/pdf/10.1145/3170427.3177771?casa_token=jgsZqjW4RUEAAAAA:X2cftSU4Qpn3kcRxhbtffecJPSt0gbTbukWpfEn30iEMvKc9NZb68lRtKXvsZNkcMQBqaFTFgHIZ,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
92,92,92.0,92.0,92.0,92.0,92.0,A methodology for the comparison of melodic generation models using meta-melo,https://scholar.google.com/scholar?q=A%20methodology%20for%20the%20comparison%20of%20melodic%20generation%20models%20using%20meta-melo,92,14.0,https://archives.ismir.net/ismir2013/paper/000228.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
93,93,93.0,93.0,93.0,93.0,93.0,Frankensteinian methods for evolutionary music,https://scholar.google.com/scholar?q=Frankensteinian%20methods%20for%20evolutionary%20music,93,220.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=06393b6228ef8add341ef6c0e015e7409570a7b0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
94,94,94.0,94.0,94.0,94.0,94.0,Andante: A mobile musical agents infrastructure,https://scholar.google.com/scholar?q=Andante%3A%20A%20mobile%20musical%20agents%20infrastructure,94,16.0,https://www.researchgate.net/profile/Fabio-Kon/publication/2890459_Andante_A_Mobile_Musical_Agents_Infrastructure/links/004635303764b2cfa8000000/Andante-A-Mobile-Musical-Agents-Infrastructure.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
95,95,95.0,95.0,95.0,95.0,95.0,Guided music synthesis with variable Markov oracle,https://scholar.google.com/scholar?q=Guided%20music%20synthesis%20with%20variable%20Markov%20oracle,95,52.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12767/12615,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
96,96,96.0,96.0,96.0,96.0,96.0,Context-aware hidden Markov models of jazz music with variable Markov oracle,https://scholar.google.com/scholar?q=Context-aware%20hidden%20Markov%20models%20of%20jazz%20music%20with%20variable%20Markov%20oracle,96,7.0,https://www.researchgate.net/profile/Shlomo-Dubnov/publication/320268052_Context-Aware_Hidden_Markov_Models_of_Jazz_Music_with_Variable_Markov_Oracle/links/59d8f28daca272e60966c983/Context-Aware-Hidden-Markov-Models-of-Jazz-Music-with-Variable-Markov-Oracle.pdf,https://github.com/wangsix/markov_,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
97,97,97.0,97.0,97.0,97.0,97.0,Conceptual bases for creative thinking in music,https://scholar.google.com/scholar?q=Conceptual%20bases%20for%20creative%20thinking%20in%20music,97,158.0,https://link.springer.com/chapter/10.1007/978-1-4613-8698-8_8,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
98,98,98.0,98.0,98.0,98.0,98.0,Using ideas from natural selection to evolve synthesized sounds,https://scholar.google.com/scholar?q=Using%20ideas%20from%20natural%20selection%20to%20evolve%20synthesized%20sounds,98,23.0,https://www.dafx.de/paper-archive/1998/WEH46.PS.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
99,99,99.0,99.0,99.0,99.0,99.0,A multiagent approach for musical interactive systems,https://scholar.google.com/scholar?q=A%20multiagent%20approach%20for%20musical%20interactive%20systems,99,44.0,https://dl.acm.org/doi/pdf/10.1145/860575.860669?casa_token=h8VLXAEXSukAAAAA:F-Zd7aj6ST8Jy-g_tLIbeFga-xP1kUeq8SnO4VxaibrrYBtgXOaSISR4nzupDksZKyMTLFQqJkP9,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
100,100,100.0,100.0,100.0,100.0,100.0,Experience driven design of creative systems,https://scholar.google.com/scholar?q=Experience%20driven%20design%20of%20creative%20systems,100,13.0,https://research.gold.ac.uk/id/eprint/18617/1/COM_d%27Inverno_2016b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
101,101,101.0,101.0,101.0,101.0,101.0,An automated music improviser using a genetic algorithm driven synthesis engine,https://scholar.google.com/scholar?q=An%20automated%20music%20improviser%20using%20a%20genetic%20algorithm%20driven%20synthesis%20engine,101,24.0,https://www.researchgate.net/profile/Matthew-Yee-King/publication/220867158_An_Automated_Music_Improviser_Using_a_Genetic_Algorithm_Driven_Synthesis_Engine/links/599af0710f7e9b892bacff5a/An-Automated-Music-Improviser-Using-a-Genetic-Algorithm-Driven-Synthesis-Engine.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
102,102,102.0,102.0,102.0,102.0,102.0,"This time with feeling: Learning
expressive musical performance",https://scholar.google.com/scholar?q=This%20time%20with%20feeling%3A%20Learning%0Aexpressive%20musical%20performance,102,200.0,https://link.springer.com/article/10.1007/s00521-018-3758-9,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
103,103,104.0,104.0,104.0,104.0,104.0,Research in music and artificial intelligence,https://scholar.google.com/scholar?q=Research%20in%20music%20and%20artificial%20intelligence,104,161.0,https://dl.acm.org/doi/pdf/10.1145/4468.4469,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
104,104,105.0,105.0,105.0,105.0,105.0,Algorithmic composition: paradigms of automated music generation,https://scholar.google.com/scholar?q=Algorithmic%20composition%3A%20paradigms%20of%20automated%20music%20generation,105,560.0,https://books.google.com/books?hl=en&lr=&id=jaowAtnXsDQC&oi=fnd&pg=PA1&dq=Algorithmic+composition:+paradigms+of+automated+music+generation&ots=GPMgdRchQm&sig=QIg5gXn08BDJavwBwC-x51hgmIk,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
105,105,106.0,106.0,106.0,106.0,106.0,Ai methods in algorithmic composition: A comprehensive survey,https://scholar.google.com/scholar?q=Ai%20methods%20in%20algorithmic%20composition%3A%20A%20comprehensive%20survey,106,439.0,https://www.jair.org/index.php/jair/article/download/10845/25883/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
106,106,107.0,107.0,107.0,107.0,107.0,Computational intelligence in music composition: A survey,https://scholar.google.com/scholar?q=Computational%20intelligence%20in%20music%20composition%3A%20A%20survey,107,66.0,http://mx.nthu.edu.tw/~ckting/pubs/tetci2017.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
107,107,109.0,109.0,109.0,109.0,109.0,Music generation by deep learning-challenges and directions,https://scholar.google.com/scholar?q=Music%20generation%20by%20deep%20learning-challenges%20and%20directions,109,58.0,https://arxiv.org/pdf/1712.04371,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
108,108,110.0,110.0,110.0,110.0,110.0,"Algoritmic music composition based on
artificial intelligence: A survey",https://scholar.google.com/scholar?q=Algoritmic%20music%20composition%20based%20on%0Aartificial%20intelligence%3A%20A%20survey,110,52.0,https://ieeexplore.ieee.org/abstract/document/8327197/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
109,109,111.0,111.0,111.0,111.0,111.0,"From artificial neural networks to deep learning for music generation–history, concepts and trends",https://scholar.google.com/scholar?q=From%20artificial%20neural%20networks%20to%20deep%20learning%20for%20music%20generation%E2%80%93history%2C%20concepts%20and%20trends,111,76.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s00521-020-05399-0&casa_token=LLyBxm2MYGwAAAAA:xF5iL-jvzcITCSo8YP39ypFalYm2kPbJoJptxNaZyL2QpkZG4SCEpy2b20juMvG6Ja6k3dRFDnzo8BjL,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
110,110,112.0,112.0,112.0,112.0,112.0,"Programming languages for computer music synthesis, performance, and composition",https://scholar.google.com/scholar?q=Programming%20languages%20for%20computer%20music%20synthesis%2C%20performance%2C%20and%20composition,112,109.0,https://dl.acm.org/doi/pdf/10.1145/4468.4485,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
111,111,113.0,113.0,113.0,113.0,113.0,Automated composition in retrospect: 1956-1986,https://scholar.google.com/scholar?q=Automated%20composition%20in%20retrospect%3A%201956-1986,113,165.0,https://www.jstor.org/stable/pdf/1578334.pdf?casa_token=1OE-TazVL_IAAAAA:8y1hNgOy2-Tu0B7NvsG1h8hS_lDVo4BWPAyE_b-gEtxtysWHaCDOmWMjeAqytcBv9NipZ8TyEj0R4vMkoFAlBce39qfq-S3rsWyUzmUSLWnngvOO6z8,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
112,112,114.0,114.0,114.0,114.0,114.0,"The history and development of algorithms in music composition, 1957-1993",https://scholar.google.com/scholar?q=The%20history%20and%20development%20of%20algorithms%20in%20music%20composition%2C%201957-1993,114,30.0,https://search.proquest.com/openview/ad7eed2c1aa8bd5a03e209c77cb06ea5/1?pq-origsite=gscholar&cbl=18750&diss=y,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
113,113,115.0,115.0,115.0,115.0,115.0,"Ai methods for algorithmic composition: A survey, a critical view and
future prospects",https://scholar.google.com/scholar?q=Ai%20methods%20for%20algorithmic%20composition%3A%20A%20survey%2C%20a%20critical%20view%20and%0Afuture%20prospects,115,326.0,https://www.researchgate.net/profile/Geraint-Wiggins/publication/209436205_AI_Methods_for_Algorithmic_Composition_A_Survey_a_Critical_view_and_Future_Prospects/links/5464887b0cf2c0c6aec570ce/AI-Methods-for-Algorithmic-Composition-A-Survey-a-Critical-view-and-Future-Prospects.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
114,114,116.0,116.0,116.0,116.0,116.0,Motivations and methodologies for automation of the compositional process,https://scholar.google.com/scholar?q=Motivations%20and%20methodologies%20for%20automation%20of%20the%20compositional%20process,116,119.0,https://journals.sagepub.com/doi/pdf/10.1177/102986490200600203?casa_token=LkVN5qyH5wYAAAAA:3Lv5BR0Bzn98Hy4fj5nj-SNDrT_XXzirbaoeRRcCox_0xJoY3bkosLlod8XSQAvm9WUlrqviGfYQ,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
115,115,117.0,117.0,117.0,117.0,117.0,Musical form and algorithmic composition,https://scholar.google.com/scholar?q=Musical%20form%20and%20algorithmic%20composition,117,51.0,https://www.tandfonline.com/doi/pdf/10.1080/07494460802664064?casa_token=n3GZNWKnw_MAAAAA:gGwCg74lecNqE52QGfH1Ic-sxsTKNV-IfxQ5x9JTsbuz2NTb3-jRcwXm17y9DsDRPUx6pXYMnBR0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
116,116,118.0,118.0,118.0,118.0,118.0,A functional taxonomy of music generation systems,https://scholar.google.com/scholar?q=A%20functional%20taxonomy%20of%20music%20generation%20systems,118,169.0,https://dl.acm.org/doi/pdf/10.1145/3108242?casa_token=ywDQEdMYGq4AAAAA:lbAcoCxC0-oofiKe5o1mFUopelc2ffwpIhWLxrvYCRPS0WbX1qiG4Bvnj-8ZrSit35nI4U0dzr48,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
117,117,119.0,119.0,119.0,119.0,119.0,Computational models of expressive music performance: The state of the art,https://scholar.google.com/scholar?q=Computational%20models%20of%20expressive%20music%20performance%3A%20The%20state%20of%20the%20art,119,314.0,https://www.tandfonline.com/doi/pdf/10.1080/0929821042000317804?casa_token=uvt7fQyVx0QAAAAA:xyp6C8GPjJ428JblgcfMQX4Y850UYiqunz1HRRuMvtNyqQKcj4MgRLSOrR5I0K0Ld1KEUnllhfAU,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
118,118,120.0,120.0,120.0,120.0,120.0,An overview of computer systems for expressive music performance,https://scholar.google.com/scholar?q=An%20overview%20of%20computer%20systems%20for%20expressive%20music%20performance,120,46.0,https://link.springer.com/chapter/10.1007/978-1-4471-4123-5_1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
119,119,121.0,121.0,121.0,121.0,121.0,"Music generation with variational recurrent autoencoder supported by
history",https://scholar.google.com/scholar?q=Music%20generation%20with%20variational%20recurrent%20autoencoder%20supported%20by%0Ahistory,121,26.0,https://www.researchgate.net/profile/Ivan-Yamshchikov/publication/316985278_Music_generation_with_variational_recurrent_autoencoder_supported_by_history/links/5a982f3baca27214056d3ea6/Music-generation-with-variational-recurrent-autoencoder-supported-by-history.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
120,120,122.0,122.0,122.0,122.0,122.0,Dice music in the eighteenth century,https://scholar.google.com/scholar?q=Dice%20music%20in%20the%20eighteenth%20century,122,154.0,https://www.jstor.org/stable/pdf/734136.pdf?casa_token=98-eMcu728sAAAAA:pc-GLH9a3EFUOMZqli48ne1ueIcmh1Ak9vn58uQqXnbR2OczqT1uzWmU5-gV4q8qSJZ31EENHMiQzgtffQb2QNXHQgpdaWIrkieuTWsUwIS3KsAPBl0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
121,121,123.0,123.0,123.0,123.0,123.0,The completion of john cage’s freeman etudes,https://scholar.google.com/scholar?q=The%20completion%20of%20john%20cage%E2%80%99s%20freeman%20etudes,123,14.0,https://www.jstor.org/stable/pdf/833611.pdf?casa_token=hQRedCkMU6oAAAAA:aaGJflOUpIpSPQv27EJBe8NsW6Awsur9HjMxVyOuKIia4KPjqFJqFAFN__WJ_AjyX9ebXPS6froz_07-TPg5jcaGlPjrCvKEuK4_M4z5Rre6iFDvQpM,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
122,122,124.0,124.0,124.0,124.0,124.0,Information theory and melody,https://scholar.google.com/scholar?q=Information%20theory%20and%20melody,124,249.0,https://www.jstor.org/stable/26171737,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
123,123,125.0,125.0,125.0,125.0,125.0,An experiment in musical composition,https://scholar.google.com/scholar?q=An%20experiment%20in%20musical%20composition,125,179.0,https://ieeexplore.ieee.org/abstract/document/5222016/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
124,124,126.0,126.0,126.0,126.0,126.0,Experimental Music: Composition with an electronic computer,https://scholar.google.com/scholar?q=Experimental%20Music%3A%20Composition%20with%20an%20electronic%20computer,126,8.0,https://www.jstor.org/stable/842857,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
125,125,127.0,127.0,127.0,127.0,127.0,"Automatic music
composition with simple probabilistic generative grammars",https://scholar.google.com/scholar?q=Automatic%20music%0Acomposition%20with%20simple%20probabilistic%20generative%20grammars,127,17.0,https://www.scielo.org.mx/pdf/poli/n44/n44a10.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
126,126,128.0,128.0,128.0,128.0,128.0,"A maximization technique occurring in the statistical
analysis of probabilistic functions of markov chains",https://scholar.google.com/scholar?q=A%20maximization%20technique%20occurring%20in%20the%20statistical%0Aanalysis%20of%20probabilistic%20functions%20of%20markov%20chains,128,6408.0,https://www.jstor.org/stable/pdf/2239727.pdf?casa_token=erEA-iEj4wQAAAAA:xbcm2aqzEj5tWyqmt9VoidTiNEMpi0WzcXCgYz_uUiH8uZQvaImLYUCncvuPz4UF-Qa3dE9WHVrSwYLuidsw1wqfvwIlahMA7Jk8yqTRwKotwrE9xb4,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
127,127,129.0,129.0,129.0,129.0,129.0,"The algorithmic composer, volume 16",https://scholar.google.com/scholar?q=The%20algorithmic%20composer%2C%20volume%2016,129,196.0,"https://books.google.com/books?hl=en&lr=&id=rFGH07I2KTcC&oi=fnd&pg=PR9&dq=The+algorithmic+composer,+volume+16&ots=Fto1ikE8ZJ&sig=AJp2oiwXoLhEHVUf0NS9OHwHk5Q",,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
128,128,130.0,130.0,130.0,130.0,130.0,A connectionist approach to algorithmic composition,https://scholar.google.com/scholar?q=A%20connectionist%20approach%20to%20algorithmic%20composition,130,366.0,https://www.jstor.org/stable/pdf/3679551.pdf?casa_token=6hhISBILU00AAAAA:wuA7lniuc3IF8ebhzGhwlZ5QdmL9_FbqkoKnqtmHDH-vxpM1NUs-KMe5b9r0AfrtydMaXVnvbsBvflPbKgh44tHvzQ17NZyvhhLSdsyNafGSbeA-3C4,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
129,129,131.0,131.0,131.0,131.0,131.0,"Music, mind, and meaning",https://scholar.google.com/scholar?q=Music%2C%20mind%2C%20and%20meaning,131,135.0,https://link.springer.com/chapter/10.1007/978-1-4684-8917-0_1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
130,130,132.0,132.0,132.0,132.0,132.0,Genjam: A genetic algorithm for generating jazz solos,https://scholar.google.com/scholar?q=Genjam%3A%20A%20genetic%20algorithm%20for%20generating%20jazz%20solos,132,850.0,https://www.researchgate.net/profile/John-Biles-2/publication/2342018_GenJam_A_Genetic_Algorithm_for_Generating_Jazz_Solos/links/54debcb80cf2510fcee4b529/GenJam-A-Genetic-Algorithm-for-Generating-Jazz-Solos.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
131,131,133.0,133.0,133.0,133.0,133.0,Experiments in musical intelligence (emi): Non-linear linguistic-based composition,https://scholar.google.com/scholar?q=Experiments%20in%20musical%20intelligence%20%28emi%29%3A%20Non-linear%20linguistic-based%20composition,133,70.0,https://www.tandfonline.com/doi/abs/10.1080/09298218908570541,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
132,132,134.0,134.0,134.0,134.0,134.0,Polyphonic music modeling with random fields,https://scholar.google.com/scholar?q=Polyphonic%20music%20modeling%20with%20random%20fields,134,47.0,https://dl.acm.org/doi/pdf/10.1145/957013.957041?casa_token=fZZ45ZsGGycAAAAA:qlu1C0a-NbGyj6yFO9HwSbqiLgQIe8aB27bUNdWDuq2uskTqHngiALiKT8zI2CHTka2EIVNh_V86,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
133,133,135.0,135.0,135.0,135.0,135.0,A style-specific music composition neural network,https://scholar.google.com/scholar?q=A%20style-specific%20music%20composition%20neural%20network,135,40.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11063-020-10241-8&casa_token=Y1JdXc0qmp4AAAAA:DKiQTRaNU_uVdXlZk0ae8h2-Za-j3mJcLRyRsl1VkUQGhNBLssTQydw-8t8aUqYxv2phqAOnvemv7gG1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
134,134,136.0,136.0,136.0,136.0,136.0,"Lstm based music generation with dataset
preprocessing and reconstruction techniques",https://scholar.google.com/scholar?q=Lstm%20based%20music%20generation%20with%20dataset%0Apreprocessing%20and%20reconstruction%20techniques,136,24.0,https://ieeexplore.ieee.org/abstract/document/8628712/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
135,135,137.0,137.0,137.0,137.0,137.0,"Generating musical performances with director
musices",https://scholar.google.com/scholar?q=Generating%20musical%20performances%20with%20director%0Amusices,137,128.0,https://www.jstor.org/stable/pdf/3681735.pdf?casa_token=6LwWpmCGHakAAAAA:mDLOMUqBcvfuxfsXxwHqLXcjmnnB2FIuLSfcr-9BwktW87UyebI2tm_4xuXlnl9m9ungdsYfJrP1ogNRoA1rMqAOEnpz2Y2oEEbphJP_c2pcvrIGAyc,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
136,136,138.0,138.0,138.0,138.0,138.0,pdm: an expressive sequencer with real-time control of the kth music-performance rules,https://scholar.google.com/scholar?q=pdm%3A%20an%20expressive%20sequencer%20with%20real-time%20control%20of%20the%20kth%20music-performance%20rules,138,91.0,https://www.jstor.org/stable/pdf/3682025.pdf?casa_token=0uKtvTtepscAAAAA:li7kdkOnav7s5PwIpkVqtpWJ_24pjpx9RSLewR1PRJwGp_7zTqEzeqIM0zCn66YnIVk2eQr_uKr1BhQ76XE9poHksni1tbRmb7hFGayVCFDdNt9Li7E,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
137,137,139.0,139.0,139.0,139.0,139.0,"Modeling, analyzing, and synthesizing expressive piano performance with
graphical models",https://scholar.google.com/scholar?q=Modeling%2C%20analyzing%2C%20and%20synthesizing%20expressive%20piano%20performance%20with%0Agraphical%20models,139,47.0,https://link.springer.com/content/pdf/10.1007/s10994-006-8751-3.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
138,138,140.0,140.0,140.0,140.0,140.0,"Expressive performance rendering with probabilistic
models",https://scholar.google.com/scholar?q=Expressive%20performance%20rendering%20with%20probabilistic%0Amodels,140,24.0,http://www.cp.jku.at/research/papers/flossmann_etal_book_2012_preprint.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
139,139,141.0,141.0,141.0,141.0,141.0,"Statistical approach to automatic
expressive rendition of polyphonic piano music",https://scholar.google.com/scholar?q=Statistical%20approach%20to%20automatic%0Aexpressive%20rendition%20of%20polyphonic%20piano%20music,141,13.0,https://link.springer.com/chapter/10.1007/978-1-4471-4123-5_6,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
140,140,142.0,142.0,142.0,142.0,142.0,Modeling piano interpretation using switching kalman filter,https://scholar.google.com/scholar?q=Modeling%20piano%20interpretation%20using%20switching%20kalman%20filter,142,15.0,https://ismir2012.ismir.net/event/papers/145_ISMIR_2012.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
141,141,143.0,143.0,143.0,143.0,143.0,"An evaluation of score descriptors combined with non-linear
models of expressive dynamics in music",https://scholar.google.com/scholar?q=An%20evaluation%20of%20score%20descriptors%20combined%20with%20non-linear%0Amodels%20of%20expressive%20dynamics%20in%20music,143,13.0,http://www.carloscancinochacon.com/documents/peer_reviewed/CancinoEtAl-DS-2015.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
142,142,144.0,144.0,144.0,144.0,144.0,"An evaluation of
linear and non-linear models of expressive dynamics in classical piano and symphonic music",https://scholar.google.com/scholar?q=An%20evaluation%20of%0Alinear%20and%20non-linear%20models%20of%20expressive%20dynamics%20in%20classical%20piano%20and%20symphonic%20music,144,42.0,https://link.springer.com/article/10.1007/s10994-017-5631-y,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
143,143,145.0,145.0,145.0,145.0,145.0,"An assessment of learned score features for modeling expressive dynamics in
music",https://scholar.google.com/scholar?q=An%20assessment%20of%20learned%20score%20features%20for%20modeling%20expressive%20dynamics%20in%0Amusic,145,26.0,http://www.cp.jku.at/research/papers/Grachten_Krebs_TMM_2013.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
144,144,146.0,146.0,146.0,146.0,146.0,"Predicting expressive dynamics in piano performances
using neural networks",https://scholar.google.com/scholar?q=Predicting%20expressive%20dynamics%20in%20piano%20performances%0Ausing%20neural%20networks,146,22.0,https://dspace.library.uu.nl/bitstream/handle/1874/303776/Herwaarden_Grachten_De_Haas_2014_ISMIR_NN_Piano.pdf?sequence=1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
145,145,147.0,147.0,147.0,147.0,147.0,Duet interaction: learning musicianship for automatic accompaniment,https://scholar.google.com/scholar?q=Duet%20interaction%3A%20learning%20musicianship%20for%20automatic%20accompaniment,147,17.0,http://www.cs.cmu.edu/afs/cs/user/gxia/www/PDF/NIME2015paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
146,146,148.0,148.0,148.0,148.0,148.0,"Spectral learning for expressive interactive
ensemble music performance",https://scholar.google.com/scholar?q=Spectral%20learning%20for%20expressive%20interactive%0Aensemble%20music%20performance,148,40.0,https://www.cs.cmu.edu/~ggordon/xia-wang-dannenberg-gordon-ismir2015spectral.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
147,147,149.0,149.0,149.0,149.0,149.0,Physical modeling using digital waveguides,https://scholar.google.com/scholar?q=Physical%20modeling%20using%20digital%20waveguides,149,977.0,https://www.jstor.org/stable/pdf/3680470.pdf?casa_token=t9waxRJI2bEAAAAA:_jFLP1qoVpGoZ092XfdL0T69YyOtAco14fHWbEz8wXys1JRWsyohsSsoAjOISfZy_7PbtKYa_Us8NpNof_Be7GmhsiuJls0vX2P8JI-jQHLuBTZvQd8,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
148,148,150.0,150.0,150.0,150.0,150.0,"Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and
future trends",https://scholar.google.com/scholar?q=Deep%20learning%20for%20acoustic%20modeling%20in%20parametric%20speech%20generation%3A%20A%20systematic%20review%20of%20existing%20techniques%20and%0Afuture%20trends,150,281.0,https://www1.se.cuhk.edu.hk/~hccl/publications/pub/2015_07078992.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
149,149,151.0,151.0,151.0,151.0,151.0,Synthesis of the singing voice by performance sampling and spectral models,https://scholar.google.com/scholar?q=Synthesis%20of%20the%20singing%20voice%20by%20performance%20sampling%20and%20spectral%20models,151,136.0,https://mycourses.aalto.fi/pluginfile.php/924051/mod_assign/intro/4-SynthesisOfSingingVoice.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
150,150,152.0,152.0,152.0,152.0,152.0,"Expressive singing synthesis based on unit selection for
the singing synthesis challenge 2016",https://scholar.google.com/scholar?q=Expressive%20singing%20synthesis%20based%20on%20unit%20selection%20for%0Athe%20singing%20synthesis%20challenge%202016,152,43.0,https://repositori.upf.edu/bitstream/handle/10230/32188/Bonada_Interspeech2016_expr.PDF?sequence=1&isAllowed=y,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
151,151,153.0,153.0,153.0,153.0,153.0,"Recent
development of the hmm-based singing voice synthesis system—sinsy",https://scholar.google.com/scholar?q=Recent%0Adevelopment%20of%20the%20hmm-based%20singing%20voice%20synthesis%20system%E2%80%94sinsy,153,128.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
152,152,154.0,154.0,154.0,154.0,154.0,Integration of speaker and pitch adaptive training for hmm-based singing voice synthesis,https://scholar.google.com/scholar?q=Integration%20of%20speaker%20and%20pitch%20adaptive%20training%20for%20hmm-based%20singing%20voice%20synthesis,154,21.0,https://nitech.repo.nii.ac.jp/?action=repository_action_common_download&item_id=5943&item_no=1&attribute_id=15&file_no=1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
153,153,155.0,155.0,155.0,155.0,155.0,A neural parametric singing synthesizer,https://scholar.google.com/scholar?q=A%20neural%20parametric%20singing%20synthesizer,155,75.0,https://arxiv.org/pdf/1704.03809,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
154,154,156.0,156.0,156.0,156.0,156.0,Long short-term memory,https://scholar.google.com/scholar?q=Long%20short-term%20memory,156,1446.0,https://link.springer.com/chapter/10.1007/978-3-642-24797-2_4,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
155,155,157.0,157.0,157.0,157.0,157.0,"Finding temporal structure in music: Blues improvisation with lstm recurrent
networks",https://scholar.google.com/scholar?q=Finding%20temporal%20structure%20in%20music%3A%20Blues%20improvisation%20with%20lstm%20recurrent%0Anetworks,157,330.0,ftp://ftp.idsia.ch/pub/juergen/2002_ieee.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
156,156,158.0,158.0,158.0,158.0,158.0,Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription,https://scholar.google.com/scholar?q=Modeling%20temporal%20dependencies%20in%20highdimensional%20sequences%3A%20Application%20to%20polyphonic%20music%20generation%20and%20transcription,158,891.0,https://arxiv.org/pdf/1206.6392,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
157,157,159.0,159.0,159.0,159.0,159.0,Generating long-term structure in songs and stories,https://scholar.google.com/scholar?q=Generating%20long-term%20structure%20in%20songs%20and%20stories,159,63.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
158,158,161.0,161.0,161.0,161.0,161.0,Generating polyphonic music using tied parallel networks,https://scholar.google.com/scholar?q=Generating%20polyphonic%20music%20using%20tied%20parallel%20networks,161,132.0,https://www.danieldjohnson.com/files/2017generatingpolyphonic.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
159,159,162.0,162.0,162.0,162.0,162.0,"A hierarchical latent vector model for
learning long-term structure in music",https://scholar.google.com/scholar?q=A%20hierarchical%20latent%20vector%20model%20for%0Alearning%20long-term%20structure%20in%20music,162,475.0,http://proceedings.mlr.press/v80/roberts18a/roberts18a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
160,160,163.0,163.0,163.0,163.0,163.0,"Impromptu accompaniment of pop music using coupled latent
variable model with binary regularizer",https://scholar.google.com/scholar?q=Impromptu%20accompaniment%20of%20pop%20music%20using%20coupled%20latent%0Avariable%20model%20with%20binary%20regularizer,163,5.0,https://ieeexplore.ieee.org/abstract/document/8852373/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
161,161,164.0,164.0,164.0,164.0,164.0,"Midinet: A convolutional generative adversarial network for
symbolic-domain music generation",https://scholar.google.com/scholar?q=Midinet%3A%20A%20convolutional%20generative%20adversarial%20network%20for%0Asymbolic-domain%20music%20generation,164,536.0,https://arxiv.org/pdf/1703.10847.pdf?source=post_page---------------------------,https://github.com/RichardYang40148/MidiNet2,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
162,162,168.0,168.0,168.0,168.0,168.0,"Lakhnes: Improving
multi-instrumental music generation with cross-domain pre-training",https://scholar.google.com/scholar?q=Lakhnes%3A%20Improving%0Amulti-instrumental%20music%20generation%20with%20cross-domain%20pre-training,168,116.0,https://arxiv.org/pdf/1907.04868,https://github.com/chrisdonahue/LakhNESModel,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
163,163,169.0,169.0,169.0,169.0,169.0,Pop music transformer: Generating music with rhythm and harmony,https://scholar.google.com/scholar?q=Pop%20music%20transformer%3A%20Generating%20music%20with%20rhythm%20and%20harmony,169,42.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
164,164,170.0,170.0,170.0,170.0,170.0,"Pop music transformer: Beat-based modeling and generation of expressive pop
piano compositions",https://scholar.google.com/scholar?q=Pop%20music%20transformer%3A%20Beat-based%20modeling%20and%20generation%20of%20expressive%20pop%0Apiano%20compositions,170,178.0,https://dl.acm.org/doi/pdf/10.1145/3394171.3413671?casa_token=te-ff9L_vfUAAAAA:1ygyB3iIWZBp3T358m2VqV-YTXfDFpehcXzUTIjjPUKXFbFkHJNKkphZKJb2fxo1GBHF8stDIEa5,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
165,165,171.0,171.0,171.0,171.0,171.0,"Transformer-xl:
Attentive language models beyond a fixed-length context",https://scholar.google.com/scholar?q=Transformer-xl%3A%0AAttentive%20language%20models%20beyond%20a%20fixed-length%20context,171,3332.0,https://arxiv.org/pdf/1901.02860.pdf%3Ffbclid%3DIwAR3nwzQA7VyD36J6u8nEOatG0CeW4FwEU_upvvrgXSES1f0Kd-,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
166,166,172.0,172.0,172.0,172.0,172.0,"Modélisation de l’interprétation des pianistes & applications d’auto-encodeurs sur des modèles
temporels",https://scholar.google.com/scholar?q=Mod%C3%A9lisation%20de%20l%E2%80%99interpr%C3%A9tation%20des%20pianistes%20%26%20applications%20d%E2%80%99auto-encodeurs%20sur%20des%20mod%C3%A8les%0Atemporels,172,5.0,https://papyrus.bib.umontreal.ca/xmlui/bitstream/handle/1866/4426/Lauly_Stanislas_2010_memoire.pdf?sequence=6,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
167,167,173.0,173.0,173.0,173.0,173.0,Neural translation of musical style,https://scholar.google.com/scholar?q=Neural%20translation%20of%20musical%20style,173,52.0,https://arxiv.org/pdf/1708.03535,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
168,168,174.0,174.0,174.0,174.0,174.0,"Rendering music performance with interpretation variations
using conditional variational rnn",https://scholar.google.com/scholar?q=Rendering%20music%20performance%20with%20interpretation%20variations%0Ausing%20conditional%20variational%20rnn,174,15.0,https://archives.ismir.net/ismir2019/paper/000105.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
169,169,175.0,175.0,175.0,175.0,175.0,"Graph neural network for music score data and modeling
expressive piano performance",https://scholar.google.com/scholar?q=Graph%20neural%20network%20for%20music%20score%20data%20and%20modeling%0Aexpressive%20piano%20performance,175,42.0,http://proceedings.mlr.press/v97/jeong19a/jeong19a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
170,170,176.0,176.0,176.0,176.0,176.0,Performance rnn: Generating music with expressive timing and dynamics,https://scholar.google.com/scholar?q=Performance%20rnn%3A%20Generating%20music%20with%20expressive%20timing%20and%20dynamics,176,109.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
171,171,177.0,177.0,177.0,177.0,177.0,"Learning to groove with inverse sequence
transformations",https://scholar.google.com/scholar?q=Learning%20to%20groove%20with%20inverse%20sequence%0Atransformations,177,82.0,http://proceedings.mlr.press/v97/gillick19a/gillick19a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
172,172,178.0,178.0,178.0,178.0,178.0,Groovae: Generating and controlling expressive drum performances,https://scholar.google.com/scholar?q=Groovae%3A%20Generating%20and%20controlling%20expressive%20drum%20performances,178,4.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
173,173,179.0,179.0,179.0,179.0,179.0,"Computational models of expressive
music performance: A comprehensive and critical review",https://scholar.google.com/scholar?q=Computational%20models%20of%20expressive%0Amusic%20performance%3A%20A%20comprehensive%20and%20critical%20review,179,51.0,https://www.frontiersin.org/articles/10.3389/fdigh.2018.00025/full,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
174,174,180.0,180.0,180.0,180.0,180.0,Wavenet: A generative model for raw audio,https://scholar.google.com/scholar?q=Wavenet%3A%20A%20generative%20model%20for%20raw%20audio,180,5974.0,https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&utm_medium=email&utm_campaign=Uncubed%20Entry%20%2361%20-%20April%203%2C%202019&utm_term=entry,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
175,175,181.0,181.0,181.0,181.0,181.0,Parallel wavenet: Fast high-fidelity speech synthesis,https://scholar.google.com/scholar?q=Parallel%20wavenet%3A%20Fast%20high-fidelity%20speech%20synthesis,181,875.0,http://proceedings.mlr.press/v80/oord18a/oord18a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
176,176,182.0,182.0,182.0,182.0,182.0,Efficient neural audio synthesis,https://scholar.google.com/scholar?q=Efficient%20neural%20audio%20synthesis,182,875.0,http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
177,177,183.0,183.0,183.0,183.0,183.0,Neural audio synthesis of musical notes with wavenet autoencoders,https://scholar.google.com/scholar?q=Neural%20audio%20synthesis%20of%20musical%20notes%20with%20wavenet%20autoencoders,183,624.0,http://proceedings.mlr.press/v70/engel17a/engel17a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
178,178,185.0,185.0,185.0,185.0,185.0,Synthesizing audio with generative adversarial networks,https://scholar.google.com/scholar?q=Synthesizing%20audio%20with%20generative%20adversarial%20networks,185,161.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
179,179,186.0,186.0,186.0,186.0,186.0,Adversarial audio synthesis,https://scholar.google.com/scholar?q=Adversarial%20audio%20synthesis,186,640.0,https://arxiv.org/pdf/1802.04208,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
180,180,187.0,187.0,187.0,187.0,187.0,"Gansynth:
Adversarial neural audio synthesis",https://scholar.google.com/scholar?q=Gansynth%3A%0AAdversarial%20neural%20audio%20synthesis,187,448.0,https://arxiv.org/pdf/1902.08710,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
181,181,188.0,188.0,188.0,188.0,188.0,"Singing voice synthesis
based on deep neural networks",https://scholar.google.com/scholar?q=Singing%20voice%20synthesis%0Abased%20on%20deep%20neural%20networks,188,102.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
182,182,189.0,189.0,189.0,189.0,189.0,"Singing voice synthesis based
on generative adversarial networks",https://scholar.google.com/scholar?q=Singing%20voice%20synthesis%20based%0Aon%20generative%20adversarial%20networks,189,61.0,https://ieeexplore.ieee.org/abstract/document/8683154/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
183,183,190.0,190.0,190.0,190.0,190.0,"Korean singing voice synthesis
system based on an lstm recurrent neural network",https://scholar.google.com/scholar?q=Korean%20singing%20voice%20synthesis%0Asystem%20based%20on%20an%20lstm%20recurrent%20neural%20network,190,46.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
184,184,191.0,191.0,191.0,191.0,191.0,"Korean singing voice synthesis based on
auto-regressive boundary equilibrium gan",https://scholar.google.com/scholar?q=Korean%20singing%20voice%20synthesis%20based%20on%0Aauto-regressive%20boundary%20equilibrium%20gan,191,27.0,https://mac.kaist.ac.kr/pubs/ChoiKimParkYongNam-icassp2020.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
185,185,192.0,192.0,192.0,192.0,192.0,"Adversarially trained end-to-end
korean singing voice synthesis system",https://scholar.google.com/scholar?q=Adversarially%20trained%20end-to-end%0Akorean%20singing%20voice%20synthesis%20system,192,81.0,https://arxiv.org/pdf/1908.01919,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
186,186,193.0,193.0,193.0,193.0,193.0,"Bytesing: A chinese singing voice synthesis system using duration allocated encoder-decoder acoustic models and
wavernn vocoders",https://scholar.google.com/scholar?q=Bytesing%3A%20A%20chinese%20singing%20voice%20synthesis%20system%20using%20duration%20allocated%20encoder-decoder%20acoustic%20models%20and%0Awavernn%20vocoders,193,61.0,https://arxiv.org/pdf/2004.11012,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
187,187,194.0,194.0,194.0,194.0,194.0,"Xiaoicesing: A high-quality and integrated singing voice synthesis
system",https://scholar.google.com/scholar?q=Xiaoicesing%3A%20A%20high-quality%20and%20integrated%20singing%20voice%20synthesis%0Asystem,194,68.0,https://arxiv.org/pdf/2006.06261,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
188,188,196.0,196.0,196.0,196.0,196.0,Magenta,https://scholar.google.com/scholar?q=Magenta,196,455.0,https://www.nature.com/articles/cdd201142,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
189,189,197.0,197.0,197.0,197.0,197.0,Flow machines,https://scholar.google.com/scholar?q=Flow%20machines,197,624.0,https://books.google.com/books?hl=en&lr=&id=9s2jBQAAQBAJ&oi=fnd&pg=PP1&dq=Flow+machines&ots=WkMpI39tPo&sig=tqA4Xyq5BpQpRB3gLC5bAeIRqA0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
190,190,198.0,198.0,198.0,198.0,198.0,"Some reflections on the potential and limitations of deep learning
for automated music generation",https://scholar.google.com/scholar?q=Some%20reflections%20on%20the%20potential%20and%20limitations%20of%20deep%20learning%0Afor%20automated%20music%20generation,198,11.0,https://ieeexplore.ieee.org/abstract/document/8581038/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
191,191,199.0,199.0,199.0,199.0,199.0,"The challenge of realistic music generation: modelling
raw audio at scale",https://scholar.google.com/scholar?q=The%20challenge%20of%20realistic%20music%20generation%3A%20modelling%0Araw%20audio%20at%20scale,199,183.0,https://proceedings.neurips.cc/paper/2018/file/3e441eec3456b703a4fe741005f3981f-Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
192,192,200.0,200.0,200.0,200.0,200.0,Midi specifications,https://scholar.google.com/scholar?q=Midi%20specifications,200,164.0,https://books.google.com/books?hl=en&lr=&id=ajDaXh-qgDUC&oi=fnd&pg=PR11&dq=Midi+specifications&ots=EdE2-B_VDG&sig=mWrk8NBU565MzkxkVWwzEmUUhNk,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
193,193,202.0,202.0,202.0,202.0,202.0,"Automatic composition of guitar tabs by
transformers and groove modeling",https://scholar.google.com/scholar?q=Automatic%20composition%20of%20guitar%20tabs%20by%0Atransformers%20and%20groove%20modeling,202,35.0,https://arxiv.org/pdf/2008.01431,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
194,194,203.0,203.0,203.0,203.0,203.0,The rhythmic structure of music,https://scholar.google.com/scholar?q=The%20rhythmic%20structure%20of%20music,203,1832.0,https://books.google.com/books?hl=en&lr=&id=V2yXrIWDTIQC&oi=fnd&pg=PA2&dq=The+rhythmic+structure+of+music&ots=Qw--PQ_PVl&sig=hXvaNPfs2lnu-CaZfiED9BaUnOY,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
195,195,204.0,204.0,204.0,204.0,204.0,"Pianotree vae: Structured
representation learning for polyphonic music",https://scholar.google.com/scholar?q=Pianotree%20vae%3A%20Structured%0Arepresentation%20learning%20for%20polyphonic%20music,204,48.0,https://arxiv.org/pdf/2008.07118,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
196,196,206.0,206.0,206.0,206.0,206.0,"Bandnet: A neural network-based, multi-instrument beatles-style
midi music composition machine",https://scholar.google.com/scholar?q=Bandnet%3A%20A%20neural%20network-based%2C%20multi-instrument%20beatles-style%0Amidi%20music%20composition%20machine,206,17.0,https://arxiv.org/pdf/1812.07126,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
197,197,207.0,207.0,207.0,207.0,207.0,Transformer-nade for piano performances,https://scholar.google.com/scholar?q=Transformer-nade%20for%20piano%20performances,207,2.0,https://www.politesi.polimi.it/bitstream/10589/183481/4/MINNECI%20-%20Executive%20Summary%20%2B%20Thesis.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
198,198,208.0,208.0,208.0,208.0,208.0,Bachprop: Learning to compose music in multiple styles,https://scholar.google.com/scholar?q=Bachprop%3A%20Learning%20to%20compose%20music%20in%20multiple%20styles,208,17.0,https://arxiv.org/pdf/1802.05162,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
199,199,209.0,209.0,209.0,209.0,209.0,Learning adversarial transformer for symbolic music generation,https://scholar.google.com/scholar?q=Learning%20adversarial%20transformer%20for%20symbolic%20music%20generation,209,47.0,https://ieeexplore.ieee.org/abstract/document/9132664/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
200,200,210.0,210.0,210.0,210.0,210.0,"Rhythm, chord and melody generation for lead
sheets using recurrent neural networks",https://scholar.google.com/scholar?q=Rhythm%2C%20chord%20and%20melody%20generation%20for%20lead%0Asheets%20using%20recurrent%20neural%20networks,210,10.0,https://arxiv.org/pdf/2002.10266,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
201,201,212.0,212.0,212.0,212.0,212.0,Deepbach: a steerable model for bach chorales generation,https://scholar.google.com/scholar?q=Deepbach%3A%20a%20steerable%20model%20for%20bach%20chorales%20generation,212,509.0,http://proceedings.mlr.press/v70/hadjeres17a/hadjeres17a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
202,202,213.0,213.0,213.0,213.0,213.0,Learning to traverse latent spaces for musical score inpainting,https://scholar.google.com/scholar?q=Learning%20to%20traverse%20latent%20spaces%20for%20musical%20score%20inpainting,213,56.0,https://arxiv.org/pdf/1907.01164,https://github.com/ashispati/InpaintNetModel,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
203,203,214.0,214.0,214.0,214.0,214.0,Deepj: Style-specific music generation,https://scholar.google.com/scholar?q=Deepj%3A%20Style-specific%20music%20generation,214,134.0,https://arxiv.org/pdf/1801.00887,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
204,204,215.0,215.0,215.0,215.0,215.0,"Coupled recurrent models for polyphonic music
composition",https://scholar.google.com/scholar?q=Coupled%20recurrent%20models%20for%20polyphonic%20music%0Acomposition,215,11.0,https://arxiv.org/pdf/1811.08045,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
205,205,216.0,216.0,216.0,216.0,216.0,"Rl-duet: Online music accompaniment generation using
deep reinforcement learning",https://scholar.google.com/scholar?q=Rl-duet%3A%20Online%20music%20accompaniment%20generation%20using%0Adeep%20reinforcement%20learning,216,44.0,https://ojs.aaai.org/index.php/AAAI/article/download/5413/5269,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
206,206,217.0,217.0,217.0,217.0,217.0,Bachduet: A human-machine duet improvisation system,https://scholar.google.com/scholar?q=Bachduet%3A%20A%20human-machine%20duet%20improvisation%20system,217,4.0,http://labsites.rochester.edu/air/publications/benetatos19bachduet.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
207,207,219.0,219.0,219.0,219.0,219.0,Symbolic music genre transfer with cyclegan,https://scholar.google.com/scholar?q=Symbolic%20music%20genre%20transfer%20with%20cyclegan,219,92.0,https://arxiv.org/pdf/1809.07575,https://github.com/sumuzhao/CycleGAN-Music-Style-Transferraw,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
208,208,220.0,220.0,220.0,220.0,220.0,An unsupervised methodology for musical style translation,https://scholar.google.com/scholar?q=An%20unsupervised%20methodology%20for%20musical%20style%20translation,220,3.0,https://ieeexplore.ieee.org/abstract/document/9023753/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
209,209,222.0,222.0,222.0,222.0,222.0,"Learning interpretable representation for controllable polyphonic
music generation",https://scholar.google.com/scholar?q=Learning%20interpretable%20representation%20for%20controllable%20polyphonic%0Amusic%20generation,222,56.0,https://arxiv.org/pdf/2008.07122,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
210,210,223.0,223.0,223.0,223.0,223.0,"An emotional symbolic music generation system
based on lstm networks",https://scholar.google.com/scholar?q=An%20emotional%20symbolic%20music%20generation%20system%0Abased%20on%20lstm%20networks,223,51.0,https://ieeexplore.ieee.org/abstract/document/8729266/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
211,211,225.0,225.0,225.0,225.0,225.0,Modelling symbolic music: Beyond the piano roll,https://scholar.google.com/scholar?q=Modelling%20symbolic%20music%3A%20Beyond%20the%20piano%20roll,225,34.0,http://proceedings.mlr.press/v63/walder88.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
212,212,226.0,226.0,226.0,226.0,226.0,A first look at music composition using lstm recurrent neural networks,https://scholar.google.com/scholar?q=A%20first%20look%20at%20music%20composition%20using%20lstm%20recurrent%20neural%20networks,226,405.0,https://people.idsia.ch/~juergen/blues/IDSIA-07-02.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
213,213,227.0,227.0,227.0,227.0,227.0,"Conlon: A pseudo-song generator based on a new
pianoroll, wasserstein autoencoders, and optimal interpolations",https://scholar.google.com/scholar?q=Conlon%3A%20A%20pseudo-song%20generator%20based%20on%20a%20new%0Apianoroll%2C%20wasserstein%20autoencoders%2C%20and%20optimal%20interpolations,227,5.0,https://flore.unifi.it/bitstream/2158/1213892/1/2020_ISMIR_CONLON.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
214,214,228.0,228.0,228.0,228.0,228.0,"Modeling temporal tonal relations in polyphonic music through deep
networks with a novel image-based representation",https://scholar.google.com/scholar?q=Modeling%20temporal%20tonal%20relations%20in%20polyphonic%20music%20through%20deep%0Anetworks%20with%20a%20novel%20image-based%20representation,228,38.0,https://ojs.aaai.org/index.php/AAAI/article/view/11880/11739,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
215,215,229.0,229.0,229.0,229.0,229.0,"abc notation home page, accessed on 21/12/2016",https://scholar.google.com/scholar?q=abc%20notation%20home%20page%2C%20accessed%20on%2021/12/2016,229,32.0,https://www.sciencedirect.com/science/article/pii/S0165032719307670?casa_token=RK3MehT2EuAAAAAA:VU5eqD_m-jGffnTRBMvmXFDAzWiKaD6YKtpvJ2SU9MRjCITqjb3IMc13mKY1T7acTPWmuQj6-A,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
216,216,230.0,230.0,230.0,230.0,230.0,"Distributed representations of words and
phrases and their compositionality",https://scholar.google.com/scholar?q=Distributed%20representations%20of%20words%20and%0Aphrases%20and%20their%20compositionality,230,41093.0,https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
217,217,231.0,231.0,231.0,231.0,231.0,"Efficient estimation of word representations in vector
space",https://scholar.google.com/scholar?q=Efficient%20estimation%20of%20word%20representations%20in%20vector%0Aspace,231,38073.0,https://arxiv.org/pdf/1301.3781.pdf%C3%AC%E2%80%94%20%C3%AC%E2%80%9E%C5%93,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
218,218,232.0,232.0,232.0,232.0,232.0,From frequency to meaning: Vector space models of semantics,https://scholar.google.com/scholar?q=From%20frequency%20to%20meaning%3A%20Vector%20space%20models%20of%20semantics,232,3768.0,https://www.jair.org/index.php/jair/article/download/10640/25440/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
219,219,233.0,233.0,233.0,233.0,233.0,"From context to concept: exploring semantic relationships in
music with word2vec",https://scholar.google.com/scholar?q=From%20context%20to%20concept%3A%20exploring%20semantic%20relationships%20in%0Amusic%20with%20word2vec,233,43.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s00521-018-3923-1&casa_token=wbkl2BPLwJ0AAAAA:4WaGBMt_Pz9qeZ6bLdEPSUm-3Xqrac-peHtcXHddSCHiwa4ZcHhgD-WtdHdyOf8_Ua7GvDSqiMrHiybf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
220,220,234.0,234.0,234.0,234.0,234.0,Chord2vec: Learning musical chord embeddings,https://scholar.google.com/scholar?q=Chord2vec%3A%20Learning%20musical%20chord%20embeddings,234,48.0,https://www.researchgate.net/profile/Sephora-Madjiheurem/publication/311452700_Chord2Vec_Learning_Musical_Chord_Embeddings/links/5847098c08ae2d2175703922/Chord2Vec-Learning-Musical-Chord-Embeddings.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
221,221,235.0,235.0,235.0,235.0,235.0,"Chordripple: Recommending chords to help novice
composers go beyond the ordinary",https://scholar.google.com/scholar?q=Chordripple%3A%20Recommending%20chords%20to%20help%20novice%0Acomposers%20go%20beyond%20the%20ordinary,235,72.0,https://dl.acm.org/doi/pdf/10.1145/2856767.2856792?casa_token=Laxun51FFE8AAAAA:RoXIdBLeecLd3d8bb9HO8xglxzysK5msrTd0KXQal-EZ8JnaUR6Xa4aoHSxqE5sLOZ86zHmP_IPB,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
222,222,237.0,237.0,237.0,237.0,237.0,Audio spectrogram representations for processing with convolutional neural networks,https://scholar.google.com/scholar?q=Audio%20spectrogram%20representations%20for%20processing%20with%20convolutional%20neural%20networks,237,202.0,https://arxiv.org/pdf/1706.09559,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
223,223,238.0,238.0,238.0,238.0,238.0,Signal estimation from modified short-time fourier transform,https://scholar.google.com/scholar?q=Signal%20estimation%20from%20modified%20short-time%20fourier%20transform,238,2491.0,http://musicweb.ucsd.edu/~sdubnov/CATbox/Reader/GriffinLimMSTFT.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
224,224,240.0,240.0,240.0,240.0,240.0,Statistical parametric speech synthesis,https://scholar.google.com/scholar?q=Statistical%20parametric%20speech%20synthesis,240,1567.0,https://www.sciencedirect.com/science/article/pii/S0167639309000648?casa_token=nAu2igG3DNkAAAAA:Q7RBWgfsBSDSPR20Nvh7P97xAOR_MNroqNklYu48gXz7l2hRzUSNTYZiwT4RTYlXRpVC6wFTag,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
225,225,241.0,241.0,241.0,241.0,241.0,"A unit selection methodology for music generation using deep neural
networks",https://scholar.google.com/scholar?q=A%20unit%20selection%20methodology%20for%20music%20generation%20using%20deep%20neural%0Anetworks,241,78.0,https://arxiv.org/pdf/1612.03789,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
226,226,242.0,242.0,242.0,242.0,242.0,Midime: Personalizing a musicvae model with user data,https://scholar.google.com/scholar?q=Midime%3A%20Personalizing%20a%20musicvae%20model%20with%20user%20data,242,25.0,https://research.google/pubs/pub48628.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
227,227,243.0,243.0,243.0,243.0,243.0,"Glsr-vae: Geodesic latent space regularization for variational
autoencoder architectures",https://scholar.google.com/scholar?q=Glsr-vae%3A%20Geodesic%20latent%20space%20regularization%20for%20variational%0Aautoencoder%20architectures,243,64.0,https://arxiv.org/pdf/1707.04588,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
228,228,244.0,244.0,244.0,244.0,244.0,Mggan: Solving mode collapse using manifold guided training,https://scholar.google.com/scholar?q=Mggan%3A%20Solving%20mode%20collapse%20using%20manifold%20guided%20training,244,66.0,https://openaccess.thecvf.com/content/ICCV2021W/MELEX/papers/Bang_MGGAN_Solving_Mode_Collapse_Using_Manifold-Guided_Training_ICCVW_2021_paper.pdf,https://github.com/QuickSolverKyle/Tensorflow-,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
229,229,246.0,246.0,246.0,246.0,246.0,"Tuning recurrent neural networks with reinforcement
learning",https://scholar.google.com/scholar?q=Tuning%20recurrent%20neural%20networks%20with%20reinforcement%0Alearning,246,75.0,https://openreview.net/pdf?id=Syyv2e-Kx,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
230,230,247.0,247.0,247.0,247.0,247.0,Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control,https://scholar.google.com/scholar?q=Sequence%20tutor%3A%20Conservative%20fine-tuning%20of%20sequence%20generation%20models%20with%20kl-control,247,158.0,http://proceedings.mlr.press/v70/jaques17a/jaques17a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
231,231,248.0,248.0,248.0,248.0,248.0,Polyphonic music modelling with lstm-rtrbm,https://scholar.google.com/scholar?q=Polyphonic%20music%20modelling%20with%20lstm-rtrbm,248,25.0,https://dl.acm.org/doi/pdf/10.1145/2733373.2806383?casa_token=g6j_ghmOyn8AAAAA:nYWt7P8ZRLQH4mWfsh-mTX7ovfuJYPc1VHwt_gMhk4hyd6x_9UhThDHcnLNGDwcEYpHkp8beROul,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
232,232,249.0,249.0,249.0,249.0,249.0,"Modelling high-dimensional sequences with lstm-rtrbm: Application
to polyphonic music generation",https://scholar.google.com/scholar?q=Modelling%20high-dimensional%20sequences%20with%20lstm-rtrbm%3A%20Application%0Ato%20polyphonic%20music%20generation,249,48.0,https://www.ijcai.org/Proceedings/15/Papers/582.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
233,233,250.0,250.0,250.0,250.0,250.0,The recurrent temporal restricted boltzmann machine,https://scholar.google.com/scholar?q=The%20recurrent%20temporal%20restricted%20boltzmann%20machine,250,576.0,https://proceedings.neurips.cc/paper_files/paper/2008/file/9ad6aaed513b73148b7d49f70afcfb32-Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
234,234,251.0,251.0,251.0,251.0,251.0,"Imposing higher-level structure in polyphonic music generation
using convolutional restricted boltzmann machines and constraints",https://scholar.google.com/scholar?q=Imposing%20higher-level%20structure%20in%20polyphonic%20music%20generation%0Ausing%20convolutional%20restricted%20boltzmann%20machines%20and%20constraints,251,84.0,https://search.informit.org/doi/pdf/10.3316/informit.668426282761995,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
235,235,252.0,252.0,252.0,252.0,252.0,Variational recurrent auto-encoders,https://scholar.google.com/scholar?q=Variational%20recurrent%20auto-encoders,252,327.0,https://arxiv.org/pdf/1412.6581,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
236,236,254.0,254.0,254.0,254.0,254.0,Mahlernet: Unbounded orchestral music with neural networks,https://scholar.google.com/scholar?q=Mahlernet%3A%20Unbounded%20orchestral%20music%20with%20neural%20networks,254,5.0,https://www.diva-portal.org/smash/get/diva2:1391379/FULLTEXT01.pdf,https://github.com/fast-reflexes/MahlerNetcurrent,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
237,237,255.0,255.0,255.0,255.0,255.0,"Music fadernets: Controllable music generation based on high-level features via
low-level feature modelling",https://scholar.google.com/scholar?q=Music%20fadernets%3A%20Controllable%20music%20generation%20based%20on%20high-level%20features%20via%0Alow-level%20feature%20modelling,255,57.0,https://arxiv.org/pdf/2007.15474,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
238,238,257.0,257.0,257.0,257.0,257.0,Musenet,https://scholar.google.com/scholar?q=Musenet,257,23.0,https://ieeexplore.ieee.org/abstract/document/6984896/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
239,239,259.0,259.0,259.0,259.0,259.0,allan2005,https://scholar.google.com/scholar?q=allan2005,259,48.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=fe50097738743fc40b0e860f03a6d8d4ff9a43c6,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
240,240,262.0,262.0,262.0,262.0,262.0,Improving polyphonic music models with feature-rich encoding,https://scholar.google.com/scholar?q=Improving%20polyphonic%20music%20models%20with%20feature-rich%20encoding,262,15.0,https://arxiv.org/pdf/1911.11775,https://github.com/czhuang/JSB-Chorales-dataset3,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
241,241,263.0,263.0,263.0,263.0,263.0,"Part-invariant model for music generation and
harmonization",https://scholar.google.com/scholar?q=Part-invariant%20model%20for%20music%20generation%20and%0Aharmonization,263,19.0,https://www.researchgate.net/profile/Yujia-Yan/publication/328737123_Part-invariant_Model_For_Music_Generation_And_Harmonization/links/5bdfb4e2299bf1124fbb800a/Part-invariant-Model-For-Music-Generation-And-Harmonization.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
242,242,265.0,265.0,265.0,265.0,265.0,"Live orchestral piano, a system for real-time orchestral music generation",https://scholar.google.com/scholar?q=Live%20orchestral%20piano%2C%20a%20system%20for%20real-time%20orchestral%20music%20generation,265,23.0,https://arxiv.org/pdf/1609.01203,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
243,243,266.0,266.0,266.0,266.0,266.0,Song from pi: A musically plausible network for pop music generation,https://scholar.google.com/scholar?q=Song%20from%20pi%3A%20A%20musically%20plausible%20network%20for%20pop%20music%20generation,266,161.0,https://arxiv.org/pdf/1611.03477,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
244,244,267.0,267.0,267.0,267.0,267.0,Automatic music generation and machine learning based evaluation,https://scholar.google.com/scholar?q=Automatic%20music%20generation%20and%20machine%20learning%20based%20evaluation,267,8.0,https://www.researchgate.net/profile/Young-Min-Kang-2/publication/302206040_Automatic_Music_Generation_and_Machine_Learning_Based_Evaluation/links/5bc79871a6fdcc03c78aaff2/Automatic-Music-Generation-and-Machine-Learning-Based-Evaluation.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
245,245,268.0,268.0,268.0,268.0,268.0,"Convolutional generative adversarial networks with binary neurons for polyphonic
music generation",https://scholar.google.com/scholar?q=Convolutional%20generative%20adversarial%20networks%20with%20binary%20neurons%20for%20polyphonic%0Amusic%20generation,268,112.0,https://arxiv.org/pdf/1804.09399,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
246,246,269.0,269.0,269.0,269.0,269.0,"A gan model with self-attention mechanism to generate multi-instruments
symbolic music",https://scholar.google.com/scholar?q=A%20gan%20model%20with%20self-attention%20mechanism%20to%20generate%20multi-instruments%0Asymbolic%20music,269,23.0,https://ieeexplore.ieee.org/abstract/document/8852291/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
247,247,270.0,270.0,270.0,270.0,270.0,Learning a latent space of style-aware symbolic music representations by adversarial autoencoders,https://scholar.google.com/scholar?q=Learning%20a%20latent%20space%20of%20style-aware%20symbolic%20music%20representations%20by%20adversarial%20autoencoders,270,4.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
248,248,271.0,271.0,271.0,271.0,271.0,"Musicality-novelty generative adversarial nets for
algorithmic composition",https://scholar.google.com/scholar?q=Musicality-novelty%20generative%20adversarial%20nets%20for%0Aalgorithmic%20composition,271,12.0,https://dl.acm.org/doi/pdf/10.1145/3240508.3240604?casa_token=Jmq8uQj0FZIAAAAA:ktTOQr6qWVnkT3GWNem2rzPo8sDoSka03rvlOGHlfKmfrMW1-DtAu5tjA65I9SBkNfxMVFmBle90,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
249,249,272.0,272.0,272.0,272.0,272.0,Unsupervised feature selection via nonnegative spectral analysis and redundancy control,https://scholar.google.com/scholar?q=Unsupervised%20feature%20selection%20via%20nonnegative%20spectral%20analysis%20and%20redundancy%20control,272,199.0,https://ieeexplore.ieee.org/abstract/document/7271072/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
250,250,273.0,273.0,273.0,273.0,273.0,Mmm: Exploring conditional multi-track music generation with the transformer,https://scholar.google.com/scholar?q=Mmm%3A%20Exploring%20conditional%20multi-track%20music%20generation%20with%20the%20transformer,273,55.0,https://arxiv.org/pdf/2008.06048,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
251,251,274.0,274.0,274.0,274.0,274.0,"Explicitly conditioned melody generation: A case study with
interdependent rnns",https://scholar.google.com/scholar?q=Explicitly%20conditioned%20melody%20generation%3A%20A%20case%20study%20with%0Ainterdependent%20rnns,274,13.0,https://arxiv.org/pdf/1907.05208,https://github.com/bgenchel/Explicitly-Conditioning-Melody-,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
252,252,275.0,275.0,275.0,275.0,275.0,Generating nontrivial melodies for music as a service,https://scholar.google.com/scholar?q=Generating%20nontrivial%20melodies%20for%20music%20as%20a%20service,275,13.0,https://arxiv.org/pdf/1710.02280,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
253,253,276.0,276.0,276.0,276.0,276.0,Xiaoice band: A melody and arrangement generation framework for pop music,https://scholar.google.com/scholar?q=Xiaoice%20band%3A%20A%20melody%20and%20arrangement%20generation%20framework%20for%20pop%20music,276,84.0,https://dl.acm.org/doi/pdf/10.1145/3219819.3220105?casa_token=H6U_tlCWDAcAAAAA:YDIDyvtRpQJj7ecreVQESUto-WQBybWqJuGfwtTLYSSq5FBYaHx1-lAKA7pRFdlMTNmL7ZI_Ol8A,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
254,254,277.0,277.0,277.0,277.0,277.0,Jazzgan: Improvising with generative adversarial networks,https://scholar.google.com/scholar?q=Jazzgan%3A%20Improvising%20with%20generative%20adversarial%20networks,277,32.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
255,255,278.0,278.0,278.0,278.0,278.0,Improv rnn,https://scholar.google.com/scholar?q=Improv%20rnn,278,49.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12926/12774/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
256,256,279.0,279.0,279.0,279.0,279.0,"Bebopnet: Deep neural models for personalized jazz
improvisations",https://scholar.google.com/scholar?q=Bebopnet%3A%20Deep%20neural%20models%20for%20personalized%20jazz%0Aimprovisations,279,19.0,https://program.ismir2020.net/static/final_papers/132.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
257,257,280.0,280.0,280.0,280.0,280.0,"Automatic melody harmonization with triad chords: A comparative
study",https://scholar.google.com/scholar?q=Automatic%20melody%20harmonization%20with%20triad%20chords%3A%20A%20comparative%0Astudy,280,42.0,https://www.tandfonline.com/doi/pdf/10.1080/09298215.2021.1873392?casa_token=-7BhEIcgLlsAAAAA:4t28-llOs8P6DtV8JnZFWHL8BT_LzWia6rx9gVSvhjUqDbli0ixizy4fdOPUwIiev4KPZoaP74T4,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
258,258,281.0,281.0,281.0,281.0,281.0,Chord generation from symbolic melody using blstm networks,https://scholar.google.com/scholar?q=Chord%20generation%20from%20symbolic%20melody%20using%20blstm%20networks,281,76.0,https://arxiv.org/pdf/1712.01011,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
259,259,282.0,282.0,282.0,282.0,282.0,"Functional harmony recognition of symbolic music data with multi-task recurrent neural
networks",https://scholar.google.com/scholar?q=Functional%20harmony%20recognition%20of%20symbolic%20music%20data%20with%20multi-task%20recurrent%20neural%0Anetworks,282,63.0,https://archives.ismir.net/ismir2018/paper/000178.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
260,260,283.0,283.0,283.0,283.0,283.0,"Clstms: A combination of two lstm models to generate chords
accompaniment for symbolic melody",https://scholar.google.com/scholar?q=Clstms%3A%20A%20combination%20of%20two%20lstm%20models%20to%20generate%20chords%0Aaccompaniment%20for%20symbolic%20melody,283,15.0,https://ieeexplore.ieee.org/abstract/document/8735487/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
261,261,285.0,285.0,285.0,285.0,285.0,"Lead sheet generation and arrangement via a hybrid generative
model",https://scholar.google.com/scholar?q=Lead%20sheet%20generation%20and%20arrangement%20via%20a%20hybrid%20generative%0Amodel,285,8.0,https://liuhaumin.github.io/LeadsheetArrangement/pdf/ismir-2018-leadsheet-arrangement.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
262,262,286.0,286.0,286.0,286.0,286.0,"Pop music generation: From
melody to multi-style arrangement",https://scholar.google.com/scholar?q=Pop%20music%20generation%3A%20From%0Amelody%20to%20multi-style%20arrangement,286,21.0,https://dl.acm.org/doi/pdf/10.1145/3374915?casa_token=uiqhDapUSuIAAAAA:y8hZJDI30vCuOkGA5OuTl6v31q0-H5gg1_vuwYaStOwaDCKaHo7mjjGHTky5ch47ijTjsVmqe1J6,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
263,263,287.0,287.0,287.0,287.0,287.0,Approachable music composition with machine learning at scale,https://scholar.google.com/scholar?q=Approachable%20music%20composition%20with%20machine%20learning%20at%20scale,287,5.0,https://chrome.google.com/webstore/detail/google-scholar-button/ldipcbpaocekfooobnbcddclnhejkcpn?hl=en,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
264,264,288.0,288.0,288.0,288.0,288.0,Mysong: automatic accompaniment generation for vocal melodies,https://scholar.google.com/scholar?q=Mysong%3A%20automatic%20accompaniment%20generation%20for%20vocal%20melodies,288,238.0,https://dl.acm.org/doi/pdf/10.1145/1357054.1357169?casa_token=RzQL0s-gtBEAAAAA:PD_RrN-qxgTZ1SEgJSq1Qa4UJxg8UFIbsT6d0eIkQYzqHtcKbUDUCKsmi-B6HQUtPQSbIKFVj5St,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
265,265,289.0,289.0,289.0,289.0,289.0,Hyperscore: a graphical sketchpad for novice composers,https://scholar.google.com/scholar?q=Hyperscore%3A%20a%20graphical%20sketchpad%20for%20novice%20composers,289,133.0,http://www.hyperscore.com/pdfs/Graphical%20Sketchpad.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
266,266,290.0,290.0,290.0,290.0,290.0,Popmag: Pop music accompaniment generation,https://scholar.google.com/scholar?q=Popmag%3A%20Pop%20music%20accompaniment%20generation,290,82.0,https://dl.acm.org/doi/pdf/10.1145/3394171.3413721?casa_token=CbGjOcCYV50AAAAA:g9AdNZq4JKpEDHche1lLm4DJA51H949tyqW-o_YKwcdM_oIrzSUdIZtMxoA0VMAQYypsL_eEWsJ_,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
267,267,291.0,291.0,291.0,291.0,291.0,Generating structured drum pattern using variational autoencoder and selfsimilarity matrix,https://scholar.google.com/scholar?q=Generating%20structured%20drum%20pattern%20using%20variational%20autoencoder%20and%20selfsimilarity%20matrix,291,17.0,https://www.researchgate.net/profile/I-Chieh-Wei/publication/329317191_Online_Music_Performance_Tracking_Using_Parallel_Dynamic_Time_Warping/links/5de08360a6fdcc2837f3e376/Online-Music-Performance-Tracking-Using-Parallel-Dynamic-Time-Warping.pdf,https://github.com/Sma1033/drum_generation_with_ssmFigure,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
268,268,292.0,292.0,292.0,292.0,292.0,"Combining lstm
and feed forward neural networks for conditional rhythm composition",https://scholar.google.com/scholar?q=Combining%20lstm%0Aand%20feed%20forward%20neural%20networks%20for%20conditional%20rhythm%20composition,292,55.0,http://users.ionio.gr/~karydis/my_papers/MKPKarydisK2017%20-%20Combining%20LSTM%20and%20Feed%20Forward%20Neural%20Networks%20for%20Conditional%20Rhythm%20Composition.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
269,269,293.0,293.0,293.0,293.0,293.0,"Deepdrum: An adaptive conditional
neural network",https://scholar.google.com/scholar?q=Deepdrum%3A%20An%20adaptive%20conditional%0Aneural%20network,293,7.0,https://arxiv.org/pdf/1809.06127,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
270,270,294.0,294.0,294.0,294.0,294.0,"Virtuosonet: A hierarchical rnn-based system
for modeling expressive piano performance",https://scholar.google.com/scholar?q=Virtuosonet%3A%20A%20hierarchical%20rnn-based%20system%0Afor%20modeling%20expressive%20piano%20performance,294,41.0,https://archives.ismir.net/ismir2019/paper/000112.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
271,271,295.0,295.0,295.0,295.0,295.0,"Expression control in singing
voice synthesis: Features, approaches, evaluation, and challenges",https://scholar.google.com/scholar?q=Expression%20control%20in%20singing%0Avoice%20synthesis%3A%20Features%2C%20approaches%2C%20evaluation%2C%20and%20challenges,295,44.0,https://staff.aist.go.jp/m.goto/PAPER/IEEESPM201511umbert.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
272,272,296.0,296.0,296.0,296.0,296.0,Using ai and machine learning to study expressive music performance: Project survey and first report,https://scholar.google.com/scholar?q=Using%20ai%20and%20machine%20learning%20to%20study%20expressive%20music%20performance%3A%20Project%20survey%20and%20first%20report,296,84.0,https://content.iospress.com/articles/ai-communications/aic243,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
273,273,297.0,297.0,297.0,297.0,297.0,"Midi-vae: Modeling dynamics and instrumentation
of music with applications to style transfer",https://scholar.google.com/scholar?q=Midi-vae%3A%20Modeling%20dynamics%20and%20instrumentation%0Aof%20music%20with%20applications%20to%20style%20transfer,297,139.0,https://arxiv.org/pdf/1809.07600,https://github.com/brunnergino/MIDI-VAE7,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
274,274,298.0,298.0,298.0,298.0,298.0,"Score and performance features for rendering expressive
music performances",https://scholar.google.com/scholar?q=Score%20and%20performance%20features%20for%20rendering%20expressive%0Amusic%20performances,298,10.0,https://mac.kaist.ac.kr/pubs/JeongKwonKimNam-mec2019.pdf,https://github.com/jdasam/pyScoreParserMusic,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
275,275,299.0,299.0,299.0,299.0,299.0,Deep piano performance rendering with conditional vae,https://scholar.google.com/scholar?q=Deep%20piano%20performance%20rendering%20with%20conditional%20vae,299,8.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
276,276,300.0,300.0,300.0,300.0,300.0,"The nes music database: A multi-instrumental dataset with
expressive performance attributes",https://scholar.google.com/scholar?q=The%20nes%20music%20database%3A%20A%20multi-instrumental%20dataset%20with%0Aexpressive%20performance%20attributes,300,28.0,https://arxiv.org/pdf/1806.04278,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
277,277,301.0,301.0,301.0,301.0,301.0,Learning to create piano performances,https://scholar.google.com/scholar?q=Learning%20to%20create%20piano%20performances,301,25.0,https://nips2017creativity.github.io/doc/Learning_Piano.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
278,278,302.0,302.0,302.0,302.0,302.0,"Encoding musical style with
transformer autoencoders",https://scholar.google.com/scholar?q=Encoding%20musical%20style%20with%0Atransformer%20autoencoders,302,75.0,http://proceedings.mlr.press/v119/choi20b/choi20b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
279,279,303.0,303.0,303.0,303.0,303.0,"Conditional image generation
with pixelcnn decoders",https://scholar.google.com/scholar?q=Conditional%20image%20generation%0Awith%20pixelcnn%20decoders,303,2383.0,https://proceedings.neurips.cc/paper_files/paper/2016/file/b1301141feffabac455e1f90a7de2054-Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
280,280,305.0,305.0,305.0,305.0,305.0,"Synthnet: Learning to synthesize music
end-to-end",https://scholar.google.com/scholar?q=Synthnet%3A%20Learning%20to%20synthesize%20music%0Aend-to-end,305,4.0,https://www.researchgate.net/profile/Florin-Schimbinschi/publication/334843711_SynthNet_Learning_to_Synthesize_Music_End-to-End/links/5d6f67fd45851542789f8f76/SynthNet-Learning-to-Synthesize-Music-End-to-End.pdf,https://github.com/ﬂorinsch/synthnet,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
281,281,306.0,306.0,306.0,306.0,306.0,"Generating albums with samplernn to imitate metal, rock, and punk bands",https://scholar.google.com/scholar?q=Generating%20albums%20with%20samplernn%20to%20imitate%20metal%2C%20rock%2C%20and%20punk%20bands,306,31.0,https://arxiv.org/pdf/1811.06633,https://github.com/ZVK/sampleRNN,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
282,282,307.0,307.0,307.0,307.0,307.0,Fast and flexible neural audio synthesis,https://scholar.google.com/scholar?q=Fast%20and%20flexible%20neural%20audio%20synthesis,307,29.0,https://archives.ismir.net/ismir2019/paper/000063.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
283,283,309.0,309.0,309.0,309.0,309.0,"Music generation and transformation with moment matching-scattering inverse
networks",https://scholar.google.com/scholar?q=Music%20generation%20and%20transformation%20with%20moment%20matching-scattering%20inverse%0Anetworks,309,13.0,https://archives.ismir.net/ismir2018/paper/000131.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
284,284,310.0,310.0,310.0,310.0,310.0,Neural discrete representation learning,https://scholar.google.com/scholar?q=Neural%20discrete%20representation%20learning,310,2682.0,https://proceedings.neurips.cc/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
285,285,311.0,311.0,311.0,311.0,311.0,"High-level control of drum track generation using learned patterns of rhythmic
interaction",https://scholar.google.com/scholar?q=High-level%20control%20of%20drum%20track%20generation%20using%20learned%20patterns%20of%20rhythmic%0Ainteraction,311,30.0,https://arxiv.org/pdf/1908.00948,https://github.com/CPJKU/madmom3,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
286,286,313.0,313.0,313.0,313.0,313.0,Melnet: A generative model for audio in the frequency domain,https://scholar.google.com/scholar?q=Melnet%3A%20A%20generative%20model%20for%20audio%20in%20the%20frequency%20domain,313,147.0,https://arxiv.org/pdf/1906.01083.pdf%EF%BC%89%E4%B8%AD%EF%BC%8CFacebook,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
287,287,315.0,315.0,315.0,315.0,315.0,"Vapar synth-a variational parametric model for audio
synthesis",https://scholar.google.com/scholar?q=Vapar%20synth-a%20variational%20parametric%20model%20for%20audio%0Asynthesis,315,9.0,https://arxiv.org/pdf/2004.00001,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
288,288,317.0,317.0,317.0,317.0,317.0,"Singing voice synthesis using deep autoregressive neural
networks for acoustic modeling",https://scholar.google.com/scholar?q=Singing%20voice%20synthesis%20using%20deep%20autoregressive%20neural%0Anetworks%20for%20acoustic%20modeling,317,31.0,https://arxiv.org/pdf/1906.08977,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
289,289,318.0,318.0,318.0,318.0,318.0,"Singing voice synthesis
based on convolutional neural networks",https://scholar.google.com/scholar?q=Singing%20voice%20synthesis%0Abased%20on%20convolutional%20neural%20networks,318,36.0,https://arxiv.org/pdf/1904.06868,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
290,290,319.0,319.0,319.0,319.0,319.0,Recent development of the dnn-based singing voice synthesis system—sinsy,https://scholar.google.com/scholar?q=Recent%20development%20of%20the%20dnn-based%20singing%20voice%20synthesis%20system%E2%80%94sinsy,319,37.0,http://www.apsipa.org/proceedings/2018/pdfs/0001003.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
291,291,320.0,320.0,320.0,320.0,320.0,"A neural parametric singing synthesizer modeling timbre and expression from
natural songs",https://scholar.google.com/scholar?q=A%20neural%20parametric%20singing%20synthesizer%20modeling%20timbre%20and%20expression%20from%0Anatural%20songs,320,112.0,https://www.mdpi.com/2076-3417/7/12/1313/pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
292,292,322.0,322.0,322.0,322.0,322.0,"Generative moment
matching network-based random modulation post-filter for dnn-based singing voice synthesis and neural doubletracking",https://scholar.google.com/scholar?q=Generative%20moment%0Amatching%20network-based%20random%20modulation%20post-filter%20for%20dnn-based%20singing%20voice%20synthesis%20and%20neural%20doubletracking,322,9.0,https://arxiv.org/pdf/1902.03389,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
293,293,323.0,323.0,323.0,323.0,323.0,Conditional generative adversarial nets,https://scholar.google.com/scholar?q=Conditional%20generative%20adversarial%20nets,323,11387.0,https://arxiv.org/pdf/1411.1784.pdf%EF%BC%88CGAN%EF%BC%89,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
294,294,325.0,325.0,325.0,325.0,325.0,Adversarially trained multi-singer sequence-to-sequence singing synthesizer,https://scholar.google.com/scholar?q=Adversarially%20trained%20multi-singer%20sequence-to-sequence%20singing%20synthesizer,325,26.0,https://arxiv.org/pdf/2006.10317,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
295,295,326.0,326.0,326.0,326.0,326.0,High fidelity speech synthesis with adversarial networks,https://scholar.google.com/scholar?q=High%20fidelity%20speech%20synthesis%20with%20adversarial%20networks,326,91.0,https://arxiv.org/pdf/1909.11646,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
296,296,327.0,327.0,327.0,327.0,327.0,Sequence-to-sequence singing synthesis using the feed-forward transformer,https://scholar.google.com/scholar?q=Sequence-to-sequence%20singing%20synthesis%20using%20the%20feed-forward%20transformer,327,50.0,https://arxiv.org/pdf/1910.09989,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
297,297,329.0,329.0,329.0,329.0,329.0,"Multi-singer singing voice synthesis
system",https://scholar.google.com/scholar?q=Multi-singer%20singing%20voice%20synthesis%0Asystem,329,17.0,https://arxiv.org/pdf/1910.13069,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
298,298,330.0,330.0,330.0,330.0,330.0,Disentangling timbre and singing style with multisinger singing synthesis system,https://scholar.google.com/scholar?q=Disentangling%20timbre%20and%20singing%20style%20with%20multisinger%20singing%20synthesis%20system,330,17.0,https://arxiv.org/pdf/1910.13069,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
299,299,331.0,331.0,331.0,331.0,331.0,Score and lyrics-free singing voice generation,https://scholar.google.com/scholar?q=Score%20and%20lyrics-free%20singing%20voice%20generation,331,25.0,https://arxiv.org/pdf/1912.11747,"https://github.com/fatchord/WaveRNNfollowing,https://github.com/mathigatti/midi2voice,https://github.com/marl/crepe",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
300,300,332.0,332.0,332.0,332.0,332.0,"Unconditional audio generation with generative
adversarial networks and cycle regularization",https://scholar.google.com/scholar?q=Unconditional%20audio%20generation%20with%20generative%0Aadversarial%20networks%20and%20cycle%20regularization,332,30.0,https://arxiv.org/pdf/2005.08526.pdf%C2%A0%C2%A0,https://github.com/keums/melodyExtraction_,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
301,301,333.0,333.0,333.0,333.0,333.0,"Peking opera synthesis
via duration informed attention network",https://scholar.google.com/scholar?q=Peking%20opera%20synthesis%0Avia%20duration%20informed%20attention%20network,333,6.0,https://arxiv.org/pdf/2008.03029,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
302,302,334.0,334.0,334.0,334.0,334.0,"Conditioning deep generative raw audio models for
structured automatic music",https://scholar.google.com/scholar?q=Conditioning%20deep%20generative%20raw%20audio%20models%20for%0Astructured%20automatic%20music,334,49.0,https://arxiv.org/pdf/1806.09905,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
303,303,335.0,335.0,335.0,335.0,335.0,Enabling factorized piano music modeling and generation with the maestro dataset,https://scholar.google.com/scholar?q=Enabling%20factorized%20piano%20music%20modeling%20and%20generation%20with%20the%20maestro%20dataset,335,396.0,https://arxiv.org/pdf/1810.12247,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
304,304,336.0,336.0,336.0,336.0,336.0,Onsets and frames: Dual-objective piano transcription,https://scholar.google.com/scholar?q=Onsets%20and%20frames%3A%20Dual-objective%20piano%20transcription,336,291.0,https://arxiv.org/pdf/1710.11153,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
305,305,337.0,337.0,337.0,337.0,337.0,"An improved relative self-attention mechanism for transformer with application
to music generation",https://scholar.google.com/scholar?q=An%20improved%20relative%20self-attention%20mechanism%20for%20transformer%20with%20application%0Ato%20music%20generation,337,71.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
306,306,338.0,338.0,338.0,338.0,338.0,"Neural music synthesis for flexible timbre
control",https://scholar.google.com/scholar?q=Neural%20music%20synthesis%20for%20flexible%20timbre%0Acontrol,338,39.0,https://arxiv.org/pdf/1811.00223,https://github.com/NVIDIA/nv-wavenet3,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
307,307,341.0,341.0,341.0,341.0,341.0,Musical instrument sound morphing guided by perceptually motivated features,https://scholar.google.com/scholar?q=Musical%20instrument%20sound%20morphing%20guided%20by%20perceptually%20motivated%20features,341,31.0,http://articles.ircam.fr/textes/Caetano13a/index.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
308,308,342.0,342.0,342.0,342.0,342.0,"Musical composition style transfer via disentangled
timbre representations",https://scholar.google.com/scholar?q=Musical%20composition%20style%20transfer%20via%20disentangled%0Atimbre%20representations,342,37.0,https://arxiv.org/pdf/1905.13567,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
309,309,343.0,343.0,343.0,343.0,343.0,A bassline generation system based on sequence-to-sequence learning,https://scholar.google.com/scholar?q=A%20bassline%20generation%20system%20based%20on%20sequence-to-sequence%20learning,343,4.0,https://www.academia.edu/download/77838321/nime2019_paper040.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
310,310,344.0,344.0,344.0,344.0,344.0,Smug: Scientific music generator,https://scholar.google.com/scholar?q=Smug%3A%20Scientific%20music%20generator,344,35.0,http://gabbbarros.com/assets/papers/Scirea2015Smug.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
311,311,345.0,345.0,345.0,345.0,345.0,Algorithmic songwriting with alysia,https://scholar.google.com/scholar?q=Algorithmic%20songwriting%20with%20alysia,345,44.0,https://arxiv.org/pdf/1612.01058,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
312,312,346.0,346.0,346.0,346.0,346.0,"Neural melody
composition from lyrics",https://scholar.google.com/scholar?q=Neural%20melody%0Acomposition%20from%20lyrics,346,29.0,https://arxiv.org/pdf/1809.04318,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
313,313,347.0,347.0,347.0,347.0,347.0,Conditional lstm-gan for melody generation from lyrics,https://scholar.google.com/scholar?q=Conditional%20lstm-gan%20for%20melody%20generation%20from%20lyrics,347,114.0,https://dl.acm.org/doi/pdf/10.1145/3424116?casa_token=v45ys9_GR74AAAAA:7zUaJc3E5o5R-q7z8qwHIYUw8Aue0utZ7rD0oXDhfuNCPwlHa4Mc87C2wFP2WQIYKqFoM_sJAvtn,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
314,314,348.0,348.0,348.0,348.0,348.0,"Lyrics-conditioned neural
melody generation",https://scholar.google.com/scholar?q=Lyrics-conditioned%20neural%0Amelody%20generation,348,12.0,https://link.springer.com/chapter/10.1007/978-3-030-37734-2_58,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
315,315,349.0,349.0,349.0,349.0,349.0,"icomposer: An automatic songwriting system for chinese popular
music",https://scholar.google.com/scholar?q=icomposer%3A%20An%20automatic%20songwriting%20system%20for%20chinese%20popular%0Amusic,349,26.0,https://aclanthology.org/N19-4015.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
316,316,350.0,350.0,350.0,350.0,350.0,Image style transfer using convolutional neural networks,https://scholar.google.com/scholar?q=Image%20style%20transfer%20using%20convolutional%20neural%20networks,350,5490.0,https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
317,317,351.0,351.0,351.0,351.0,351.0,Play as you like: Timbre-enhanced multimodal music style transfer,https://scholar.google.com/scholar?q=Play%20as%20you%20like%3A%20Timbre-enhanced%20multimodal%20music%20style%20transfer,351,23.0,https://ojs.aaai.org/index.php/AAAI/article/download/3897/3775,https://github.com/ChienYuLu/Play-As-You-Like-,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
318,318,352.0,352.0,352.0,352.0,352.0,"Transferring the style of homophonic music using recurrent neural networks and
autoregressive model",https://scholar.google.com/scholar?q=Transferring%20the%20style%20of%20homophonic%20music%20using%20recurrent%20neural%20networks%20and%0Aautoregressive%20model,352,19.0,http://ismir2018.ircam.fr/doc/pdfs/107_Paper.pdf,https://github.com/s603122001/Music-Style-Transfervaries,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
319,319,353.0,353.0,353.0,353.0,353.0,Supervised symbolic music style translation using synthetic data,https://scholar.google.com/scholar?q=Supervised%20symbolic%20music%20style%20translation%20using%20synthetic%20data,353,19.0,https://arxiv.org/pdf/1907.02265,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
320,320,354.0,354.0,354.0,354.0,354.0,"Improving automatic jazz melody
generation by transfer learning techniques",https://scholar.google.com/scholar?q=Improving%20automatic%20jazz%20melody%0Ageneration%20by%20transfer%20learning%20techniques,354,21.0,https://arxiv.org/pdf/1908.09484,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
321,321,355.0,355.0,355.0,355.0,355.0,Style transfer of abstract drum patterns using a light-weight hierarchical autoencoder,https://scholar.google.com/scholar?q=Style%20transfer%20of%20abstract%20drum%20patterns%20using%20a%20light-weight%20hierarchical%20autoencoder,355,1.0,https://link.springer.com/chapter/10.1007/978-3-030-31978-6_10,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
322,322,356.0,356.0,356.0,356.0,356.0,"Inspecting and interacting with meaningful music representations
using vae",https://scholar.google.com/scholar?q=Inspecting%20and%20interacting%20with%20meaningful%20music%20representations%0Ausing%20vae,356,12.0,https://arxiv.org/pdf/1904.08842,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
323,323,357.0,357.0,357.0,357.0,357.0,"Deep music analogy via latent
representation disentanglement",https://scholar.google.com/scholar?q=Deep%20music%20analogy%20via%20latent%0Arepresentation%20disentanglement,357,67.0,https://arxiv.org/pdf/1906.03626,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
324,324,358.0,358.0,358.0,358.0,358.0,"Learning to fuse music genres
with generative adversarial dual learning",https://scholar.google.com/scholar?q=Learning%20to%20fuse%20music%20genres%0Awith%20generative%20adversarial%20dual%20learning,358,13.0,https://arxiv.org/pdf/1712.01456,https://github.com/aquastar/fusion,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
325,325,359.0,359.0,359.0,359.0,359.0,Audio texture synthesis and style transfer,https://scholar.google.com/scholar?q=Audio%20texture%20synthesis%20and%20style%20transfer,359,22.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s00521-019-04053-8&casa_token=fsv7SdcwfWIAAAAA:tj1LVjqqAdE4He6cXXt4ZAxSKEEyJ2in_QzFYmfN9DV42WmvljceAVV5LwwvtWAbPcmj77Q6jXer3s7n,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
326,326,360.0,360.0,360.0,360.0,360.0,Time domain neural audio style transfer,https://scholar.google.com/scholar?q=Time%20domain%20neural%20audio%20style%20transfer,360,13.0,https://arxiv.org/pdf/1711.11160,https://github.com/pkmital/neural-audio-style-transfer,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
327,327,361.0,361.0,361.0,361.0,361.0,Modulated variational auto-encoders for many-tomany musical timbre transfer,https://scholar.google.com/scholar?q=Modulated%20variational%20auto-encoders%20for%20many-tomany%20musical%20timbre%20transfer,361,23.0,https://arxiv.org/pdf/1810.00222,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
328,328,364.0,364.0,364.0,364.0,364.0,"Learning disentangled representations for timber and pitch in music
audio",https://scholar.google.com/scholar?q=Learning%20disentangled%20representations%20for%20timber%20and%20pitch%20in%20music%0Aaudio,364,16.0,https://arxiv.org/pdf/1811.03271,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
329,329,365.0,365.0,365.0,365.0,365.0,"Learning disentangled representations of timbre and pitch for
musical instrument sounds using gaussian mixture variational autoencoders",https://scholar.google.com/scholar?q=Learning%20disentangled%20representations%20of%20timbre%20and%20pitch%20for%0Amusical%20instrument%20sounds%20using%20gaussian%20mixture%20variational%20autoencoders,365,43.0,https://arxiv.org/pdf/1906.08152,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
330,330,366.0,366.0,366.0,366.0,366.0,"Timbretron: A wavenet (cyclegan
(cqt (audio))) pipeline for musical timbre transfer",https://scholar.google.com/scholar?q=Timbretron%3A%20A%20wavenet%20%28cyclegan%0A%28cqt%20%28audio%29%29%29%20pipeline%20for%20musical%20timbre%20transfer,366,109.0,https://arxiv.org/pdf/1811.09620,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
331,331,367.0,367.0,367.0,367.0,367.0,"Singan: Singing voice conversion with generative
adversarial networks",https://scholar.google.com/scholar?q=Singan%3A%20Singing%20voice%20conversion%20with%20generative%0Aadversarial%20networks,367,39.0,http://www.apsipa.org/proceedings/2019/pdfs/73.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
332,332,368.0,368.0,368.0,368.0,368.0,Non-parallel many-to-many singing voice conversion by adversarial learning,https://scholar.google.com/scholar?q=Non-parallel%20many-to-many%20singing%20voice%20conversion%20by%20adversarial%20learning,368,3.0,https://ieeexplore.ieee.org/abstract/document/9023357/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
333,333,369.0,369.0,369.0,369.0,369.0,Singing voice conversion with non-parallel data,https://scholar.google.com/scholar?q=Singing%20voice%20conversion%20with%20non-parallel%20data,369,30.0,https://arxiv.org/pdf/1903.04124,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
334,334,370.0,370.0,370.0,370.0,370.0,Unsupervised singing voice conversion,https://scholar.google.com/scholar?q=Unsupervised%20singing%20voice%20conversion,370,57.0,https://arxiv.org/pdf/1904.06590,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
335,335,371.0,371.0,371.0,371.0,371.0,Data efficient voice cloning for neural singing synthesis,https://scholar.google.com/scholar?q=Data%20efficient%20voice%20cloning%20for%20neural%20singing%20synthesis,371,37.0,https://arxiv.org/pdf/1902.07292,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
336,336,372.0,372.0,372.0,372.0,372.0,Zero-shot singing voice conversion,https://scholar.google.com/scholar?q=Zero-shot%20singing%20voice%20conversion,372,13.0,https://archives.ismir.net/ismir2020/paper/000142.pdf,https://github.com/CorentinJ/Real-Time-V,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
337,337,373.0,373.0,373.0,373.0,373.0,"Learning singing
from speech",https://scholar.google.com/scholar?q=Learning%20singing%0Afrom%20speech,373,7.0,https://arxiv.org/pdf/1912.10128.pdf%EF%BB%BF,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
338,338,374.0,374.0,374.0,374.0,374.0,Speech-to-singing conversion in an encoder-decoder framework,https://scholar.google.com/scholar?q=Speech-to-singing%20conversion%20in%20an%20encoder-decoder%20framework,374,13.0,https://arxiv.org/pdf/2002.06595,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
339,339,375.0,375.0,375.0,375.0,375.0,"Speaker-independent spectral mapping for
speech-to-singing conversion",https://scholar.google.com/scholar?q=Speaker-independent%20spectral%20mapping%20for%0Aspeech-to-singing%20conversion,375,12.0,https://www.academia.edu/download/82715109/200.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
340,340,376.0,376.0,376.0,376.0,376.0,"Speech-to-singing synthesis: Converting speaking
voices to singing voices by controlling acoustic features unique to singing voices",https://scholar.google.com/scholar?q=Speech-to-singing%20synthesis%3A%20Converting%20speaking%0Avoices%20to%20singing%20voices%20by%20controlling%20acoustic%20features%20unique%20to%20singing%20voices,376,135.0,https://dspace02.jaist.ac.jp/dspace/bitstream/10119/7808/1/A11719.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
341,341,377.0,377.0,377.0,377.0,377.0,Durian-sc: Duration informed attention network based singing voice conversion system,https://scholar.google.com/scholar?q=Durian-sc%3A%20Duration%20informed%20attention%20network%20based%20singing%20voice%20conversion%20system,377,31.0,https://arxiv.org/pdf/2008.03009,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
342,342,378.0,378.0,378.0,378.0,378.0,"Towards mixed-initiative generation of multi-channel
sequential structure",https://scholar.google.com/scholar?q=Towards%20mixed-initiative%20generation%20of%20multi-channel%0Asequential%20structure,378,10.0,https://openreview.net/pdf?id=Sksy1ckvG,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
343,343,379.0,379.0,379.0,379.0,379.0,Musical agents: A typology and state of the art towards musical metacreation,https://scholar.google.com/scholar?q=Musical%20agents%3A%20A%20typology%20and%20state%20of%20the%20art%20towards%20musical%20metacreation,379,61.0,https://www.tandfonline.com/doi/pdf/10.1080/09298215.2018.1511736?casa_token=wbhU5eFU71oAAAAA:3LRbOigH5-J0d5wINTVdmZjzflHn62dCd6KtuiaS4NhY3DU42sSHql5tY4hUlrxTYBAaUXty9n9J,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
344,344,380.0,380.0,380.0,380.0,380.0,Beyond the cybernetic jam fantasy: The continuator,https://scholar.google.com/scholar?q=Beyond%20the%20cybernetic%20jam%20fantasy%3A%20The%20continuator,380,61.0,https://www.academia.edu/download/67125824/Beyond_the_cybernetic_jam_fantasy_The_co20210505-10484-799lpt.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
345,345,381.0,381.0,381.0,381.0,381.0,Deep music: Towards musical dialogue,https://scholar.google.com/scholar?q=Deep%20music%3A%20Towards%20musical%20dialogue,381,15.0,https://ojs.aaai.org/index.php/AAAI/article/download/10544/10403,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
346,346,382.0,382.0,382.0,382.0,382.0,Ai duet,https://scholar.google.com/scholar?q=Ai%20duet,382,4.0,http://labsites.rochester.edu/air/publications/benetatos19bachduet.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
347,347,384.0,384.0,384.0,384.0,384.0,"Learning latent representations of music to generate
interactive musical palettes",https://scholar.google.com/scholar?q=Learning%20latent%20representations%20of%20music%20to%20generate%0Ainteractive%20musical%20palettes,384,23.0,http://ceur-ws.org/Vol-2068/milc7.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
348,348,386.0,386.0,386.0,386.0,386.0,Assisted lead sheet composition using flowcomposer,https://scholar.google.com/scholar?q=Assisted%20lead%20sheet%20composition%20using%20flowcomposer,386,96.0,https://www.francoispachet.fr/wp-content/uploads/2021/01/roy-16b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
349,349,387.0,387.0,387.0,387.0,387.0,Piano genie,https://scholar.google.com/scholar?q=Piano%20genie,387,41.0,https://dl.acm.org/doi/pdf/10.1145/3301275.3302288,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
350,350,388.0,388.0,388.0,388.0,388.0,"Novice-ai music co-creation via
ai-steering tools for deep generative models",https://scholar.google.com/scholar?q=Novice-ai%20music%20co-creation%20via%0Aai-steering%20tools%20for%20deep%20generative%20models,388,143.0,https://dl.acm.org/doi/pdf/10.1145/3313831.3376739,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
351,351,389.0,389.0,389.0,389.0,389.0,Mind band: a crossmedia ai music composing platform,https://scholar.google.com/scholar?q=Mind%20band%3A%20a%20crossmedia%20ai%20music%20composing%20platform,389,11.0,https://dl.acm.org/doi/pdf/10.1145/3343031.3350610?casa_token=EoDUWV1S1vAAAAAA:Z5jIJYihSYujwu0TcMwlz6w10zvxDLBr-4Bl-b3N5R31N5uWTlb8j7-rr29thyVn2-A-zWq0OkZu,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
352,352,390.0,390.0,390.0,390.0,390.0,Composer4everyone: Automatic music generation with audio motif,https://scholar.google.com/scholar?q=Composer4everyone%3A%20Automatic%20music%20generation%20with%20audio%20motif,390,5.0,https://ieeexplore.ieee.org/abstract/document/8695345/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
353,353,392.0,392.0,392.0,392.0,392.0,"Deepdrummer: Generating
drum loops using deep learning and a human in the loop",https://scholar.google.com/scholar?q=Deepdrummer%3A%20Generating%0Adrum%20loops%20using%20deep%20learning%20and%20a%20human%20in%20the%20loop,392,7.0,https://arxiv.org/pdf/2008.04391,"https://github.com/mila-iqia/DeepDrummerThe,https://github.com/mila-iqia/DeepDrummer",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
354,354,393.0,393.0,393.0,393.0,393.0,Musical audio synthesis using autoencoding neural nets,https://scholar.google.com/scholar?q=Musical%20audio%20synthesis%20using%20autoencoding%20neural%20nets,393,58.0,https://research.gold.ac.uk/id/eprint/17628/1/AndySarroffMichaelCaseyICMC2014.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
355,355,394.0,394.0,394.0,394.0,394.0,Improving neural net auto encoders for music synthesis,https://scholar.google.com/scholar?q=Improving%20neural%20net%20auto%20encoders%20for%20music%20synthesis,394,18.0,https://www.aes.org/e-lib/browse.cfm?conv=143&papernum=9846,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
356,356,395.0,395.0,395.0,395.0,395.0,Infilling piano performances,https://scholar.google.com/scholar?q=Infilling%20piano%20performances,395,11.0,https://nips2018creativity.github.io/doc/infilling_piano_performances.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
357,357,396.0,396.0,396.0,396.0,396.0,"Melody completion based on convolutional neural
networks and generative adversarial learning",https://scholar.google.com/scholar?q=Melody%20completion%20based%20on%20convolutional%20neural%0Anetworks%20and%20generative%20adversarial%20learning,396,2.0,https://link.springer.com/chapter/10.1007/978-3-030-03748-2_14,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
358,358,397.0,397.0,397.0,397.0,397.0,"Generating music with a self-correcting
non-chronological autoregressive model",https://scholar.google.com/scholar?q=Generating%20music%20with%20a%20self-correcting%0Anon-chronological%20autoregressive%20model,397,10.0,https://arxiv.org/pdf/2008.08927,"https://github.com/jamesrobertlloyd/inﬁnite-bach,https://github.com/czhuang/JSB-Chorales-datasetWe",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
359,359,398.0,398.0,398.0,398.0,398.0,A method for long extrapolation of audio signals,https://scholar.google.com/scholar?q=A%20method%20for%20long%20extrapolation%20of%20audio%20signals,398,52.0,https://www.aes.org/e-lib/browse.cfm?elib=10169,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
360,360,399.0,399.0,399.0,399.0,399.0,"Restoration of a discrete-time signal segment by interpolation based on the left-sided and right-sided
autoregressive parameters",https://scholar.google.com/scholar?q=Restoration%20of%20a%20discrete-time%20signal%20segment%20by%20interpolation%20based%20on%20the%20left-sided%20and%20right-sided%0Aautoregressive%20parameters,399,118.0,https://ieeexplore.ieee.org/abstract/document/502326/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
361,361,400.0,400.0,400.0,400.0,400.0,"Waveform substitution techniques for recovering
missing speech segments in packet voice communications",https://scholar.google.com/scholar?q=Waveform%20substitution%20techniques%20for%20recovering%0Amissing%20speech%20segments%20in%20packet%20voice%20communications,400,309.0,https://ieeexplore.ieee.org/abstract/document/1164984/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
362,362,401.0,401.0,401.0,401.0,401.0,Introducing spain (sparse audio inpainter),https://scholar.google.com/scholar?q=Introducing%20spain%20%28sparse%20audio%20inpainter%29,401,30.0,https://arxiv.org/pdf/1810.13137,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
363,363,402.0,402.0,402.0,402.0,402.0,A context encoder for audio inpainting,https://scholar.google.com/scholar?q=A%20context%20encoder%20for%20audio%20inpainting,402,69.0,https://www.researchgate.net/profile/Andres-Marafioti/publication/336389335_A_Context_Encoder_For_Audio_Inpainting/links/5d9f2dfba6fdcc8fc3459ee3/A-Context-Encoder-For-Audio-Inpainting.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
364,364,403.0,403.0,403.0,403.0,403.0,"Inpainting of long audio segments with
similarity graphs",https://scholar.google.com/scholar?q=Inpainting%20of%20long%20audio%20segments%20with%0Asimilarity%20graphs,403,43.0,https://arxiv.org/pdf/1607.06667,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
365,365,405.0,405.0,405.0,405.0,405.0,"Gacela–a generative adversarial context
encoder for long audio inpainting",https://scholar.google.com/scholar?q=Gacela%E2%80%93a%20generative%20adversarial%20context%0Aencoder%20for%20long%20audio%20inpainting,405,42.0,https://arxiv.org/pdf/2005.05032,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
366,366,406.0,406.0,406.0,406.0,406.0,Learning to generate music with sentiment,https://scholar.google.com/scholar?q=Learning%20to%20generate%20music%20with%20sentiment,406,75.0,https://arxiv.org/pdf/2103.06125,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
367,367,408.0,408.0,408.0,408.0,408.0,Alarm sound recommendation based on music generating system,https://scholar.google.com/scholar?q=Alarm%20sound%20recommendation%20based%20on%20music%20generating%20system,408,1.0,https://eudl.eu/pdf/10.1007/978-3-030-17513-9_7,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
368,368,409.0,409.0,409.0,409.0,409.0,"Sleepy style music through
variational autoencoder",https://scholar.google.com/scholar?q=Sleepy%20style%20music%20through%0Avariational%20autoencoder,409,0.0,https://www.researchgate.net/profile/Pujana-Paliyawan/publication/332538664_Sleepy_Style_Music_through_Variational_Autoencoder/links/5d20c5e0299bf1547c9e8b44/Sleepy-Style-Music-through-Variational-Autoencoder.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
369,369,410.0,410.0,410.0,410.0,410.0,"Musebeat: Experience real-time music
generation in sync with heartbeat",https://scholar.google.com/scholar?q=Musebeat%3A%20Experience%20real-time%20music%0Ageneration%20in%20sync%20with%20heartbeat,410,2.0,https://dl.acm.org/doi/pdf/10.1145/3170427.3186532?casa_token=UjvPwnpZMc0AAAAA:iAM5jVmtq2V7Lp8Q_8bba4mJvvr7_GJlZWTXhVvLgUp-zGfNrguMCixiTramsuiyRMc_f_wWBtbT,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
370,370,411.0,411.0,411.0,411.0,411.0,Rethinking reflexive looper for structured pop music,https://scholar.google.com/scholar?q=Rethinking%20reflexive%20looper%20for%20structured%20pop%20music,411,16.0,https://www.nime.org/proceedings/2017/nime2017_paper0027.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
371,371,412.0,412.0,412.0,412.0,412.0,"music21: A toolkit for computer-aided musicology and symbolic music
data",https://scholar.google.com/scholar?q=music21%3A%20A%20toolkit%20for%20computer-aided%20musicology%20and%20symbolic%20music%0Adata,412,472.0,https://dspace.mit.edu/bitstream/handle/1721.1/84963/Cuthbert_Ariza_ISMIR_2010.pdf?sequence=1&isAllowed=y,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
372,372,413.0,413.0,413.0,413.0,413.0,Interactive deep generative models for symbolic music,https://scholar.google.com/scholar?q=Interactive%20deep%20generative%20models%20for%20symbolic%20music,413,6.0,https://theses.hal.science/tel-02108994/file/2018SORUS027.pdf,"https://github.com/Ghadjeres/DeepBach64,https://github.com/fchollet/keras,https://github.com/pytorch/pytorch",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
373,373,415.0,415.0,415.0,415.0,415.0,"Studio online 3.0: An internet"" killer
application"" for remote access to ircam sounds and processing tools",https://scholar.google.com/scholar?q=Studio%20online%203.0%3A%20An%20internet%22%20killer%0Aapplication%22%20for%20remote%20access%20to%20ircam%20sounds%20and%20processing%20tools,415,51.0,https://hal.science/hal-03112091/document,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
374,374,416.0,416.0,416.0,416.0,416.0,"The nus sung and spoken lyrics corpus: A quantitative
comparison of singing and speech",https://scholar.google.com/scholar?q=The%20nus%20sung%20and%20spoken%20lyrics%20corpus%3A%20A%20quantitative%0Acomparison%20of%20singing%20and%20speech,416,98.0,https://www.researchgate.net/profile/Ye-Wang-25/publication/261277428_The_NUS_sung_and_spoken_lyrics_corpus_A_quantitative_comparison_of_singing_and_speech/links/54bb92100cf253b50e2d0fbc/The-NUS-sung-and-spoken-lyrics-corpus-A-quantitative-comparison-of-singing-and-speech.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
375,375,417.0,417.0,417.0,417.0,417.0,"A database linking piano and orchestral midi
scores with application to automatic projective orchestration",https://scholar.google.com/scholar?q=A%20database%20linking%20piano%20and%20orchestral%20midi%0Ascores%20with%20application%20to%20automatic%20projective%20orchestration,417,12.0,https://arxiv.org/pdf/1810.08611,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
376,376,420.0,420.0,420.0,420.0,420.0,Online database of scores in the humdrum file format,https://scholar.google.com/scholar?q=Online%20database%20of%20scores%20in%20the%20humdrum%20file%20format,420,98.0,https://ismir2005.ismir.net/proceedings/3123.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
377,377,421.0,421.0,421.0,421.0,421.0,"Hybrid long-and short-term models of folk
melodies",https://scholar.google.com/scholar?q=Hybrid%20long-and%20short-term%20models%20of%20folk%0Amelodies,421,8.0,https://www.ismir2015.uma.es/articles/140_Paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
378,378,422.0,422.0,422.0,422.0,422.0,"Minst, a collection of musical sound datasets",https://scholar.google.com/scholar?q=Minst%2C%20a%20collection%20of%20musical%20sound%20datasets,422,624.0,http://proceedings.mlr.press/v70/engel17a/engel17a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
379,379,424.0,424.0,424.0,424.0,424.0,"The gtzan dataset: Its contents, its faults, their effects on evaluation, and its future use",https://scholar.google.com/scholar?q=The%20gtzan%20dataset%3A%20Its%20contents%2C%20its%20faults%2C%20their%20effects%20on%20evaluation%2C%20and%20its%20future%20use,424,139.0,https://arxiv.org/pdf/1306.1461,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
380,380,425.0,425.0,425.0,425.0,425.0,"On the improvement of singing voice separation for monaural recordings
using the mir-1k dataset",https://scholar.google.com/scholar?q=On%20the%20improvement%20of%20singing%20voice%20separation%20for%20monaural%20recordings%0Ausing%20the%20mir-1k%20dataset,425,284.0,https://ieeexplore.ieee.org/abstract/document/5153305/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
381,381,426.0,426.0,426.0,426.0,426.0,"Vocal
activity informed singing voice separation with the ikala dataset",https://scholar.google.com/scholar?q=Vocal%0Aactivity%20informed%20singing%20voice%20separation%20with%20the%20ikala%20dataset,426,133.0,https://ieeexplore.ieee.org/abstract/document/7178063/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
382,382,427.0,427.0,427.0,427.0,427.0,Learning features of music from scratch,https://scholar.google.com/scholar?q=Learning%20features%20of%20music%20from%20scratch,427,203.0,https://arxiv.org/pdf/1611.09827,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
383,383,428.0,428.0,428.0,428.0,428.0,"Pop909: A pop-song
dataset for music arrangement generation",https://scholar.google.com/scholar?q=Pop909%3A%20A%20pop-song%0Adataset%20for%20music%20arrangement%20generation,428,85.0,https://arxiv.org/pdf/2008.07142,https://github.com/music-x-lab/POP909-,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
384,384,432.0,432.0,432.0,432.0,432.0,On the evaluation of generative models in music,https://scholar.google.com/scholar?q=On%20the%20evaluation%20of%20generative%20models%20in%20music,432,151.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s00521-018-3849-7%3Fwt_mc%3DInternal.Event.1.SEM.ArticleAuthorOnlineFirst%26utm_source%3DArticleAuthorOnlineFirst%26utm_medium%3Demail%26utm_content%3DAA_en_06082018%26ArticleAuthorOnlineFirst_20181106&casa_token=kNZ3Tqztui4AAAAA:XhxTeJ6_id6OKHrsrLLmDwaZkqS82clFfbwtxUc76ur0TWnb9b3w1TdrbMfbJ6jbssXiC9w9XtAcVh3Z,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
385,385,433.0,433.0,433.0,433.0,433.0,"High-fidelity audio generation and representation learning
with guided adversarial autoencoder",https://scholar.google.com/scholar?q=High-fidelity%20audio%20generation%20and%20representation%20learning%0Awith%20guided%20adversarial%20autoencoder,433,9.0,https://ieeexplore.ieee.org/iel7/6287639/6514899/09272282.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
386,386,435.0,435.0,435.0,435.0,435.0,On the generalized distance in statistics,https://scholar.google.com/scholar?q=On%20the%20generalized%20distance%20in%20statistics,435,11818.0,https://www.jstor.org/stable/48723335,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
387,387,436.0,436.0,436.0,436.0,436.0,"Learning to create jazz melodies using a product of
experts",https://scholar.google.com/scholar?q=Learning%20to%20create%20jazz%20melodies%20using%20a%20product%20of%0Aexperts,436,25.0,https://computationalcreativity.net/iccc2017/ICCC_17_accepted_submissions/ICCC-17_paper_62.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
388,388,437.0,437.0,437.0,437.0,437.0,Guided music synthesis with variable markov oracle,https://scholar.google.com/scholar?q=Guided%20music%20synthesis%20with%20variable%20markov%20oracle,437,52.0,https://ojs.aaai.org/index.php/AIIDE/article/download/12767/12615,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
389,389,438.0,438.0,438.0,438.0,438.0,"Rethinking recurrent latent variable model for music
composition",https://scholar.google.com/scholar?q=Rethinking%20recurrent%20latent%20variable%20model%20for%20music%0Acomposition,438,16.0,https://arxiv.org/pdf/1810.03226,https://github.com/skokoh/c_vrnn_mmsp_2018MelodiesTotal,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
390,390,439.0,439.0,439.0,439.0,439.0,"Rethinking recurrent latent variable model for music
composition",https://scholar.google.com/scholar?q=Rethinking%20recurrent%20latent%20variable%20model%20for%20music%0Acomposition,439,16.0,https://arxiv.org/pdf/1810.03226,https://github.com/skokoh/c_vrnn_mmsp_2018MelodiesTotal,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
391,391,440.0,440.0,440.0,440.0,440.0,"The effect of explicit structure encoding of deep neural
networks for symbolic music generation",https://scholar.google.com/scholar?q=The%20effect%20of%20explicit%20structure%20encoding%20of%20deep%20neural%0Anetworks%20for%20symbolic%20music%20generation,440,52.0,https://arxiv.org/pdf/1811.08380,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
392,392,441.0,441.0,441.0,441.0,441.0,Music style transformer,https://scholar.google.com/scholar?q=Music%20style%20transformer,441,75.0,http://proceedings.mlr.press/v119/choi20b/choi20b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
393,393,442.0,442.0,442.0,442.0,442.0,"Music information processing using the humdrum toolkit: Concepts, examples, and lessons",https://scholar.google.com/scholar?q=Music%20information%20processing%20using%20the%20humdrum%20toolkit%3A%20Concepts%2C%20examples%2C%20and%20lessons,442,190.0,https://www.jstor.org/stable/pdf/3681454.pdf?casa_token=XlEmIKMQEUwAAAAA:fzCxAzUu1xfMuexSgOa2R41bbapLHWugmc9YzNssZbmT9l8cCTUEovpilBhQAckKggVuJGidNXWyW4Bh5jZFT3DgAuvaNO6cWYtmzskpVGrnNVya-Cs,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
394,394,443.0,443.0,443.0,443.0,443.0,What’s key for key? the krumhansl-schmuckler key-finding algorithm reconsidered,https://scholar.google.com/scholar?q=What%E2%80%99s%20key%20for%20key%3F%20the%20krumhansl-schmuckler%20key-finding%20algorithm%20reconsidered,443,231.0,https://scholar.archive.org/work/aahkn2faynevjk3zp3dg2jk5y4/access/wayback/http://theory.esm.rochester.edu/temperley/papers/temperley-mp99.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
395,395,444.0,444.0,444.0,444.0,444.0,"A segment-based fitness measure for capturing repetitive structures
of music recordings",https://scholar.google.com/scholar?q=A%20segment-based%20fitness%20measure%20for%20capturing%20repetitive%20structures%0Aof%20music%20recordings,444,43.0,https://domino.mpi-inf.mpg.de/intranet/ag4/ag4publ.nsf/3561a79a83e6557ac1256b91004f4bdd/c09736ce6f79c9f2c12579950044b7f4/$FILE/2011_MuellerGroscheJiang_AudioStructure_ISMIR.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
396,396,445.0,445.0,445.0,445.0,445.0,A scape plot representation for visualizing repetitive structures of music recordings,https://scholar.google.com/scholar?q=A%20scape%20plot%20representation%20for%20visualizing%20repetitive%20structures%20of%20music%20recordings,445,38.0,https://www.audiolabs-erlangen.com/content/05-fau/professor/00-mueller/03-publications/2012_MuellerJiang_StructureVisualization_ISMIR.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
397,397,446.0,446.0,446.0,446.0,446.0,"Sm toolbox: Matlab implementations for computing and
enhancing similarity matrices",https://scholar.google.com/scholar?q=Sm%20toolbox%3A%20Matlab%20implementations%20for%20computing%20and%0Aenhancing%20similarity%20matrices,446,22.0,https://www.audiolabs-erlangen.de/content/resources/MIR/00-2019_TutorialFMP_ISMIR/2014_MuellerJiangGrohganz_ToolboxSM_AES.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
398,398,447.0,447.0,447.0,447.0,447.0,"Anticipatory model of musical style imitation using collaborative
and competitive reinforcement learning",https://scholar.google.com/scholar?q=Anticipatory%20model%20of%20musical%20style%20imitation%20using%20collaborative%0Aand%20competitive%20reinforcement%20learning,447,52.0,https://inria.hal.science/docs/00/83/90/73/PDF/ArshiaCont_ABIALS06_Chapter.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
399,399,448.0,448.0,448.0,448.0,448.0,"Crowdmos: An approach for crowdsourcing mean
opinion score studies",https://scholar.google.com/scholar?q=Crowdmos%3A%20An%20approach%20for%20crowdsourcing%20mean%0Aopinion%20score%20studies,448,261.0,https://www.microsoft.com/en-us/research/wp-content/uploads/2011/05/0002416.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
400,400,449.0,449.0,449.0,449.0,449.0,Conditional end-to-end audio transforms,https://scholar.google.com/scholar?q=Conditional%20end-to-end%20audio%20transforms,449,38.0,https://arxiv.org/pdf/1804.00047,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
401,401,451.0,451.0,451.0,451.0,451.0,"Modeling self-repetition in music generation using generative adversarial
networks",https://scholar.google.com/scholar?q=Modeling%20self-repetition%20in%20music%20generation%20using%20generative%20adversarial%0Anetworks,451,25.0,https://cs.cmu.edu/~jharsh/papers/music_workshop_paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
402,402,452.0,452.0,452.0,452.0,452.0,A note on the evaluation of generative models,https://scholar.google.com/scholar?q=A%20note%20on%20the%20evaluation%20of%20generative%20models,452,1156.0,https://arxiv.org/pdf/1511.01844,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
403,403,453.0,453.0,453.0,453.0,453.0,"Ai song
contest: Human-ai co-creation in songwriting",https://scholar.google.com/scholar?q=Ai%20song%0Acontest%3A%20Human-ai%20co-creation%20in%20songwriting,453,58.0,https://arxiv.org/pdf/2010.05388,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
404,404,454.0,454.0,454.0,454.0,454.0,"Innovations in studio design and construction in the capitol tower recording studios,",https://scholar.google.com/scholar?q=Innovations%20in%20studio%20design%20and%20construction%20in%20the%20capitol%20tower%20recording%20studios%2C,454,11.0,https://www.aes.org/e-lib/online/browse.cfm?elib=278,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
405,405,455.0,455.0,455.0,455.0,455.0,"An interdisciplinary synthesis of reverberation viewpoints,",https://scholar.google.com/scholar?q=An%20interdisciplinary%20synthesis%20of%20reverberation%20viewpoints%2C,455,144.0,https://www.aes.org/e-lib/browse.cfm?elib=10176,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
406,406,456.0,456.0,456.0,456.0,456.0,"Statistics of natural reverberation enable perceptual separation of sound and space,",https://scholar.google.com/scholar?q=Statistics%20of%20natural%20reverberation%20enable%20perceptual%20separation%20of%20sound%20and%20space%2C,456,141.0,https://www.pnas.org/doi/full/10.1073/pnas.1612524113,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
407,407,457.0,457.0,457.0,457.0,457.0,"Fifty years of artificial reverberation,",https://scholar.google.com/scholar?q=Fifty%20years%20of%20artificial%20reverberation%2C,457,332.0,https://ieeexplore.ieee.org/abstract/document/6161610/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
408,408,458.0,458.0,458.0,458.0,458.0,"More than 50 years of artificial reverberation,",https://scholar.google.com/scholar?q=More%20than%2050%20years%20of%20artificial%20reverberation%2C,458,72.0,https://www.dreams-itn.eu/uploads/files/Valimaki-AES60-keynote.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
409,409,459.0,459.0,459.0,459.0,459.0,"Effect design, part 1: reverberator and other filters,",https://scholar.google.com/scholar?q=Effect%20design%2C%20part%201%3A%20reverberator%20and%20other%20filters%2C,459,206.0,https://freeverb3vst.osdn.jp/doc/EffectDesignPart1.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
410,410,460.0,460.0,460.0,460.0,460.0,"A real-time multichannel room simulator,",https://scholar.google.com/scholar?q=A%20real-time%20multichannel%20room%20simulator%2C,460,186.0,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/content/pdf/10.1007/s005300050111.pdf&casa_token=xb3wnmDqHLIAAAAA:ho5hAmaN_wGprHiep-eJOjzOwN4bk5LBgOird2ZTVAyB3-6jcwWZBq-a7wMd37mYeGRydE_W2FdwsHn0,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
411,411,461.0,461.0,461.0,461.0,461.0,"Efficient synthesis of room acoustics via scattering delay networks,",https://scholar.google.com/scholar?q=Efficient%20synthesis%20of%20room%20acoustics%20via%20scattering%20delay%20networks%2C,461,70.0,https://arxiv.org/pdf/1502.05751,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
412,412,462.0,462.0,462.0,462.0,462.0,"Minimally simple binaural room modeling using a single feedback delay network,",https://scholar.google.com/scholar?q=Minimally%20simple%20binaural%20room%20modeling%20using%20a%20single%20feedback%20delay%20network%2C,462,5.0,https://www.academia.edu/download/57292795/angus_jaes_preprint.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
413,413,464.0,464.0,464.0,464.0,464.0,"An analysis/synthesis approach to real-time artificial reverberation,",https://scholar.google.com/scholar?q=An%20analysis/synthesis%20approach%20to%20real-time%20artificial%20reverberation%2C,464,165.0,https://www.computer.org/csdl/proceedings-article/icassp/1992/00226080/12OmNqIhFXx,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
414,414,465.0,465.0,465.0,465.0,465.0,"Automatic design of feedback delay network reverb parameters for impulse response matching,",https://scholar.google.com/scholar?q=Automatic%20design%20of%20feedback%20delay%20network%20reverb%20parameters%20for%20impulse%20response%20matching%2C,465,7.0,https://www.aes.org/e-lib/online/browse.cfm?elib=18470,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
415,415,466.0,466.0,466.0,466.0,466.0,"Late reverberation synthesis using filtered velvet noise,",https://scholar.google.com/scholar?q=Late%20reverberation%20synthesis%20using%20filtered%20velvet%20noise%2C,466,31.0,https://www.mdpi.com/2076-3417/7/5/483/pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
416,416,467.0,467.0,467.0,467.0,467.0,"Matching artificial reverb settings to unknown room recordings: a recommendation system for reverb plugins,",https://scholar.google.com/scholar?q=Matching%20artificial%20reverb%20settings%20to%20unknown%20room%20recordings%3A%20a%20recommendation%20system%20for%20reverb%20plugins%2C,467,12.0,https://www.cnmat.berkeley.edu/sites/default/files/attachments/2012_Matching_Artificial_Reverb_Settings_to_Recommendation_System.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
417,417,468.0,468.0,468.0,468.0,468.0,"An adaptive backpropagation cascade IIR filter,",https://scholar.google.com/scholar?q=An%20adaptive%20backpropagation%20cascade%20IIR%20filter%2C,468,19.0,https://www.researchgate.net/profile/Martin-Snelgrove/publication/3324352_An_Adaptive_Backpropagation_Cascade_IIR_Filter/links/553519f10cf2222bcc403ac8/An-Adaptive-Backpropagation-Cascade-IIR-Filter.pdf?_sg%5B0%5D=started_experiment_milestone&origin=journalDetail,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
418,418,469.0,469.0,469.0,469.0,469.0,"FIR and IIR synapses, a new neural network architecture for time series modeling,",https://scholar.google.com/scholar?q=FIR%20and%20IIR%20synapses%2C%20a%20new%20neural%20network%20architecture%20for%20time%20series%20modeling%2C,469,314.0,http://andrewback.com/papers/fir_iir_nc.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
419,419,470.0,470.0,470.0,470.0,470.0,"Fast adaptive IIR-MLP neural networks for signal processing applications,",https://scholar.google.com/scholar?q=Fast%20adaptive%20IIR-MLP%20neural%20networks%20for%20signal%20processing%20applications%2C,470,11.0,https://www.researchgate.net/profile/Aurelio-Uncini/publication/2541798_Fast_Adaptive_IIR-MLP_Neural_Networks_for_Signal_Processing_Application/links/53f71baf0cf2fceacc74eec3/Fast-Adaptive-IIR-MLP-Neural-Networks-for-Signal-Processing-Application.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
420,420,471.0,471.0,471.0,471.0,471.0,"Differentiable IIR filters for machine learning application,",https://scholar.google.com/scholar?q=Differentiable%20IIR%20filters%20for%20machine%20learning%20application%2C,471,55.0,https://dafx2020.mdw.ac.at/proceedings/papers/DAFx2020_paper_52.pdf,https://github.com/boris-kuz/differentiable_,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
421,421,472.0,472.0,472.0,472.0,472.0,"Neural parametric equalizer matching using differentiable biquads,",https://scholar.google.com/scholar?q=Neural%20parametric%20equalizer%20matching%20using%20differentiable%20biquads%2C,472,28.0,https://dafx2020.mdw.ac.at/proceedings/papers/DAFx2020_paper_7.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
422,422,473.0,473.0,473.0,473.0,473.0,"Lightweight and interpretable neural modeling of an audio distortion effect using hyperconditioned differentiable biquads,",https://scholar.google.com/scholar?q=Lightweight%20and%20interpretable%20neural%20modeling%20of%20an%20audio%20distortion%20effect%20using%20hyperconditioned%20differentiable%20biquads%2C,473,34.0,https://arxiv.org/pdf/2103.08709,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
423,423,474.0,474.0,474.0,474.0,474.0,"An approach to the approximation problem for nonrecursive digital filters,",https://scholar.google.com/scholar?q=An%20approach%20to%20the%20approximation%20problem%20for%20nonrecursive%20digital%20filters%2C,474,275.0,https://web.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/020_nonrecursive%20design.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
424,424,475.0,475.0,475.0,475.0,475.0,"Designing multichannel reverberators,",https://scholar.google.com/scholar?q=Designing%20multichannel%20reverberators%2C,475,215.0,https://www.jstor.org/stable/pdf/3680358.pdf?casa_token=QUCxmNHKXzYAAAAA:U81YzpJhr_eZtLNtmJvWRmWD8zQlJNnk1fsv31Ystk1p0k13YG11vo_jsUmm2oiSIuBFw8gB7DqbAcS_zRxOJZigcIprwDch9_2nyW3YT5x1LQMQBlM,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
425,425,476.0,476.0,476.0,476.0,476.0,"Synthetic reverberation, part I,",https://scholar.google.com/scholar?q=Synthetic%20reverberation%2C%20part%20I%2C,476,144.0,https://www.aes.org/e-lib/browse.cfm?elib=10176,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
426,426,477.0,477.0,477.0,477.0,477.0,"An empirical evaluation of generic convolutional and recurrent networks for sequence modeling,",https://scholar.google.com/scholar?q=An%20empirical%20evaluation%20of%20generic%20convolutional%20and%20recurrent%20networks%20for%20sequence%20modeling%2C,477,4278.0,https://arxiv.org/pdf/1803.01271.pdf%C3%A3%E2%82%AC%E2%80%9A%C3%A6%C5%93%C2%AC%C3%A6%E2%80%93%E2%80%A1%C3%A5%C2%BC%E2%80%A2%C3%A7%E2%80%9D%C2%A8%C3%A7%E2%80%9D%C2%A8(*)%C3%A8%C2%A1%C2%A8%C3%A7%C2%A4%C2%BA,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
427,427,478.0,478.0,478.0,478.0,478.0,"Efficient neural networks for real-time analog audio effect modeling,",https://scholar.google.com/scholar?q=Efficient%20neural%20networks%20for%20real-time%20analog%20audio%20effect%20modeling%2C,478,22.0,https://www.researchgate.net/profile/Christian-Steinmetz-5/publication/349234676_Efficient_Neural_Networks_for_Real-time_Analog_Audio_Effect_Modeling/links/602ac38ca6fdcc37a82bffa0/Efficient-Neural-Networks-for-Real-time-Analog-Audio-Effect-Modeling.pdf,https://github.com/csteinmetz1/micro-tcnModel,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
428,428,480.0,480.0,480.0,480.0,480.0,"Identification of systems containing linear dynamic and static nonlinear elements,",https://scholar.google.com/scholar?q=Identification%20of%20systems%20containing%20linear%20dynamic%20and%20static%20nonlinear%20elements%2C,480,530.0,https://www.sciencedirect.com/science/article/pii/000510988290022X,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
429,429,481.0,481.0,481.0,481.0,481.0,"Spectral modeling synthesis: A sound analysis/synthesis based on a deterministic plus stochastic decomposition,",https://scholar.google.com/scholar?q=Spectral%20modeling%20synthesis%3A%20A%20sound%20analysis/synthesis%20based%20on%20a%20deterministic%20plus%20stochastic%20decomposition%2C,481,932.0,https://www.jstor.org/stable/pdf/3680788.pdf?casa_token=i9BJCy2r-bsAAAAA:v_m_7Vt0QlEGISuFpaT4n0HHO5BbJ8vBLQ1prrHIBrmF596xaIipqQnRciXHKL60DBTRTBjJ9WTR8WGQyra1eCIJ_SenwVkCryaGg6Krv-4TnEVEFRc,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
430,430,483.0,483.0,483.0,483.0,483.0,"V. Zavalishin, The Art of VA Filter Design. Native Instruments, 2020.",https://scholar.google.com/scholar?q=V.%20Zavalishin%2C%20The%20Art%20of%20VA%20Filter%20Design.%20Native%20Instruments%2C%202020.,483,31.0,https://www.native-instruments.com/fileadmin/ni_media/downloads/pdf/VAFilterDesign_2.1.2.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
431,431,484.0,484.0,484.0,484.0,484.0,"Time-varying filters for musical applications,",https://scholar.google.com/scholar?q=Time-varying%20filters%20for%20musical%20applications%2C,484,17.0,http://www.dafx14.fau.de/papers/dafx14_aaron_wishnick_time_varying_filters_for_.pdf,https://github.com/iZotope/time_varying_,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
432,432,485.0,485.0,485.0,485.0,485.0,"Acoustics — Measurement of room acoustic parameters — Part 2: Reverberation time in ordinary rooms,",https://scholar.google.com/scholar?q=Acoustics%20%E2%80%94%20Measurement%20of%20room%20acoustic%20parameters%20%E2%80%94%20Part%202%3A%20Reverberation%20time%20in%20ordinary%20rooms%2C,485,50.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
433,433,486.0,486.0,486.0,486.0,486.0,"Blind estimation of reverberation time,",https://scholar.google.com/scholar?q=Blind%20estimation%20of%20reverberation%20time%2C,486,250.0,https://www.ee.columbia.edu/~dpwe/papers/Ratnam03-reverb.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
434,434,487.0,487.0,487.0,487.0,487.0,"Blind estimation of reverberation time based on the distribution of signal decay rates,",https://scholar.google.com/scholar?q=Blind%20estimation%20of%20reverberation%20time%20based%20on%20the%20distribution%20of%20signal%20decay%20rates%2C,487,118.0,http://www.ee.ic.ac.uk/naylor/PDFs/Wen2008.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
435,435,488.0,488.0,488.0,488.0,488.0,"Blind reverberation time estimation using a convolutional neural network,",https://scholar.google.com/scholar?q=Blind%20reverberation%20time%20estimation%20using%20a%20convolutional%20neural%20network%2C,488,75.0,https://www.researchgate.net/profile/Hannes-Gamper/publication/327870544_Blind_Reverberation_Time_Estimation_Using_a_Convolutional_Neural_Network/links/5baa69fc45851574f7e62f66/Blind-Reverberation-Time-Estimation-Using-a-Convolutional-Neural-Network.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
436,436,489.0,489.0,489.0,489.0,489.0,"A hybrid method for blind estimation of frequency dependent reverberation time using speech signals,",https://scholar.google.com/scholar?q=A%20hybrid%20method%20for%20blind%20estimation%20of%20frequency%20dependent%20reverberation%20time%20using%20speech%20signals%2C,489,4.0,https://ieeexplore.ieee.org/abstract/document/8682661/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
437,437,490.0,490.0,490.0,490.0,490.0,"Accurate reverberation time control in feedback delay networks,",https://scholar.google.com/scholar?q=Accurate%20reverberation%20time%20control%20in%20feedback%20delay%20networks%2C,490,16.0,http://www.dafx17.eca.ed.ac.uk/papers/DAFx17_paper_11.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
438,438,491.0,491.0,491.0,491.0,491.0,"Improved reverberation time control for feedback delay networks,",https://scholar.google.com/scholar?q=Improved%20reverberation%20time%20control%20for%20feedback%20delay%20networks%2C,491,21.0,https://www.researchgate.net/profile/Karolina-Prawda/publication/335756510_Improved_Reverberation_Time_Control_For_Feedback_Delay_Networks/links/5d79f3fea6fdcc9961c131c3/Improved-Reverberation-Time-Control-For-Feedback-Delay-Networks.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
439,439,492.0,492.0,492.0,492.0,492.0,"A machine-learning approach to application of intelligent artificial reverberation,",https://scholar.google.com/scholar?q=A%20machine-learning%20approach%20to%20application%20of%20intelligent%20artificial%20reverberation%2C,492,31.0,https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/19457/Reiss%20A%20Machine-Learning%20Approach%202017%20Accepted.pdf?sequence=1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
440,440,494.0,494.0,494.0,494.0,494.0,"Filtered noise shaping for time domain room impulse response estimation from reverberant speech,",https://scholar.google.com/scholar?q=Filtered%20noise%20shaping%20for%20time%20domain%20room%20impulse%20response%20estimation%20from%20reverberant%20speech%2C,494,24.0,https://arxiv.org/pdf/2107.07503,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
441,441,495.0,495.0,495.0,495.0,495.0,"J. O. Smith, Mathematics of the Discrete Fourier Transform (DFT). http://www.w3k.org/books/: W3K Publishing, 2007.",https://scholar.google.com/scholar?q=J.%20O.%20Smith%2C%20Mathematics%20of%20the%20Discrete%20Fourier%20Transform%20%28DFT%29.%20http%3A//www.w3k.org/books/%3A%20W3K%20Publishing%2C%202007.,495,2.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
442,442,496.0,496.0,496.0,496.0,496.0,"A perceptual study on velvet noise and its variants at different pulse densities,",https://scholar.google.com/scholar?q=A%20perceptual%20study%20on%20velvet%20noise%20and%20its%20variants%20at%20different%20pulse%20densities%2C,496,44.0,https://ieeexplore.ieee.org/abstract/document/6490018/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
443,443,497.0,497.0,497.0,497.0,497.0,"Reverberation modeling using velvet noise,",https://scholar.google.com/scholar?q=Reverberation%20modeling%20using%20velvet%20noise%2C,497,14.0,https://www.aes.org/e-lib/online/browse.cfm?elib=13941,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
444,444,498.0,498.0,498.0,498.0,498.0,"‘colorless’ artificial reverberation,",https://scholar.google.com/scholar?q=%E2%80%98colorless%E2%80%99%20artificial%20reverberation%2C,498,250.0,https://hajim.rochester.edu/ece/sites/zduan/teaching/ece472/reading/Schroeder_1961.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
445,445,499.0,499.0,499.0,499.0,499.0,"Digital delay networks for designing artificial reverberators,",https://scholar.google.com/scholar?q=Digital%20delay%20networks%20for%20designing%20artificial%20reverberators%2C,499,446.0,https://www.researchgate.net/profile/Jean-Marc-Jot/publication/243779004_Digital_delay_networks_for_designing_artificial_reverberators/links/5c14a926299bf139c7595282/Digital-delay-networks-for-designing-artificial-reverberators.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
446,446,500.0,500.0,500.0,500.0,500.0,"Circulant and elliptic feedback delay networks for artificial reverberation,",https://scholar.google.com/scholar?q=Circulant%20and%20elliptic%20feedback%20delay%20networks%20for%20artificial%20reverberation%2C,500,174.0,https://www.di.univr.it/documenti/ArticoloRivista/allegato/allegato574545.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
447,447,501.0,501.0,501.0,501.0,501.0,"Time-varying feedback matrices in feedback delay networks and their application in artificial reverberation,",https://scholar.google.com/scholar?q=Time-varying%20feedback%20matrices%20in%20feedback%20delay%20networks%20and%20their%20application%20in%20artificial%20reverberation%2C,501,41.0,https://scholar.archive.org/work/buwf5krnhjgpxipnvg4owci52e/access/wayback/https://www.audiolabs-erlangen.de/content/05-fau/professor/00-habets/03-publications/Schlecht2015.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
448,448,502.0,502.0,502.0,502.0,502.0,"Fdntb: The feedback delay network toolbox,",https://scholar.google.com/scholar?q=Fdntb%3A%20The%20feedback%20delay%20network%20toolbox%2C,502,19.0,https://www.researchgate.net/profile/Sebastian-Schlecht/publication/344467473_FDNTB_The_Feedback_Delay_Network_Toolbox/links/5f7a281192851c14bcaec863/FDNTB-The-Feedback-Delay-Network-Toolbox.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
449,449,505.0,505.0,505.0,505.0,505.0,"Openair: An interactive auralization web resource and database,",https://scholar.google.com/scholar?q=Openair%3A%20An%20interactive%20auralization%20web%20resource%20and%20database%2C,505,56.0,https://www.aes.org/e-lib/browse.cfm?elib=15648,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
450,450,506.0,506.0,506.0,506.0,506.0,"Estimation of room acoustic parameters: The ace challenge,",https://scholar.google.com/scholar?q=Estimation%20of%20room%20acoustic%20parameters%3A%20The%20ace%20challenge%2C,506,145.0,https://spiral.imperial.ac.uk/bitstream/10044/1/33384/2/ACE_journ_final_no_header_export.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
451,451,507.0,507.0,507.0,507.0,507.0,"Computational modelling and simulation of acoutic spaces,",https://scholar.google.com/scholar?q=Computational%20modelling%20and%20simulation%20of%20acoutic%20spaces%2C,507,122.0,https://www.aes.org/e-lib/browse.cfm?elib=11119,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
452,452,509.0,509.0,509.0,509.0,509.0,Finding the onset of a room impulse response: straightforward?,https://scholar.google.com/scholar?q=Finding%20the%20onset%20of%20a%20room%20impulse%20response%3A%20straightforward%3F,509,21.0,https://pubs.aip.org/asa/jasa/article/124/4/EL248/980851,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
453,453,510.0,510.0,510.0,510.0,510.0,"Impulse response data augmentation and deep neural networks for blind room acoustic parameter estimation,",https://scholar.google.com/scholar?q=Impulse%20response%20data%20augmentation%20and%20deep%20neural%20networks%20for%20blind%20room%20acoustic%20parameter%20estimation%2C,510,38.0,https://arxiv.org/pdf/1909.03642,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
454,454,511.0,511.0,511.0,511.0,511.0,"Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92),",https://scholar.google.com/scholar?q=Cstr%20vctk%20corpus%3A%20English%20multi-speaker%20corpus%20for%20cstr%20voice%20cloning%20toolkit%20%28version%200.92%29%2C,511,56.0,https://arxiv.org/pdf/2104.01497,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
455,455,512.0,512.0,512.0,512.0,512.0,"Adam: A method for stochastic optimization,",https://scholar.google.com/scholar?q=Adam%3A%20A%20method%20for%20stochastic%20optimization%2C,512,158055.0,https://arxiv.org/pdf/1412.6980.pdf%5D,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
456,456,513.0,513.0,513.0,513.0,513.0,"Measuring room impulse responses: Impact of the decay range on derived room acoustic parameters,",https://scholar.google.com/scholar?q=Measuring%20room%20impulse%20responses%3A%20Impact%20of%20the%20decay%20range%20on%20derived%20room%20acoustic%20parameters%2C,513,88.0,https://research.tue.nl/files/3477262/352481346918469.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
457,457,514.0,514.0,514.0,514.0,514.0,"The just noticeable difference of noise length and reverberation perception,",https://scholar.google.com/scholar?q=The%20just%20noticeable%20difference%20of%20noise%20length%20and%20reverberation%20perception%2C,514,62.0,https://scholar.archive.org/work/roewu7f4djeorjtiggzdqcqhwe/access/wayback/http://www.cuc.edu.cn:80/shengxue/papers/D-03.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
458,458,515.0,515.0,515.0,515.0,515.0,"Quantifying the just noticeable difference of reverberation time with band-limited noise centered around 1000 hz using a transformed up-down adaptive method,",https://scholar.google.com/scholar?q=Quantifying%20the%20just%20noticeable%20difference%20of%20reverberation%20time%20with%20band-limited%20noise%20centered%20around%201000%20hz%20using%20a%20transformed%20up-down%20adaptive%20method%2C,515,28.0,https://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1063&context=archengfacpub,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
459,459,516.0,516.0,516.0,516.0,516.0,"A just noticeable difference in c50 for speech,",https://scholar.google.com/scholar?q=A%20just%20noticeable%20difference%20in%20c50%20for%20speech%2C,516,292.0,https://www.sciencedirect.com/science/article/pii/S0003682X98000759?casa_token=YgoySKBtvmYAAAAA:RRRySlgK596MfbNbeEuJq-Su-Q004rWTuvD8-BvvicmcBiGgCLiGrj1IRSmMwlqruPHiVMLW4g,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
460,460,517.0,517.0,517.0,517.0,517.0,"The just noticeable difference of center time and clarity index in large reverberant spaces,",https://scholar.google.com/scholar?q=The%20just%20noticeable%20difference%20of%20center%20time%20and%20clarity%20index%20in%20large%20reverberant%20spaces%2C,517,106.0,https://www.researchgate.net/profile/Francesco-Martellotta/publication/45648221_The_just_noticeable_difference_of_center_time_and_clarity_index_in_large_reverberant_spaces/links/561bba6308aea80367242c6e/The-just-noticeable-difference-of-center-time-and-clarity-index-in-large-reverberant-spaces.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
461,461,518.0,518.0,518.0,518.0,518.0,"On the minimum audible difference in direct-to-reverberant energy ratio,",https://scholar.google.com/scholar?q=On%20the%20minimum%20audible%20difference%20in%20direct-to-reverberant%20energy%20ratio%2C,518,113.0,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2677334/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
462,462,519.0,519.0,519.0,519.0,519.0,"Method for the subjective assessment of intermediate quality level of audio systems,",https://scholar.google.com/scholar?q=Method%20for%20the%20subjective%20assessment%20of%20intermediate%20quality%20level%20of%20audio%20systems%2C,519,105.0,https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.1534-3-201510-I!!PDF-E.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
463,463,520.0,520.0,520.0,520.0,520.0,"webMUSHRA — a comprehensive framework for web-based listening tests. journal of open research software,",https://scholar.google.com/scholar?q=webMUSHRA%20%E2%80%94%20a%20comprehensive%20framework%20for%20web-based%20listening%20tests.%20journal%20of%20open%20research%20software%2C,520,193.0,https://pdfs.semanticscholar.org/eab2/04af8ab5f40dfceb1490f72d03d308fba043.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
464,464,521.0,521.0,521.0,521.0,521.0,"Converting series biquad filters into delayed parallel form: Application to graphic equalizers,",https://scholar.google.com/scholar?q=Converting%20series%20biquad%20filters%20into%20delayed%20parallel%20form%3A%20Application%20to%20graphic%20equalizers%2C,521,20.0,https://ieeexplore.ieee.org/iel7/78/4359509/08723568.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
465,465,522.0,522.0,522.0,522.0,522.0,"J. O. Smith, Introduction to Digital Filters with Audio Applications. W3K Publishing, 2007",https://scholar.google.com/scholar?q=J.%20O.%20Smith%2C%20Introduction%20to%20Digital%20Filters%20with%20Audio%20Applications.%20W3K%20Publishing%2C%202007,522,686.0,"https://books.google.com/books?hl=en&lr=&id=pC1iCQUAsHEC&oi=fnd&pg=PR15&dq=J.+O.+Smith,+Introduction+to+Digital+Filters+with+Audio+Applications.+W3K+Publishing,+2007&ots=0CvkVY8aA-&sig=u482WgHBGfiHm7LIvT2wX25Ps9o",,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
466,466,523.0,523.0,523.0,523.0,523.0,Fast Spectrogram Inversion Using Multi-Head Convolutional Neural Networks,https://scholar.google.com/scholar?q=Fast%20Spectrogram%20Inversion%20Using%20Multi-Head%20Convolutional%20Neural%20Networks,523,117.0,https://arxiv.org/pdf/1808.06719,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
467,467,524.0,524.0,524.0,524.0,524.0,Speech Analysis and Synthesis by Linear Prediction of the Speech Wave,https://scholar.google.com/scholar?q=Speech%20Analysis%20and%20Synthesis%20by%20Linear%20Prediction%20of%20the%20Speech%20Wave,524,1944.0,https://jontalle.web.engr.illinois.edu/uploads/537.F18/Papers/AtalHauauer71.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
468,468,525.0,525.0,525.0,525.0,525.0,"FIR and IIR Synapses, a New Neural Network Architecture for Time  Series Modeling",https://scholar.google.com/scholar?q=FIR%20and%20IIR%20Synapses%2C%20a%20New%20Neural%20Network%20Architecture%20for%20Time%20%20Series%20Modeling,525,314.0,http://andrewback.com/papers/fir_iir_nc.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
469,469,528.0,528.0,528.0,528.0,528.0,Optimization of cascaded parametric peak and  shelving filters with backpropagation algorithm,https://scholar.google.com/scholar?q=Optimization%20of%20cascaded%20parametric%20peak%20and%20%20shelving%20filters%20with%20backpropagation%20algorithm,528,12.0,https://www.hsu-hh.de/ant/wp-content/uploads/sites/699/2020/09/DAFx2020_paper_318.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
470,470,529.0,529.0,529.0,529.0,529.0,Modeling Consonant-Vowel Coarticulation for Articulatory Speech Synthesis,https://scholar.google.com/scholar?q=Modeling%20Consonant-Vowel%20Coarticulation%20for%20Articulatory%20Speech%20Synthesis,529,180.0,https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0060603,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
471,471,531.0,531.0,531.0,531.0,531.0,RAVE: A variational autoencoder for fast and high-quality neural audio  synthesis,https://scholar.google.com/scholar?q=RAVE%3A%20A%20variational%20autoencoder%20for%20fast%20and%20high-quality%20neural%20audio%20%20synthesis,531,52.0,https://arxiv.org/pdf/2111.05011,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
472,472,532.0,532.0,532.0,532.0,532.0,"Musical Instrument Synthesis and Morphing in Multidimensional  Latent Space Using Variational, Convolutional Recurrent Autoencoders",https://scholar.google.com/scholar?q=Musical%20Instrument%20Synthesis%20and%20Morphing%20in%20Multidimensional%20%20Latent%20Space%20Using%20Variational%2C%20Convolutional%20Recurrent%20Autoencoders,532,5.0,https://trepo.tuni.fi/bitstream/handle/10024/129505/AES_2018_Musical_Instrument_Synthesis_and_Morphing_in_Multidimensional_Latent_Space_Using_Variational_Convolutional_Recurrent_Autoencoders.pdf?sequence=1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
473,473,533.0,533.0,533.0,533.0,533.0,On-line learning algorithms for neural networks with  IIR synapses,https://scholar.google.com/scholar?q=On-line%20learning%20algorithms%20for%20neural%20networks%20with%20%20IIR%20synapses,533,28.0,http://www.uncini.com/research_activity/pdf/047_icnn95.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
474,474,534.0,534.0,534.0,534.0,534.0,Tone Transfer: In-Browser Interactive Neural  Audio Synthesis,https://scholar.google.com/scholar?q=Tone%20Transfer%3A%20In-Browser%20Interactive%20Neural%20%20Audio%20Synthesis,534,8.0,http://ceur-ws.org/Vol-2903/IUI21WS-HAIGEN-3.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
475,475,535.0,535.0,535.0,535.0,535.0,Differentiable Grey-box Modelling  of Phaser Effects using Frame-based Spectral Processing,https://scholar.google.com/scholar?q=Differentiable%20Grey-box%20Modelling%20%20of%20Phaser%20Effects%20using%20Frame-based%20Spectral%20Processing,535,0.0,https://arxiv.org/pdf/2306.01332,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
476,476,539.0,539.0,539.0,539.0,539.0,Voice conversion: Factors responsible for quality,https://scholar.google.com/scholar?q=Voice%20conversion%3A%20Factors%20responsible%20for%20quality,539,107.0,https://ieeexplore.ieee.org/abstract/document/1168479/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
477,477,540.0,540.0,540.0,540.0,540.0,A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems,https://scholar.google.com/scholar?q=A%20Survey%20on%20Recent%20Deep%20Learning-driven%20Singing%20Voice%20Synthesis%20Systems,540,11.0,https://arxiv.org/pdf/2110.02511,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
478,478,541.0,541.0,541.0,541.0,541.0,RTNeural: Fast neural inferencing for real-time systems,https://scholar.google.com/scholar?q=RTNeural%3A%20Fast%20neural%20inferencing%20for%20real-time%20systems,541,10.0,https://arxiv.org/pdf/2106.03037,"https://github.com/tensorﬂow/tensorﬂow,https://github.com/pytorch/pytorchblocks,https://github.com/jatinchowdhury18/RTNeural,https://github.com/xtensor-stack/xsimd,https://github.com/jatinchowdhury18/RTNeural-compare",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
479,479,542.0,542.0,542.0,542.0,542.0,The synthesis of complex audio spectra by means of frequency modulation,https://scholar.google.com/scholar?q=The%20synthesis%20of%20complex%20audio%20spectra%20by%20means%20of%20frequency%20modulation,542,1036.0,http://www-reynal.ensea.fr/docs/amea/Article_Chowning_Synthese_FM.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
480,480,544.0,544.0,544.0,544.0,544.0,"Singing Voice Synthesis: History, Current Work, and Future Directions",https://scholar.google.com/scholar?q=Singing%20Voice%20Synthesis%3A%20History%2C%20Current%20Work%2C%20and%20Future%20Directions,544,88.0,https://www.jstor.org/stable/pdf/3680822.pdf?casa_token=OWibgtHF3ywAAAAA:zZssf2wI8watWUFiRd0lgmh6WppaVEPRZYNxs3IgUgmr5La7JynM3cixAd50aEoMLHIsa99i-BeQ6Cf1nvCxswJA5DgdaBrF7Cxlt9txSeaRq1VhzzQ,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
481,481,545.0,545.0,545.0,545.0,545.0,An algorithm for the machine calculation of complex Fourier series,https://scholar.google.com/scholar?q=An%20algorithm%20for%20the%20machine%20calculation%20of%20complex%20Fourier%20series,545,19357.0,https://community.ams.org/journals/mcom/1965-19-090/S0025-5718-1965-0178586-1/S0025-5718-1965-0178586-1.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
482,482,546.0,546.0,546.0,546.0,546.0,"Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings",https://scholar.google.com/scholar?q=Look%2C%20Listen%2C%20and%20Learn%20More%3A%20Design%20Choices%20for%20Deep%20Audio%20Embeddings,546,304.0,https://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf,"https://github.com/marl/audiosetdl,https://github.com/marl/openl3,https://github.com/marl/l3embedding6",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
483,483,548.0,548.0,548.0,548.0,548.0,Continuous Descriptor-Based Control for Deep Audio Synthesis,https://scholar.google.com/scholar?q=Continuous%20Descriptor-Based%20Control%20for%20Deep%20Audio%20Synthesis,548,3.0,https://arxiv.org/pdf/2302.13542,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
484,484,549.0,549.0,549.0,549.0,549.0,Rigid-Body Sound Synthesis with Differentiable Modal Resonators,https://scholar.google.com/scholar?q=Rigid-Body%20Sound%20Synthesis%20with%20Differentiable%20Modal%20Resonators,549,4.0,https://arxiv.org/pdf/2210.15306,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
485,485,550.0,550.0,550.0,550.0,550.0,The Speaking Machine of Wolfgang von Kempelen,https://scholar.google.com/scholar?q=The%20Speaking%20Machine%20of%20Wolfgang%20von%20Kempelen,550,245.0,https://pubs.aip.org/asa/jasa/article-abstract/22/2/151/700124,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
486,486,551.0,551.0,551.0,551.0,551.0,Spatial Sound Design in a Car Cockpit: Challenges and Perspectives,https://scholar.google.com/scholar?q=Spatial%20Sound%20Design%20in%20a%20Car%20Cockpit%3A%20Challenges%20and%20Perspectives,551,6.0,https://hal.science/hal-03456711/document,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
487,487,552.0,552.0,552.0,552.0,552.0,Finding Structure in Time,https://scholar.google.com/scholar?q=Finding%20Structure%20in%20Time,552,15529.0,https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
488,488,553.0,553.0,553.0,553.0,553.0,GANSynth: Adversarial Neural Audio Synthesis,https://scholar.google.com/scholar?q=GANSynth%3A%20Adversarial%20Neural%20Audio%20Synthesis,553,448.0,https://arxiv.org/pdf/1902.08710,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
489,489,554.0,554.0,554.0,554.0,554.0,GANSynth: Adversarial Neural Audio Synthesis,https://scholar.google.com/scholar?q=GANSynth%3A%20Adversarial%20Neural%20Audio%20Synthesis,554,448.0,https://arxiv.org/pdf/1902.08710,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
490,490,556.0,556.0,556.0,556.0,556.0,Neural audio synthesis of musical notes with WaveNet autoencoders,https://scholar.google.com/scholar?q=Neural%20audio%20synthesis%20of%20musical%20notes%20with%20WaveNet%20autoencoders,556,624.0,http://proceedings.mlr.press/v70/engel17a/engel17a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
491,491,557.0,557.0,557.0,557.0,557.0,Self-supervised Pitch Detection by Inverse Audio Synthesis,https://scholar.google.com/scholar?q=Self-supervised%20Pitch%20Detection%20by%20Inverse%20Audio%20Synthesis,557,30.0,https://openreview.net/pdf?id=RlVTYWhsky7,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
492,492,559.0,559.0,559.0,559.0,559.0,Flow Synthesizer: Universal Audio Synthesizer Control with Normalizing Flows,https://scholar.google.com/scholar?q=Flow%20Synthesizer%3A%20Universal%20Audio%20Synthesizer%20Control%20with%20Normalizing%20Flows,559,34.0,https://www.mdpi.com/2076-3417/10/1/302/pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
493,493,560.0,560.0,560.0,560.0,560.0,Generative Adversarial Nets,https://scholar.google.com/scholar?q=Generative%20Adversarial%20Nets,560,2590.0,https://arxiv.org/pdf/1710.07035,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
494,494,561.0,561.0,561.0,561.0,561.0,Improving Adversarial Waveform Generation Based Singing Voice Conversion with Harmonic Signals,https://scholar.google.com/scholar?q=Improving%20Adversarial%20Waveform%20Generation%20Based%20Singing%20Voice%20Conversion%20with%20Harmonic%20Signals,561,10.0,https://arxiv.org/pdf/2201.10130,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
495,495,563.0,563.0,563.0,563.0,563.0,Fast and Flexible Neural Audio Synthesis,https://scholar.google.com/scholar?q=Fast%20and%20Flexible%20Neural%20Audio%20Synthesis,563,29.0,https://archives.ismir.net/ismir2019/paper/000063.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
496,496,564.0,564.0,564.0,564.0,564.0,Neural Waveshaping Synthesis,https://scholar.google.com/scholar?q=Neural%20Waveshaping%20Synthesis,564,26.0,https://arxiv.org/pdf/2107.05050,https://github.com/acids-ircam/ddsp_pytorch7,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
497,497,565.0,565.0,565.0,565.0,565.0,Sinusoidal Frequency Estimation by Gradient Descent,https://scholar.google.com/scholar?q=Sinusoidal%20Frequency%20Estimation%20by%20Gradient%20Descent,565,7.0,https://arxiv.org/pdf/2210.14476,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
498,498,566.0,566.0,566.0,566.0,566.0,Periodnet: A  Non-Autoregressive Waveform Generation Model with a Structure Separating Periodic and Aperiodic  Components,https://scholar.google.com/scholar?q=Periodnet%3A%20A%20%20Non-Autoregressive%20Waveform%20Generation%20Model%20with%20a%20Structure%20Separating%20Periodic%20and%20Aperiodic%20%20Components,566,17.0,https://arxiv.org/pdf/2102.07786,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
499,499,567.0,567.0,567.0,567.0,567.0,Machine tongues XVI,https://scholar.google.com/scholar?q=Machine%20tongues%20XVI,567,205.0,https://www.jstor.org/stable/pdf/3680541.pdf?casa_token=ffviI-z2WJ4AAAAA:00q-60NlsdJLBEzfbaRwk8-cO2u81M2gBZfnq8tm87yQFRW1iy8rJSDlUUzELrGQfYt_P8Q6v-rlkPkUDG7g1fTJRiXdWGsdGEvn-uYN-O1OTW63Myc,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
500,500,568.0,568.0,568.0,568.0,568.0,Unit selection in a concatenative speech synthesis system using a large  speech database,https://scholar.google.com/scholar?q=Unit%20selection%20in%20a%20concatenative%20speech%20synthesis%20system%20using%20a%20large%20%20speech%20database,568,1919.0,https://era.ed.ac.uk/bitstream/handle/1842/1082/Hunt%201996.pdf?sequence=1&isAllowed=y,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
501,501,569.0,569.0,569.0,569.0,569.0,Deep generative models for musical audio synthesis,https://scholar.google.com/scholar?q=Deep%20generative%20models%20for%20musical%20audio%20synthesis,569,18.0,https://arxiv.org/pdf/2006.06426,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
502,502,571.0,571.0,571.0,571.0,571.0,Action-sound Latency and the  Perceived Quality of Digital Musical Instruments,https://scholar.google.com/scholar?q=Action-sound%20Latency%20and%20the%20%20Perceived%20Quality%20of%20Digital%20Musical%20Instruments,571,40.0,https://online.ucpress.edu/mp/article-pdf/36/1/109/274500/mp_2018_36_1_109.pdf?casa_token=Co9KkIF2nRwAAAAA:B5O8-d772T53LEzGOkgl5gSzfRQaSF-uwqUCkZePHqaoOPa1cqLjc3k8M7GBqYIR8VFFfwus,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
503,503,572.0,572.0,572.0,572.0,572.0,ATT: Attention-based Timbre  Transfer,https://scholar.google.com/scholar?q=ATT%3A%20Attention-based%20Timbre%20%20Transfer,572,6.0,https://ieeexplore.ieee.org/abstract/document/9207146/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
504,504,573.0,573.0,573.0,573.0,573.0,Fftnet: A Real-Time Speaker-Dependent  Neural Vocoder,https://scholar.google.com/scholar?q=Fftnet%3A%20A%20Real-Time%20Speaker-Dependent%20%20Neural%20Vocoder,573,131.0,https://oar.princeton.edu/bitstream/88435/pr1xn8j/1/FftnetRealTimeNeuralVocoder.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
505,505,574.0,574.0,574.0,574.0,574.0,The control-synthesis approach for making expressive  and controllable neural music synthesizers,https://scholar.google.com/scholar?q=The%20control-synthesis%20approach%20for%20making%20expressive%20%20and%20controllable%20neural%20music%20synthesizers,574,13.0,https://www.diva-portal.org/smash/get/diva2:1510072/FULLTEXT01.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
506,506,575.0,575.0,575.0,575.0,575.0,GELP: GAN-Excited Linear Prediction  for Speech Synthesis from Mel-Spectrogram,https://scholar.google.com/scholar?q=GELP%3A%20GAN-Excited%20Linear%20Prediction%20%20for%20Speech%20Synthesis%20from%20Mel-Spectrogram,575,53.0,https://arxiv.org/pdf/1904.03976,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
507,507,576.0,576.0,576.0,576.0,576.0,Efficient Neural Audio Synthesis,https://scholar.google.com/scholar?q=Efficient%20Neural%20Audio%20Synthesis,576,875.0,http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
508,508,577.0,577.0,577.0,577.0,577.0,ISTFTNET: Fast and Lightweight Mel-  Spectrogram Vocoder Incorporating Inverse Short-Time Fourier Transform,https://scholar.google.com/scholar?q=ISTFTNET%3A%20Fast%20and%20Lightweight%20Mel-%20%20Spectrogram%20Vocoder%20Incorporating%20Inverse%20Short-Time%20Fourier%20Transform,577,36.0,https://arxiv.org/pdf/2203.02395,"https://github.com/kan-bayashi/ParallelWaveGAN4,https://github.com/espnet/espnet",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
509,509,578.0,578.0,578.0,578.0,578.0,Differentiable Rendering: A Survey,https://scholar.google.com/scholar?q=Differentiable%20Rendering%3A%20A%20Survey,578,133.0,https://arxiv.org/pdf/2006.12057,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
510,510,579.0,579.0,579.0,579.0,579.0,Differentiable Digital Signal Processing Mixture Model for Synthesis Parameter Extraction from Mixture of Harmonic Sounds,https://scholar.google.com/scholar?q=Differentiable%20Digital%20Signal%20Processing%20Mixture%20Model%20for%20Synthesis%20Parameter%20Extraction%20from%20Mixture%20of%20Harmonic%20Sounds,579,12.0,https://arxiv.org/pdf/2202.00200,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
511,511,580.0,580.0,580.0,580.0,580.0,Concatenative speech synthesis: A review,https://scholar.google.com/scholar?q=Concatenative%20speech%20synthesis%3A%20A%20review,580,43.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=dcb1aefcc8d80c90392fa9b6f2740b4516e8ec44,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
512,512,581.0,581.0,581.0,581.0,581.0,Crepe: A Convolutional Representation for Pitch Estimation,https://scholar.google.com/scholar?q=Crepe%3A%20A%20Convolutional%20Representation%20for%20Pitch%20Estimation,581,316.0,https://arxiv.org/pdf/1802.06182,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
513,513,582.0,582.0,582.0,582.0,582.0,Statistical singing voice conversion with direct waveform modification based on the spectrum differential,https://scholar.google.com/scholar?q=Statistical%20singing%20voice%20conversion%20with%20direct%20waveform%20modification%20based%20on%20the%20spectrum%20differential,582,98.0,https://ahcweb01.naist.jp/papers/conference/2014/201409_INTERSPEECH_Kobayashi_1/201409_INTERSPEECH_Kobayashi_1.paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
514,514,583.0,583.0,583.0,583.0,583.0,HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis,https://scholar.google.com/scholar?q=HiFi-GAN%3A%20Generative%20Adversarial%20Networks%20for%20Efficient%20and%20High%20Fidelity%20Speech%20Synthesis,583,1088.0,https://proceedings.neurips.cc/paper/2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf,"https://github.com/jik876/hiﬁ-gan,https://github.com/NVIDIA/tacotron2,https://github.com/NVIDIA/waveglow",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
515,515,584.0,584.0,584.0,584.0,584.0,DiffWave: A Versatile Diffusion Model for Audio Synthesis,https://scholar.google.com/scholar?q=DiffWave%3A%20A%20Versatile%20Diffusion%20Model%20for%20Audio%20Synthesis,584,636.0,https://arxiv.org/pdf/2009.09761,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
516,516,585.0,585.0,585.0,585.0,585.0,Why is musical timbre so hard to understand,https://scholar.google.com/scholar?q=Why%20is%20musical%20timbre%20so%20hard%20to%20understand,585,383.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
517,517,587.0,587.0,587.0,587.0,587.0,Differentiable IIR filters for machine learning applications,https://scholar.google.com/scholar?q=Differentiable%20IIR%20filters%20for%20machine%20learning%20applications,587,55.0,https://dafx2020.mdw.ac.at/proceedings/papers/DAFx2020_paper_52.pdf,https://github.com/boris-kuz/differentiable_,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
518,518,588.0,588.0,588.0,588.0,588.0,Digital Waveshaping Synthesis,https://scholar.google.com/scholar?q=Digital%20Waveshaping%20Synthesis,588,241.0,https://www.aes.org/e-lib/browse.cfm?elib=3212,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
519,519,589.0,589.0,589.0,589.0,589.0,Differentiable Artificial Reverberation,https://scholar.google.com/scholar?q=Differentiable%20Artificial%20Reverberation,589,27.0,https://arxiv.org/pdf/2105.13940,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
520,520,590.0,590.0,590.0,590.0,590.0,Blind Estimation of Audio Processing Graph,https://scholar.google.com/scholar?q=Blind%20Estimation%20of%20Audio%20Processing%20Graph,590,3.0,https://arxiv.org/pdf/2303.08610,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
521,521,592.0,592.0,592.0,592.0,592.0,Differentiable Signal Processing With Black-Box Audio Effects,https://scholar.google.com/scholar?q=Differentiable%20Signal%20Processing%20With%20Black-Box%20Audio%20Effects,592,27.0,https://arxiv.org/pdf/2105.04752,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
522,522,593.0,593.0,593.0,593.0,593.0,Synthesizer Sound Matching with Differentiable DSP,https://scholar.google.com/scholar?q=Synthesizer%20Sound%20Matching%20with%20Differentiable%20DSP,593,10.0,https://archives.ismir.net/ismir2021/paper/000053.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
523,523,594.0,594.0,594.0,594.0,594.0,Improving Semi-Supervised Differentiable Synthesizer Sound Matching for Practical Applications,https://scholar.google.com/scholar?q=Improving%20Semi-Supervised%20Differentiable%20Synthesizer%20Sound%20Matching%20for%20Practical%20Applications,594,1.0,https://ieeexplore.ieee.org/iel7/6570655/6633080/10017350.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
524,524,596.0,596.0,596.0,596.0,596.0,The Perceptual Representation of Timbre,https://scholar.google.com/scholar?q=The%20Perceptual%20Representation%20of%20Timbre,596,54.0,https://www.mcgill.ca/mpcl/files/mpcl/mcadams_2019_timbreacoustperceptcogn_ch2.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
525,525,598.0,598.0,598.0,598.0,598.0,Approaches in Intelligent Music Production,https://scholar.google.com/scholar?q=Approaches%20in%20Intelligent%20Music%20Production,598,28.0,https://www.mdpi.com/2076-0752/8/4/125/pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
526,526,599.0,599.0,599.0,599.0,599.0,An overview of voice conversion systems,https://scholar.google.com/scholar?q=An%20overview%20of%20voice%20conversion%20systems,599,287.0,https://www.sciencedirect.com/science/article/pii/S0167639315300698?casa_token=dH5iBHHy_dUAAAAA:WjB1i-jOMyitMWWLMMBY0BmIS2ab1H_I8C3XPcrTilNl0WpNprfIDm2Bbb3AftxnCLREC_x2Lw,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
527,527,600.0,600.0,600.0,600.0,600.0,WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications,https://scholar.google.com/scholar?q=WORLD%3A%20A%20Vocoder-Based%20High-Quality%20Speech%20Synthesis%20System%20for%20Real-Time%20Applications,600,1306.0,https://www.jstage.jst.go.jp/article/transinf/E99.D/7/E99.D_2015EDP7457/_pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
528,528,602.0,602.0,602.0,602.0,602.0,Augmentative and alternative communication: A review of current issues,https://scholar.google.com/scholar?q=Augmentative%20and%20alternative%20communication%3A%20A%20review%20of%20current%20issues,602,69.0,https://www.sciencedirect.com/science/article/pii/S1751722209001097?casa_token=G4RUrz-k05gAAAAA:2sDt_o4uOfwWsskLDFWhipWBdW87i_Kh7xC1UHOfreHIS9Rs0iK7vfnbog_z5c-AD0uE5sL65A,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
529,529,603.0,603.0,603.0,603.0,603.0,SFNet: A Computationally Efficient Source Filter Model Based Neural Speech Synthesis,https://scholar.google.com/scholar?q=SFNet%3A%20A%20Computationally%20Efficient%20Source%20Filter%20Model%20Based%20Neural%20Speech%20Synthesis,603,6.0,https://ieeexplore.ieee.org/abstract/document/9126129/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
530,530,604.0,604.0,604.0,604.0,604.0,Neural Parametric Equalizer Matching Using Differentiable Biquads,https://scholar.google.com/scholar?q=Neural%20Parametric%20Equalizer%20Matching%20Using%20Differentiable%20Biquads,604,28.0,https://dafx2020.mdw.ac.at/proceedings/papers/DAFx2020_paper_7.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
531,531,605.0,605.0,605.0,605.0,605.0,End-to-End Zero-Shot Voice Conversion Using a DDSP Vocoder,https://scholar.google.com/scholar?q=End-to-End%20Zero-Shot%20Voice%20Conversion%20Using%20a%20DDSP%20Vocoder,605,9.0,https://ieeexplore.ieee.org/abstract/document/9632754/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
532,532,606.0,606.0,606.0,606.0,606.0,Differentiable WORLD Synthesizer-Based Neural Vocoder With Application To End-To-End Audio Style Transfer,https://scholar.google.com/scholar?q=Differentiable%20WORLD%20Synthesizer-Based%20Neural%20Vocoder%20With%20Application%20To%20End-To-End%20Audio%20Style%20Transfer,606,9.0,https://arxiv.org/pdf/2208.07282,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
533,533,607.0,607.0,607.0,607.0,607.0,Lightweight and Interpretable Neural Modeling of an Audio Distortion Effect Using Hyperconditioned Differentiable Biquads,https://scholar.google.com/scholar?q=Lightweight%20and%20Interpretable%20Neural%20Modeling%20of%20an%20Audio%20Distortion%20Effect%20Using%20Hyperconditioned%20Differentiable%20Biquads,607,34.0,https://arxiv.org/pdf/2103.08709,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
534,534,608.0,608.0,608.0,608.0,608.0,Singing Voice Synthesis Based on Deep Neural Networks,https://scholar.google.com/scholar?q=Singing%20Voice%20Synthesis%20Based%20on%20Deep%20Neural%20Networks,608,102.0,,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
535,535,609.0,609.0,609.0,609.0,609.0,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,https://scholar.google.com/scholar?q=Parallel%20WaveNet%3A%20Fast%20High-Fidelity%20Speech%20Synthesis,609,875.0,http://proceedings.mlr.press/v80/oord18a/oord18a.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
536,536,610.0,610.0,610.0,610.0,610.0,Upsampling Artifacts in Neural Audio Synthesis,https://scholar.google.com/scholar?q=Upsampling%20Artifacts%20in%20Neural%20Audio%20Synthesis,610,53.0,https://arxiv.org/pdf/2010.14356,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
537,537,611.0,611.0,611.0,611.0,611.0,Waveglow: A Flow-based Generative Network for Speech Synthesis,https://scholar.google.com/scholar?q=Waveglow%3A%20A%20Flow-based%20Generative%20Network%20for%20Speech%20Synthesis,611,1049.0,https://arxiv.org/pdf/1811.00002.pdf?source=post_page---------------------------,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
538,538,612.0,612.0,612.0,612.0,612.0,Deep learning for black-box modeling of audio effects,https://scholar.google.com/scholar?q=Deep%20learning%20for%20black-box%20modeling%20of%20audio%20effects,612,46.0,https://www.mdpi.com/2076-3417/10/2/638/pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
539,539,613.0,613.0,613.0,613.0,613.0,A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions,https://scholar.google.com/scholar?q=A%20Comprehensive%20Survey%20of%20Neural%20Architecture%20Search%3A%20Challenges%20and%20Solutions,613,444.0,https://dl.acm.org/doi/pdf/10.1145/3447582?casa_token=I5U278F5SGYAAAAA:VZ90Phvq6h79eFjii0WbuT-9aTYKIadl_mWDyono3aoDL9U1_pYCZuMgvQyzIxEqFvF9Et-xS2oa,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
540,540,614.0,614.0,614.0,614.0,614.0,Differentiable Piano Model for Midi-to-Audio Performance Synthesis,https://scholar.google.com/scholar?q=Differentiable%20Piano%20Model%20for%20Midi-to-Audio%20Performance%20Synthesis,614,6.0,https://hal.science/hal-04073770/document,"https://github.com/lrenault/ddsp-pianofrom,https://github.com/lrenault/ddsp-piano",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
541,541,615.0,615.0,615.0,615.0,615.0,Synthesis and processing of the singing voice,https://scholar.google.com/scholar?q=Synthesis%20and%20processing%20of%20the%20singing%20voice,615,74.0,https://www.esat.kuleuven.be/psi/spraak/seminars/mpca/papers/rodet:mpca02.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
542,542,616.0,616.0,616.0,616.0,616.0,CRASH: Raw Audio Score-based Generative Modeling for Controllable High-resolution Drum Sound Synthesis,https://scholar.google.com/scholar?q=CRASH%3A%20Raw%20Audio%20Score-based%20Generative%20Modeling%20for%20Controllable%20High-resolution%20Drum%20Sound%20Synthesis,616,25.0,https://arxiv.org/pdf/2106.07431,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
543,543,617.0,617.0,617.0,617.0,617.0,An HMM-based singing voice synthesis system,https://scholar.google.com/scholar?q=An%20HMM-based%20singing%20voice%20synthesis%20system,617,154.0,https://www.researchgate.net/profile/Heiga-Zen/publication/221484784_An_HMM-based_singing_voice_synthesis_system/links/548736c80cf2ef34478ec4f1/An-HMM-based-singing-voice-synthesis-system.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
544,544,618.0,618.0,618.0,618.0,618.0,Unsupervised Music Source Separation Using Differentiable Parametric Source Models,https://scholar.google.com/scholar?q=Unsupervised%20Music%20Source%20Separation%20Using%20Differentiable%20Parametric%20Source%20Models,618,9.0,https://ieeexplore.ieee.org/iel7/6570655/6633080/10058592.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
545,545,619.0,619.0,619.0,619.0,619.0,Concatenative sound synthesis: The early years,https://scholar.google.com/scholar?q=Concatenative%20sound%20synthesis%3A%20The%20early%20years,619,166.0,https://www.tandfonline.com/doi/pdf/10.1080/09298210600696857?casa_token=NQePErPAj-EAAAAA:BETYmxDwf7ysX3oqUCiCy20rrw7Wqh0FVVtLfXmgBeqtu39SMYs7mSklVuY5tsBUIbAfTnpWG4v4,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
546,546,620.0,620.0,620.0,620.0,620.0,Corpus-Based Concatenative Synthesis,https://scholar.google.com/scholar?q=Corpus-Based%20Concatenative%20Synthesis,620,177.0,https://www.academia.edu/download/45793336/msp.2007.32327420160519-30523-p7o0cl.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
547,547,621.0,621.0,621.0,621.0,621.0,Automatic generation of control signals for a parallel formant speech synthesizer,https://scholar.google.com/scholar?q=Automatic%20generation%20of%20control%20signals%20for%20a%20parallel%20formant%20speech%20synthesizer,621,31.0,https://ieeexplore.ieee.org/abstract/document/1169987/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
548,548,622.0,622.0,622.0,622.0,622.0,Spectral Modeling Synthesis: A Sound Analysis/Synthesis System Based on a Deterministic Plus Stochastic Decomposition,https://scholar.google.com/scholar?q=Spectral%20Modeling%20Synthesis%3A%20A%20Sound%20Analysis/Synthesis%20System%20Based%20on%20a%20Deterministic%20Plus%20Stochastic%20Decomposition,622,932.0,https://www.jstor.org/stable/pdf/3680788.pdf?casa_token=PppOsILmcUcAAAAA:3SJTk_tZ9MFa9faAjaKjwFUtMJX8RWcZOEIUXRb5mtmqP3wNPdJNGGnSmft7ZgZI5R4KNX7GK-E9LnabwUc78meGrHZNDOGB1kJhFoCdAwcVeQvVNRs,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
549,549,623.0,623.0,623.0,623.0,623.0,Prospects for articulatory synthesis: A position paper,https://scholar.google.com/scholar?q=Prospects%20for%20articulatory%20synthesis%3A%20A%20position%20paper,623,79.0,https://eprints.soton.ac.uk/256064/1/paper.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
550,550,624.0,624.0,624.0,624.0,624.0,Differentiable Wavetable Synthesis,https://scholar.google.com/scholar?q=Differentiable%20Wavetable%20Synthesis,624,29.0,https://arxiv.org/pdf/2111.10003,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
551,551,625.0,625.0,625.0,625.0,625.0,Adaptive IIR filtering,https://scholar.google.com/scholar?q=Adaptive%20IIR%20filtering,625,804.0,https://users.metu.edu.tr/ccandan/EE504_spring2004/notes/shynk_adaptive_iir_filtering.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
552,552,626.0,626.0,626.0,626.0,626.0,Four Distinctions for the Auditory “Wastebasket” of Timbre,https://scholar.google.com/scholar?q=Four%20Distinctions%20for%20the%20Auditory%20%E2%80%9CWastebasket%E2%80%9D%20of%20Timbre,626,58.0,https://www.frontiersin.org/articles/10.3389/fpsyg.2017.01747/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Psychology&id=272043,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
553,553,627.0,627.0,627.0,627.0,627.0,SINGAN: Singing Voice Conversion with Generative Adversarial Networks,https://scholar.google.com/scholar?q=SINGAN%3A%20Singing%20Voice%20Conversion%20with%20Generative%20Adversarial%20Networks,627,39.0,http://www.apsipa.org/proceedings/2019/pdfs/73.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
554,554,628.0,628.0,628.0,628.0,628.0,An Overview of Voice Conversion and Its Challenges: From Statistical Modeling to Deep Learning,https://scholar.google.com/scholar?q=An%20Overview%20of%20Voice%20Conversion%20and%20Its%20Challenges%3A%20From%20Statistical%20Modeling%20to%20Deep%20Learning,628,244.0,https://ieeexplore.ieee.org/iel7/6570655/6633080/09262021.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
555,555,629.0,629.0,629.0,629.0,629.0,Physical Modeling Using Digital Waveguides,https://scholar.google.com/scholar?q=Physical%20Modeling%20Using%20Digital%20Waveguides,629,977.0,https://www.jstor.org/stable/pdf/3680470.pdf?casa_token=5y4Kq57KjGQAAAAA:Xo21mzBj7oo5EzszbSmWcrUX1niaiHjXd-vMozFg9gTVRYn2XE1oVq-JhHI3PUcB_oU8FZYeVBBgQWsE-GKGDnTsrDqXcDM9TTCg-5ynkAHnwn-KO0E,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
556,556,630.0,630.0,630.0,630.0,630.0,DSPGAN: A Gan-Based Universal Vocoder for High-Fidelity TTS by Time-Frequency Domain Supervision from DSP,https://scholar.google.com/scholar?q=DSPGAN%3A%20A%20Gan-Based%20Universal%20Vocoder%20for%20High-Fidelity%20TTS%20by%20Time-Frequency%20Domain%20Supervision%20from%20DSP,630,4.0,https://arxiv.org/pdf/2211.01087,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
557,557,631.0,631.0,631.0,631.0,631.0,An overview of the simultaneous perturbation method for efficient optimization,https://scholar.google.com/scholar?q=An%20overview%20of%20the%20simultaneous%20perturbation%20method%20for%20efficient%20optimization,631,755.0,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=cb1a5c699c5359b8e65f1f4d207b5c8cd8266b15,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
558,558,632.0,632.0,632.0,632.0,632.0,Speaker Generation,https://scholar.google.com/scholar?q=Speaker%20Generation,632,24.0,https://arxiv.org/pdf/2111.05095,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
559,559,633.0,633.0,633.0,633.0,633.0,Style Transfer of Audio Effects with Differentiable Signal Processing,https://scholar.google.com/scholar?q=Style%20Transfer%20of%20Audio%20Effects%20with%20Differentiable%20Signal%20Processing,633,20.0,https://arxiv.org/pdf/2207.08759,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
560,560,635.0,635.0,635.0,635.0,635.0,auraloss: Audio focused loss functions in PyTorch,https://scholar.google.com/scholar?q=auraloss%3A%20Audio%20focused%20loss%20functions%20in%20PyTorch,635,32.0,http://eecs.qmul.ac.uk/~josh/documents/2020/DMRN15__auraloss__Audio_focused_loss_functions_in_PyTorch.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
561,561,636.0,636.0,636.0,636.0,636.0,Voice Transformation: A survey,https://scholar.google.com/scholar?q=Voice%20Transformation%3A%20A%20survey,636,237.0,http://ayesha.lti.cs.cmu.edu/mlsp/courses/fall2013/lectures/Stylianou_VC.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
562,562,637.0,637.0,637.0,637.0,637.0,End-to-end LPCNet:  A Neural Vocoder With Fully-Differentiable LPC Estimation,https://scholar.google.com/scholar?q=End-to-end%20LPCNet%3A%20%20A%20Neural%20Vocoder%20With%20Fully-Differentiable%20LPC%20Estimation,637,6.0,https://arxiv.org/pdf/2202.11301,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
563,563,638.0,638.0,638.0,638.0,638.0,Speaker-dependent wavenet  vocoder,https://scholar.google.com/scholar?q=Speaker-dependent%20wavenet%20%20vocoder,638,330.0,https://www.researchgate.net/profile/Akira-Tamamori/publication/319184793_Speaker-Dependent_WaveNet_Vocoder/links/59df4dd10f7e9b2dba8310a4/Speaker-Dependent-WaveNet-Vocoder.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
564,564,639.0,639.0,639.0,639.0,639.0,I’m Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at  Pitch,https://scholar.google.com/scholar?q=I%E2%80%99m%20Sorry%20for%20Your%20Loss%3A%20Spectrally-Based%20Audio%20Distances%20Are%20Bad%20at%20%20Pitch,639,23.0,https://arxiv.org/pdf/2012.04572,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
565,565,641.0,641.0,641.0,641.0,641.0,LPCNET: Improving Neural Speech Synthesis through Linear  Prediction,https://scholar.google.com/scholar?q=LPCNET%3A%20Improving%20Neural%20Speech%20Synthesis%20through%20Linear%20%20Prediction,641,452.0,https://arxiv.org/pdf/1810.11846,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
566,566,642.0,642.0,642.0,642.0,642.0,Applying voice conversion to concatenative singing-voice  synthesis,https://scholar.google.com/scholar?q=Applying%20voice%20conversion%20to%20concatenative%20singing-voice%20%20synthesis,642,87.0,https://www.academia.edu/download/36936108/VillavicInterspeech10b.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
567,567,643.0,643.0,643.0,643.0,643.0,Neural Source-filter-based Waveform Model for  Statistical Parametric Speech Synthesis,https://scholar.google.com/scholar?q=Neural%20Source-filter-based%20Waveform%20Model%20for%20%20Statistical%20Parametric%20Speech%20Synthesis,643,146.0,https://arxiv.org/pdf/1810.11946,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
568,568,644.0,644.0,644.0,644.0,644.0,Neural Source-Filter Waveform Models for Statistical  Parametric Speech Synthesis,https://scholar.google.com/scholar?q=Neural%20Source-Filter%20Waveform%20Models%20for%20Statistical%20%20Parametric%20Speech%20Synthesis,644,134.0,https://arxiv.org/pdf/1904.12088,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
569,569,645.0,645.0,645.0,645.0,645.0,Neural Harmonic-plus-Noise Waveform Model with Trainable  Maximum Voice Frequency for Text-to-Speech Synthesis,https://scholar.google.com/scholar?q=Neural%20Harmonic-plus-Noise%20Waveform%20Model%20with%20Trainable%20%20Maximum%20Voice%20Frequency%20for%20Text-to-Speech%20Synthesis,645,35.0,https://arxiv.org/pdf/1908.10256,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
570,570,646.0,646.0,646.0,646.0,646.0,Opencpop: A High-Quality Open  Source Chinese Popular Song Corpus for Singing Voice Synthesis,https://scholar.google.com/scholar?q=Opencpop%3A%20A%20High-Quality%20Open%20%20Source%20Chinese%20Popular%20Song%20Corpus%20for%20Singing%20Voice%20Synthesis,646,33.0,https://arxiv.org/pdf/2201.07429,https://github.com/MontrealCorpusTools/Montreal-Forced-,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
571,571,647.0,647.0,647.0,647.0,647.0,PUFFIN: Pitch-Synchronous Neural  Waveform Generation for Fullband Speech on Modest Devices,https://scholar.google.com/scholar?q=PUFFIN%3A%20Pitch-Synchronous%20Neural%20%20Waveform%20Generation%20for%20Fullband%20Speech%20on%20Modest%20Devices,647,2.0,https://arxiv.org/pdf/2211.14130,https://github.com/jik876/hifi-ganTable,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
572,572,649.0,649.0,649.0,649.0,649.0,DDSP- based Singing Vocoders: A New Subtractive-based Synthesizer and A Comprehensive Evaluation,https://scholar.google.com/scholar?q=DDSP-%20based%20Singing%20Vocoders%3A%20A%20New%20Subtractive-based%20Synthesizer%20and%20A%20Comprehensive%20Evaluation,649,10.0,https://arxiv.org/pdf/2208.04756,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
573,573,650.0,650.0,650.0,650.0,650.0,Generating Detailed Music Datasets with Neural Audio Synthesis,https://scholar.google.com/scholar?q=Generating%20Detailed%20Music%20Datasets%20with%20Neural%20Audio%20Synthesis,650,2.0,http://mlasworkshop.com/MLAS_Music_Datasets.pdf,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
574,574,652.0,652.0,652.0,652.0,652.0,Parallel Wavegan: A Fast Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution Spectrogram,https://scholar.google.com/scholar?q=Parallel%20Wavegan%3A%20A%20Fast%20Waveform%20Generation%20Model%20Based%20on%20Generative%20Adversarial%20Networks%20with%20Multi-Resolution%20Spectrogram,652,703.0,https://arxiv.org/pdf/1910.11480,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
575,575,653.0,653.0,653.0,653.0,653.0,NAS-FM: Neural Architecture Search for Tunable and Interpretable Sound Synthesis based on Frequency Modulation,https://scholar.google.com/scholar?q=NAS-FM%3A%20Neural%20Architecture%20Search%20for%20Tunable%20and%20Interpretable%20Sound%20Synthesis%20based%20on%20Frequency%20Modulation,653,0.0,https://arxiv.org/pdf/2305.12868,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
576,576,654.0,654.0,654.0,654.0,654.0,Automatic Programming of VST Sound Synthesizers Using Deep Networks and Other Techniques,https://scholar.google.com/scholar?q=Automatic%20Programming%20of%20VST%20Sound%20Synthesizers%20Using%20Deep%20Networks%20and%20Other%20Techniques,654,52.0,https://research.gold.ac.uk/id/eprint/22516/1/myk_lf_vsti_programming.pdf,"https://github.com/asb2m10/dexedIEEE,https://github.com/tﬂearn/tﬂearnIEEE",,0,0,0,0,0,0,0,0,,[0],0,,0.0,
577,577,655.0,655.0,655.0,655.0,655.0,Embedding a Differentiable Mel-Cepstral Synthesis Filter to a Neural Speech Synthesis System,https://scholar.google.com/scholar?q=Embedding%20a%20Differentiable%20Mel-Cepstral%20Synthesis%20Filter%20to%20a%20Neural%20Speech%20Synthesis%20System,655,3.0,https://arxiv.org/pdf/2211.11222,https://github.com/sp-nitech/diffsptkIn,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
578,578,656.0,656.0,656.0,656.0,656.0,GAN vocoder: Multi-resolution discriminator is all you need,https://scholar.google.com/scholar?q=GAN%20vocoder%3A%20Multi-resolution%20discriminator%20is%20all%20you%20need,656,33.0,https://arxiv.org/pdf/2103.05236,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
579,579,657.0,657.0,657.0,657.0,657.0,DurIAN: Duration Informed Attention Network for Speech Synthesis,https://scholar.google.com/scholar?q=DurIAN%3A%20Duration%20Informed%20Attention%20Network%20for%20Speech%20Synthesis,657,183.0,https://arxiv.org/pdf/1909.01700,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
580,580,658.0,658.0,658.0,658.0,658.0,Statistical parametric speech synthesis,https://scholar.google.com/scholar?q=Statistical%20parametric%20speech%20synthesis,658,1567.0,https://www.sciencedirect.com/science/article/pii/S0167639309000648?casa_token=sust2CBUqQoAAAAA:VfM5ZwKR4D_HKGCCFeSn3BgvQ1ERE3l-1TjPkmyZP5BPpIj3nDTCHfr8iYyrSlijuROF-vAggA,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
581,581,659.0,659.0,659.0,659.0,659.0,Transferring Neural Speech Waveform Synthesizers to Musical Instrument Sounds Generation,https://scholar.google.com/scholar?q=Transferring%20Neural%20Speech%20Waveform%20Synthesizers%20to%20Musical%20Instrument%20Sounds%20Generation,659,17.0,https://arxiv.org/pdf/1910.12381,https://github.com/NVIDIA/waveglowin,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
582,582,660.0,660.0,660.0,660.0,660.0,HiFi-SVC: Fast High Fidelity Cross-Domain Singing Voice Conversion,https://scholar.google.com/scholar?q=HiFi-SVC%3A%20Fast%20High%20Fidelity%20Cross-Domain%20Singing%20Voice%20Conversion,660,5.0,https://ieeexplore.ieee.org/abstract/document/9746812/,,,0,0,0,0,0,0,0,0,,[0],0,,0.0,
583,583,239.0,239.0,239.0,239.0,239.0,“style” transfer for musical audio using multiple time-frequency representations,https://scholar.google.com/scholar?q=%E2%80%9Cstyle%E2%80%9D%20transfer%20for%20musical%20audio%20using%20multiple%20time-frequency%20representations,239,9.0,https://openreview.net/pdf?id=BybQ7zWCb,https://github.com/anonymousiclr2018/Style-Transfer-for-Musical-Audio,anonymousiclr2018/Style-Transfer-for-Musical-Audio,32,7,0,5,0,7,1,2193,['Python'],[1.0],80920,Python,80920.0,Application
584,584,308.0,308.0,308.0,308.0,308.0,"Bridging audio analysis, perception and synthesis
with perceptually-regularized variational timbre spaces",https://scholar.google.com/scholar?q=Bridging%20audio%20analysis%2C%20perception%20and%20synthesis%0Awith%20perceptually-regularized%20variational%20timbre%20spaces,308,40.0,https://ismir2018.ismir.net/doc/pdfs/219_Paper.pdf,https://github.com/acids-ircam/ismir2018,acids-ircam/ismir2018,0,0,0,1,0,0,1,1964,[],[],0,0,0.0,
585,585,558.0,558.0,558.0,558.0,558.0,"Bridging audio analysis, perception and synthesis with perceptually-regularized variational timbre spaces",https://scholar.google.com/scholar?q=Bridging%20audio%20analysis%2C%20perception%20and%20synthesis%20with%20perceptually-regularized%20variational%20timbre%20spaces,558,40.0,https://ismir2018.ismir.net/doc/pdfs/219_Paper.pdf,https://github.com/acids-ircam/ismir2018,acids-ircam/ismir2018,0,0,0,1,0,0,1,1965,[],[],0,0,0.0,
586,586,261.0,261.0,261.0,261.0,261.0,"Counterpoint by
convolution",https://scholar.google.com/scholar?q=Counterpoint%20by%0Aconvolution,261,171.0,https://arxiv.org/pdf/1903.07227,"https://github.com/czhuang/coconet,https://github.com/czhuang/JSB-Chorales-dataset",czhuang/coconet,10,1,0,3,1,1,2,2298,[],[],0,0,0.0,
587,587,536.0,536.0,536.0,536.0,536.0,DDX7: Differentiable FM Synthesis of Musical  Instrument Sounds,https://scholar.google.com/scholar?q=DDX7%3A%20Differentiable%20FM%20Synthesis%20of%20Musical%20%20Instrument%20Sounds,536,17.0,https://arxiv.org/pdf/2208.06169,https://github.com/andreasjansson/fmsynth,andreasjansson/fmsynth,39,0,0,1,0,0,3,678,['Python'],[1.0],8541,Python,8541.0,Tool
588,588,253.0,253.0,253.0,253.0,253.0,Latent normalizing flows for discrete sequences,https://scholar.google.com/scholar?q=Latent%20normalizing%20flows%20for%20discrete%20sequences,253,115.0,http://proceedings.mlr.press/v97/ziegler19a/ziegler19a.pdf,https://github.com/harvardnlp/TextFlow,harvardnlp/TextFlow,113,15,0,11,4,15,4,1719,['Python'],[1.0],74775,Python,74775.0,Dataset
589,589,166.0,166.0,166.0,166.0,166.0,Seqgan: Sequence generative adversarial nets with policy gradient,https://scholar.google.com/scholar?q=Seqgan%3A%20Sequence%20generative%20adversarial%20nets%20with%20policy%20gradient,166,2532.0,https://ojs.aaai.org/index.php/AAAI/article/view/10804/10663,https://github.com/samim23/obama-rnn,samim23/obama-rnn,179,35,0,13,1,35,5,3068,[],[],0,0,0.0,Model
590,590,103.0,103.0,103.0,103.0,103.0,Music style transfer issues: A position paper,https://scholar.google.com/scholar?q=Music%20style%20transfer%20issues%3A%20A%20position%20paper,103,60.0,https://arxiv.org/pdf/1803.06841,https://github.com/821760408-sp/the-wavenet-pianist,821760408-sp/the-wavenet-pianist,11,3,0,2,2,3,5,2369,['Python'],[1.0],71098,Python,71098.0,Model
591,591,430.0,430.0,430.0,430.0,430.0,"Mtm dataset for joint representation learning among sheet music, lyrics, and musical audio",https://scholar.google.com/scholar?q=Mtm%20dataset%20for%20joint%20representation%20learning%20among%20sheet%20music%2C%20lyrics%2C%20and%20musical%20audio,430,1.0,https://ask.qcloudimg.com/draft/8026517/tfdfuv6u0c.pdf,https://github.com/MorningBooks/MTM-Dataset,MorningBooks/MTM-Dataset,7,0,0,1,1,0,5,1172,[],[],0,0,0.0,
592,592,493.0,493.0,493.0,493.0,493.0,"Neural synthesis of binaural speech,",https://scholar.google.com/scholar?q=Neural%20synthesis%20of%20binaural%20speech%2C,493,37.0,https://openreview.net/pdf?id=uAX8q61EVRu,https://github.com/facebookresearch/BinauralSpeechSynthesis,facebookresearch/BinauralSpeechSynthesis,154,18,1302,20,0,18,8,964,['Python'],[1.0],45887,Python,45887.0,Tool
593,593,601.0,601.0,601.0,601.0,601.0,Differentiable Time–frequency Scattering on GPU,https://scholar.google.com/scholar?q=Differentiable%20Time%E2%80%93frequency%20Scattering%20on%20GPU,601,7.0,https://arxiv.org/pdf/2204.08269,"https://github.com/EtienneTho/strf-like-model,https://github.com/lostanlen/scattering,https://github.com/cyrusvahidi/jtfs-gpu",EtienneTho/strf-like-model,4,1,0,2,0,1,8,986,['Python'],[1.0],86589,Python,86589.0,Tool
594,594,363.0,363.0,363.0,363.0,363.0,Unsupervised image-to-image translation networks,https://scholar.google.com/scholar?q=Unsupervised%20image-to-image%20translation%20networks,363,2846.0,https://proceedings.neurips.cc/paper/2017/file/dc6a6489640ca02b0d42dabeb8e46bb7-Paper.pdf,"https://github.com/mingyuliutw/unit,https://github.com/mingyuliutw/unit",mingyuliutw/unit,1952,364,0,59,4,364,9,2428,"['Python', 'Shell', 'Dockerfile']","[0.9569203975002561, 0.03251459891404569, 0.010565003585698186]",78088,Python,74724.0,Model
595,595,586.0,586.0,586.0,586.0,586.0,MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis,https://scholar.google.com/scholar?q=MelGAN%3A%20Generative%20Adversarial%20Networks%20for%20Conditional%20Waveform%20Synthesis,586,821.0,https://proceedings.neurips.cc/paper/2019/file/6804c9bca0a615bdb9374d00a9fcba59-Paper.pdf,"https://github.com/descriptinc/melgan-neurips,https://github.com/r9y9/wavenet_",descriptinc/melgan-neurips,890,210,0,60,31,210,9,1464,"['Python', 'Shell']","[0.986800293326815, 0.013199706673185041]",20455,Python,20185.0,Dataset
596,596,304.0,304.0,304.0,304.0,304.0,"Sing: Symbol-to-instrument
neural generator",https://scholar.google.com/scholar?q=Sing%3A%20Symbol-to-instrument%0Aneural%20generator,304,70.0,https://proceedings.neurips.cc/paper/2018/file/56dc0997d871e9177069bb472574eb29-Paper.pdf,https://github.com/facebookresearch/SING,facebookresearch/SING,156,25,0,10,1,25,9,1875,['Python'],[1.0],64779,Python,64779.0,Tool
597,597,547.0,547.0,547.0,547.0,547.0,SING: Symbol-to-Instrument Neural Generator,https://scholar.google.com/scholar?q=SING%3A%20Symbol-to-Instrument%20Neural%20Generator,547,70.0,https://proceedings.neurips.cc/paper/2018/file/56dc0997d871e9177069bb472574eb29-Paper.pdf,https://github.com/facebookresearch/SING,facebookresearch/SING,156,25,0,10,1,25,9,1875,['Python'],[1.0],64779,Python,64779.0,Tool
598,598,648.0,648.0,648.0,648.0,648.0,Autovocoder:  Fast Waveform Generation from a Learned Speech Representation Using Differentiable Digital Signal Processing,https://scholar.google.com/scholar?q=Autovocoder%3A%20%20Fast%20Waveform%20Generation%20from%20a%20Learned%20Speech%20Representation%20Using%20Differentiable%20Digital%20Signal%20Processing,648,5.0,https://arxiv.org/pdf/2211.06989,"https://github.com/jik876/hifi-gan,https://github.com/CSTR-Edinburgh/qualtreats,https://github.com/fatchord/WaveRNN,https://github.com/xiph/LPCNetTable",jik876/hifi-gan,1515,431,0,32,93,431,11,1110,['Python'],[1.0],35239,Python,35239.0,Model
599,599,407.0,407.0,407.0,407.0,407.0,Computer-generated music for tabletop role-playing games,https://scholar.google.com/scholar?q=Computer-generated%20music%20for%20tabletop%20role-playing%20games,407,30.0,https://ojs.aaai.org/index.php/AIIDE/article/download/7408/7336,"https://github.com/lucasnfe/adl-piano-midi,https://github.com/lucasnfe/bardo-composerare",lucasnfe/adl-piano-midi,42,4,0,5,1,4,11,1408,['Python'],[1.0],39338,Python,39338.0,Dataset
600,600,167.0,167.0,167.0,167.0,167.0,"Music transformer: Generating music with long-term
structure",https://scholar.google.com/scholar?q=Music%20transformer%3A%20Generating%20music%20with%20long-term%0Astructure,167,691.0,https://arxiv.org/pdf/1809.04281,https://github.com/czhuang/JSB-Chorales-dataset,czhuang/JSB-Chorales-dataset,93,16,0,5,3,16,13,2368,[],[],0,0,0.0,Dataset
601,601,391.0,391.0,391.0,391.0,391.0,Improvising with mandi the ai drummer,https://scholar.google.com/scholar?q=Improvising%20with%20mandi%20the%20ai%20drummer,391,0.0,https://nips2018creativity.github.io/doc/Improvising%20With%20MANDI.pdf,https://github.com/locuslab/TCN,locuslab/TCN,3820,860,0,92,16,860,17,2066,['Python'],[1.0],65757,Python,65757.0,Dataset
602,602,328.0,328.0,328.0,328.0,328.0,"Efficiently trainable text-to-speech system based on
deep convolutional networks with guided attention",https://scholar.google.com/scholar?q=Efficiently%20trainable%20text-to-speech%20system%20based%20on%0Adeep%20convolutional%20networks%20with%20guided%20attention,328,345.0,https://arxiv.org/pdf/1710.08969,https://github.com/tachi-hi/tts_samples,tachi-hi/tts_samples,16,0,0,2,1,0,17,2199,[],[],0,0,0.0,
603,603,224.0,224.0,224.0,224.0,224.0,Text-based lstm networks for automatic music composition,https://scholar.google.com/scholar?q=Text-based%20lstm%20networks%20for%20automatic%20music%20composition,224,133.0,https://arxiv.org/pdf/1604.05358,"https://github.com/keunwoochoi/lstm_real_book,https://github.com/keunwoochoi/LSTMetallica,https://github.com/fchollet/keras",keunwoochoi/lstm_real_book,127,18,0,7,2,18,18,2809,['Python'],[1.0],4119,Python,4119.0,Model
604,604,260.0,260.0,260.0,260.0,260.0,"Empirical evaluation of gated recurrent
neural networks on sequence modeling",https://scholar.google.com/scholar?q=Empirical%20evaluation%20of%20gated%20recurrent%0Aneural%20networks%20on%20sequence%20modeling,260,14033.0,https://arxiv.org/pdf/1412.3555),https://github.com/jych/librnn,jych/librnn,87,22,0,8,0,22,18,3235,['Python'],[1.0],23293,Python,23293.0,Library
605,605,503.0,503.0,503.0,503.0,503.0,"Empirical evaluation of gated recurrent neural networks on sequence modeling,",https://scholar.google.com/scholar?q=Empirical%20evaluation%20of%20gated%20recurrent%20neural%20networks%20on%20sequence%20modeling%2C,503,14033.0,https://arxiv.org/pdf/1412.3555),https://github.com/jych/librnn,jych/librnn,87,22,0,8,0,22,18,3235,['Python'],[1.0],23293,Python,23293.0,Library
606,606,562.0,562.0,562.0,562.0,562.0,HyperNetworks,https://scholar.google.com/scholar?q=HyperNetworks,562,289.0,https://arxiv.org/pdf/1906.00695,"https://github.com/chrhenning/hypercl,https://github.com/joansj/hat",chrhenning/hypercl,144,16,0,6,0,16,20,1610,['Python'],[1.0],1006514,Python,1006514.0,Library
607,607,339.0,339.0,339.0,339.0,339.0,"Generative modelling for controllable audio synthesis of
expressive piano performance",https://scholar.google.com/scholar?q=Generative%20modelling%20for%20controllable%20audio%20synthesis%20of%0Aexpressive%20piano%20performance,339,5.0,https://arxiv.org/pdf/2006.09833,https://github.com/gudgud96/piano-synthesis,gudgud96/piano-synthesis,28,6,0,2,0,6,21,1301,"['Jupyter Notebook', 'Python']","[0.9733835915428811, 0.026616408457118858]",1438173,Jupyter Notebook,1399894.0,Tool
608,608,434.0,434.0,434.0,434.0,434.0,"Deep recurrent music writer: Memory-enhanced variational
autoencoder-based musical score composition and an objective measure",https://scholar.google.com/scholar?q=Deep%20recurrent%20music%20writer%3A%20Memory-enhanced%20variational%0Aautoencoder-based%20musical%20score%20composition%20and%20an%20objective%20measure,434,24.0,https://opus.bibliothek.uni-augsburg.de/opus4/files/71908/71908.pdf,https://github.com/david-gpu/srez,david-gpu/srez,5289,696,0,192,5,696,22,2619,['Python'],[1.0],31731,Python,31731.0,Model
609,609,256.0,256.0,256.0,256.0,256.0,C-rnn-gan: Continuous recurrent neural networks with adversarial training,https://scholar.google.com/scholar?q=C-rnn-gan%3A%20Continuous%20recurrent%20neural%20networks%20with%20adversarial%20training,256,560.0,https://arxiv.org/pdf/1611.09904,"https://github.com/olofmogren/c-rnn-gan,https://github.com/olofmogren/c-rnn-gan",olofmogren/c-rnn-gan,438,162,0,19,6,162,22,2546,"['Python', 'Shell']","[0.9703731596189329, 0.029626840381067138]",132785,Python,128851.0,Model
610,610,218.0,218.0,218.0,218.0,218.0,"Application of deep neural networks to music
composition based on midi datasets and graphical representation",https://scholar.google.com/scholar?q=Application%20of%20deep%20neural%20networks%20to%20music%0Acomposition%20based%20on%20midi%20datasets%20and%20graphical%20representation,218,7.0,https://www.researchgate.net/profile/Mateusz-Dorobek/publication/333392458_Application_of_Deep_Neural_Networks_to_Music_Composition_Based_on_MIDI_Datasets_and_Graphical_Representation/links/5dcb119792851c818049e578/Application-of-Deep-Neural-Networks-to-Music-Composition-Based-on-MIDI-Datasets-and-Graphical-Representation.pdf,https://github.com/jisungk/deepjazz,jisungk/deepjazz,2859,445,0,111,12,445,23,2766,['Python'],[1.0],32454,Python,32454.0,Model
611,611,429.0,429.0,429.0,429.0,429.0,"Asap: a dataset
of aligned scores and performances for piano transcription",https://scholar.google.com/scholar?q=Asap%3A%20a%20dataset%0Aof%20aligned%20scores%20and%20performances%20for%20piano%20transcription,429,47.0,https://infoscience.epfl.ch/record/282307/files/ismir-asap.pdf,"https://github.com/fosfrancesco/asap-dataset,https://github.com/fosfrancesco/asap-dataset",fosfrancesco/asap-dataset,127,13,0,3,4,13,26,1203,"['Jupyter Notebook', 'Python', 'Shell']","[0.7250986582478295, 0.25787574698387644, 0.01702559476829406]",88690,Jupyter Notebook,64309.0,Dataset
612,612,184.0,184.0,184.0,184.0,184.0,Samplernn: An unconditional end-to-end neural audio generation model,https://scholar.google.com/scholar?q=Samplernn%3A%20An%20unconditional%20end-to-end%20neural%20audio%20generation%20model,184,629.0,https://arxiv.org/pdf/1612.07837.pdf),https://github.com/soroushmehr/sampleRNN_ICLR2017,soroushmehr/sampleRNN_ICLR2017,532,143,0,28,12,143,27,2521,"['Python', 'Shell']","[0.9942820940957924, 0.005717905904207627]",186257,Python,185192.0,Model
613,613,324.0,324.0,324.0,324.0,324.0,"Wgansing: A multi-voice singing voice synthesizer
based on the wasserstein-gan",https://scholar.google.com/scholar?q=Wgansing%3A%20A%20multi-voice%20singing%20voice%20synthesizer%0Abased%20on%20the%20wasserstein-gan,324,65.0,https://arxiv.org/pdf/1903.10729,https://github.com/MTG/WGANSing,MTG/WGANSing,237,44,0,14,22,44,29,1595,['Python'],[1.0],61526,Python,61526.0,Dataset
614,614,450.0,450.0,450.0,450.0,450.0,"Singing synthesis: with a little help from
my attention",https://scholar.google.com/scholar?q=Singing%20synthesis%3A%20with%20a%20little%20help%20from%0Amy%20attention,450,21.0,https://arxiv.org/pdf/1912.05881,https://github.com/MTG/WGANSing,MTG/WGANSing,237,44,0,14,22,44,29,1595,['Python'],[1.0],61526,Python,61526.0,Dataset
615,615,160.0,160.0,160.0,160.0,160.0,Interactive music generation with positional constraints using anticipation-rnns,https://scholar.google.com/scholar?q=Interactive%20music%20generation%20with%20positional%20constraints%20using%20anticipation-rnns,160,33.0,https://arxiv.org/pdf/1709.06404,https://github.com/pytorch/pytorch,pytorch/pytorch,72060,19802,26123,1681,13044,19802,30,2633,"['Python', 'C++', 'Cuda', 'C', 'Objective-C++', 'CMake', 'Starlark', 'Assembly', 'Shell', 'Jupyter Notebook', 'GLSL', 'Java', 'PureBasic', 'JavaScript', 'Metal', 'Objective-C', 'Dockerfile', 'Batchfile', 'Makefile', 'Ruby', 'HTML', 'Yacc', 'CSS', 'LLVM', 'PowerShell', 'GDB', 'Smarty', 'Vim Script']","[0.4791905323402042, 0.42207869971929585, 0.038601908071810506, 0.021483649100592837, 0.013857815902453052, 0.007882628257510741, 0.0034093949929081494, 0.0030242473609699543, 0.0029489801536172144, 0.0016496205309016662, 0.0015828232446248168, 0.0011909410363746874, 0.0010350477331753313, 0.0006857578987431365, 0.0003844283180463996, 0.0002764413200406164, 0.0002452230971366964, 0.00017937890176647576, 7.934931368867913e-05, 7.114014389856422e-05, 5.298645955437803e-05, 3.459899819535834e-05, 2.1660339566688728e-05, 1.4431234954145044e-05, 7.678675794915804e-06, 5.871399641779884e-06, 3.38077529143834e-06, 1.3846792417061288e-06]",111217093,Python,53294178.0,Library
616,616,201.0,201.0,201.0,201.0,201.0,"Learning a latent space of
multitrack measures",https://scholar.google.com/scholar?q=Learning%20a%20latent%20space%20of%0Amultitrack%20measures,201,60.0,https://arxiv.org/pdf/1806.00195,https://github.com/tensorflow/magenta,tensorflow/magenta,18708,3756,0,766,397,3756,30,2732,"['Python', 'Shell']","[0.9916692724514968, 0.008330727548503173]",2740697,Python,2717865.0,Plugin
617,617,570.0,570.0,570.0,570.0,570.0,Image-to-Image Translation with Conditional  Adversarial Networks,https://scholar.google.com/scholar?q=Image-to-Image%20Translation%20with%20Conditional%20%20Adversarial%20Networks,570,20127.0,https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf,https://github.com/phillipi/pix2pix,phillipi/pix2pix,9709,1694,0,324,88,1694,30,2537,"['Lua', 'Python', 'MATLAB', 'TeX', 'Shell']","[0.7264845685211221, 0.1790185445537926, 0.06281939702305238, 0.02190310414853496, 0.009774385753498024]",89622,Lua,65109.00000000001,Model
618,618,195.0,195.0,195.0,195.0,195.0,"Jukebox: A
generative model for music",https://scholar.google.com/scholar?q=Jukebox%3A%20A%0Agenerative%20model%20for%20music,195,491.0,https://arxiv.org/pdf/2005.00341,https://github.com/openai/jukebox,openai/jukebox,7223,1309,0,290,186,1309,30,1278,"['Python', 'Cuda', 'Jupyter Notebook', 'C++', 'Shell', 'CSS', 'Makefile', 'HTML', 'Dockerfile']","[0.8580945210570957, 0.08683793976706326, 0.027220137599537002, 0.016418894507888016, 0.006555081834556527, 0.00198402556187344, 0.0013480977180240492, 0.0008828480556982687, 0.0006584538982636744]",1154219,Python,990429.0,Library
619,619,316.0,316.0,316.0,316.0,316.0,Ddsp: Differentiable digital signal processing,https://scholar.google.com/scholar?q=Ddsp%3A%20Differentiable%20digital%20signal%20processing,316,353.0,https://arxiv.org/pdf/2001.04643,"https://github.com/magenta/ddsp,https://github.com/magenta/ddsp",magenta/ddsp,2633,309,0,67,41,309,30,1384,"['Python', 'Jupyter Notebook', 'Dockerfile', 'Shell']","[0.7724831680977098, 0.22612555602046686, 0.0010454740884952897, 0.0003458017933281002]",867549,Python,670167.0,Library
620,620,482.0,482.0,482.0,482.0,482.0,"DDSP: differentiable digital signal processing,",https://scholar.google.com/scholar?q=DDSP%3A%20differentiable%20digital%20signal%20processing%2C,482,353.0,https://arxiv.org/pdf/2001.04643,"https://github.com/magenta/ddsp,https://github.com/magenta/ddsp",magenta/ddsp,2633,309,0,67,41,309,30,1384,"['Python', 'Jupyter Notebook', 'Dockerfile', 'Shell']","[0.7724831680977098, 0.22612555602046686, 0.0010454740884952897, 0.0003458017933281002]",867549,Python,670167.0,Library
621,621,555.0,555.0,555.0,555.0,555.0,DDSP: Differentiable Digital Signal Processing,https://scholar.google.com/scholar?q=DDSP%3A%20Differentiable%20Digital%20Signal%20Processing,555,353.0,https://arxiv.org/pdf/2001.04643,"https://github.com/magenta/ddsp,https://github.com/magenta/ddsp",magenta/ddsp,2633,309,0,67,41,309,30,1384,"['Python', 'Jupyter Notebook', 'Dockerfile', 'Shell']","[0.7724831680977098, 0.22612555602046686, 0.0010454740884952897, 0.0003458017933281002]",867549,Python,670167.0,Library
622,622,321.0,321.0,321.0,321.0,321.0,"Fastspeech: Fast, robust and
controllable text to speech",https://scholar.google.com/scholar?q=Fastspeech%3A%20Fast%2C%20robust%20and%0Acontrollable%20text%20to%20speech,321,891.0,https://proceedings.neurips.cc/paper_files/paper/2019/file/f63f65b503e22cb970527f23c9ad7db1-Paper.pdf,https://github.com/NVIDIA/waveglow,NVIDIA/waveglow,2166,522,0,79,76,522,30,1816,['Python'],[1.0],51964,Python,51964.0,Dataset
623,623,404.0,404.0,404.0,404.0,404.0,Deep long audio inpainting,https://scholar.google.com/scholar?q=Deep%20long%20audio%20inpainting,404,32.0,https://arxiv.org/pdf/1911.06476,"https://github.com/NVIDIA/waveglow,https://github.com/DmitryUlyanov/deep-image-prior,https://github.com/shepnerd/inpainting",NVIDIA/waveglow,2166,522,0,79,76,522,30,1816,['Python'],[1.0],51964,Python,51964.0,Dataset
624,624,245.0,245.0,245.0,245.0,245.0,Sentigan: Generating sentimental texts via mixture adversarial networks,https://scholar.google.com/scholar?q=Sentigan%3A%20Generating%20sentimental%20texts%20via%20mixture%20adversarial%20networks,245,189.0,https://www.ijcai.org/Proceedings/2018/0618.pdf,https://github.com/LantaoYu/SeqGAN,LantaoYu/SeqGAN,2054,717,0,74,34,717,30,2597,['Python'],[1.0],44472,Python,44472.0,Model
625,625,423.0,423.0,423.0,423.0,423.0,Fma: A dataset for music analysis,https://scholar.google.com/scholar?q=Fma%3A%20A%20dataset%20for%20music%20analysis,423,393.0,https://arxiv.org/pdf/1612.01840,https://github.com/mdeff/fma,mdeff/fma,2007,415,0,56,12,415,30,2522,"['Jupyter Notebook', 'Python', 'Makefile', 'Shell']","[0.7736613414355255, 0.21379332875951518, 0.008469731124832566, 0.00407559868012676]",122436,Jupyter Notebook,94724.0,Dataset
626,626,418.0,418.0,418.0,418.0,418.0,"Giantmidi-piano: A large-scale midi dataset for classical
piano music",https://scholar.google.com/scholar?q=Giantmidi-piano%3A%20A%20large-scale%20midi%20dataset%20for%20classical%0Apiano%20music,418,59.0,https://arxiv.org/pdf/2010.07061,"https://github.com/bytedance/GiantMIDI-Piano,https://github.com/bytedance/piano_transcriptiontributes",bytedance/GiantMIDI-Piano,1504,160,0,25,11,160,30,1138,"['Python', 'Shell']","[0.9548904152042147, 0.04510958479578526]",138751,Python,132492.0,Dataset
627,627,385.0,385.0,385.0,385.0,385.0,"Nonoto: A model-agnostic web interface for interactive music composition by
inpainting",https://scholar.google.com/scholar?q=Nonoto%3A%20A%20model-agnostic%20web%20interface%20for%20interactive%20music%20composition%20by%0Ainpainting,385,24.0,https://arxiv.org/pdf/1907.10380,https://github.com/djipco/webmidi,djipco/webmidi,1426,115,121,41,3,115,30,2936,"['JavaScript', 'SCSS', 'Handlebars', 'CSS']","[0.9746163119957816, 0.01789576137126504, 0.006529722899094759, 0.0009582037338586226]",860986,JavaScript,839131.0,Tool
628,628,538.0,538.0,538.0,538.0,538.0,WaveGrad: Estimating  Gradients for Waveform Generation,https://scholar.google.com/scholar?q=WaveGrad%3A%20Estimating%20%20Gradients%20for%20Waveform%20Generation,538,437.0,https://arxiv.org/pdf/2009.00713,https://github.com/kan-bayashi/ParallelWaveGAN,kan-bayashi/ParallelWaveGAN,1412,321,0,48,35,321,30,1461,"['Jupyter Notebook', 'Python', 'Shell', 'Perl', 'Makefile']","[0.8310610987865826, 0.0795255714267888, 0.07340689702189411, 0.01579353917409461, 0.0002128935906398647]",6571358,Jupyter Notebook,5461200.0,Model
629,629,419.0,419.0,419.0,419.0,419.0,"High-resolution piano transcription with
pedals by regressing onsets and offsets times",https://scholar.google.com/scholar?q=High-resolution%20piano%20transcription%20with%0Apedals%20by%20regressing%20onsets%20and%20offsets%20times,419,68.0,https://arxiv.org/pdf/2010.01815,https://github.com/bytedance/piano_transcription,bytedance/piano_transcription,1366,169,0,25,25,169,30,1158,"['Python', 'Shell']","[0.9852605645118098, 0.014739435488190242]",186710,Python,183958.0,Tool
630,630,312.0,312.0,312.0,312.0,312.0,"Adversarial generation of time-frequency
features with application in audio synthesis",https://scholar.google.com/scholar?q=Adversarial%20generation%20of%20time-frequency%0Afeatures%20with%20application%20in%20audio%20synthesis,312,77.0,http://proceedings.mlr.press/v97/marafioti19a/marafioti19a.pdf,https://github.com/chrisdonahue/wavegan,chrisdonahue/wavegan,1257,275,0,49,50,275,30,2087,"['Python', 'Jupyter Notebook', 'JavaScript', 'HTML', 'CSS', 'Shell']","[0.5406337326164757, 0.22439930493139193, 0.2041845744883674, 0.018003148508271644, 0.012223620348732698, 0.0005556191067605772]",183579,Python,99249.0,Dataset
631,631,508.0,508.0,508.0,508.0,508.0,"Pyroomacoustics: A python package for audio room simulations and array processing algorithms,",https://scholar.google.com/scholar?q=Pyroomacoustics%3A%20A%20python%20package%20for%20audio%20room%20simulations%20and%20array%20processing%20algorithms%2C,508,443.0,https://arxiv.org/pdf/1710.04196,https://github.com/LCAV/pyroomacoustics,LCAV/pyroomacoustics,1211,400,0,42,66,400,30,2866,"['Python', 'C++', 'Cython', 'Dockerfile']","[0.8886238953980337, 0.10813838879350798, 0.0025451310822602194, 0.0006925847261981716]",1061242,Python,943045.0,Library
632,632,595.0,595.0,595.0,595.0,595.0,Comparison of real-time multi-speaker neural vocoders on CPUs,https://scholar.google.com/scholar?q=Comparison%20of%20real-time%20multi-speaker%20neural%20vocoders%20on%20CPUs,595,5.0,https://www.jstage.jst.go.jp/article/ast/43/2/43_E2161/_pdf,"https://github.com/mozilla/LPCNet,https://github.com/patrickltobing/cyclevae-vc-neuralvoco,https://github.com/jik876/hiﬁ-ganAcoust",mozilla/LPCNet,1068,293,0,72,65,293,30,1829,"['C', 'Python', 'M4', 'Makefile', 'Shell', 'Batchfile']","[0.559395215836492, 0.4024550312738572, 0.02439628731768078, 0.007769912808218261, 0.005561657045658492, 0.0004218957180932992]",597304,C,334129.0,Model
633,633,284.0,284.0,284.0,284.0,284.0,Lead sheet generation and arrangement by conditional generative adversarial network,https://scholar.google.com/scholar?q=Lead%20sheet%20generation%20and%20arrangement%20by%20conditional%20generative%20adversarial%20network,284,40.0,https://arxiv.org/pdf/1807.11161,https://github.com/craffel/pretty-midi,craffel/pretty-midi,746,141,0,15,32,141,30,3741,"['Jupyter Notebook', 'Python']","[0.7041650797782247, 0.2958349202217752]",522511,Jupyter Notebook,367934.0,Library
634,634,362.0,362.0,362.0,362.0,362.0,A universal music translation network,https://scholar.google.com/scholar?q=A%20universal%20music%20translation%20network,362,140.0,https://arxiv.org/pdf/1805.07848.pdf?uuid=uj4qVTPB0qZdg1pz0022,https://github.com/NVIDIA/nv-wavenet,NVIDIA/nv-wavenet,727,126,0,47,40,126,30,2047,"['Cuda', 'Python', 'C++', 'Makefile', 'C']","[0.6236513799408857, 0.22368949201470964, 0.11884074423960331, 0.021764426839008925, 0.012053956965792386]",246641,Cuda,153818.0,Model
635,635,530.0,530.0,530.0,530.0,530.0,DawDreamer: Bridging the Gap Between Digital Audio Workstations and Python  Interfaces,https://scholar.google.com/scholar?q=DawDreamer%3A%20Bridging%20the%20Gap%20Between%20Digital%20Audio%20Workstations%20and%20Python%20%20Interfaces,530,6.0,https://arxiv.org/pdf/2111.09931,"https://github.com/DBraun/DawDreamer,https://github.com/bmcfee/pyrubberband",DBraun/DawDreamer,722,56,0,30,41,56,30,1182,"['C++', 'C', 'Objective-C++', 'Python', 'Java', 'Objective-C', 'Makefile', 'Faust', 'CMake', 'Shell', 'Dockerfile']","[0.7100861304579332, 0.2481877153270279, 0.028944594046890964, 0.005683817566510253, 0.004755200717093269, 0.0009367412313016468, 0.0006640092637734589, 0.0004862265940973328, 0.0001306965781445791, 9.438412346008522e-05, 3.0484093767235612e-05]",28309846,C++,20102429.0,Tool
636,636,414.0,414.0,414.0,414.0,414.0,The million song dataset,https://scholar.google.com/scholar?q=The%20million%20song%20dataset,414,1663.0,https://academiccommons.columbia.edu/doi/10.7916/D8377K1H/download,https://github.com/tb2332/MSongsDB,tb2332/MSongsDB,633,386,0,32,4,386,30,4857,"['Python', 'MATLAB', 'Java', 'C++', 'Makefile']","[0.775222354021062, 0.13908110250818048, 0.043084153140538595, 0.040747947530329225, 0.0018644427998896708]",663469,Python,514336.0,Dataset
637,637,165.0,165.0,165.0,165.0,165.0,"Musegan: Multi-track sequential generative
adversarial networks for symbolic music generation and accompaniment",https://scholar.google.com/scholar?q=Musegan%3A%20Multi-track%20sequential%20generative%0Aadversarial%20networks%20for%20symbolic%20music%20generation%20and%20accompaniment,165,539.0,https://ojs.aaai.org/index.php/AAAI/article/download/11312/11171,https://github.com/urinieto/msaf,urinieto/msaf,428,78,190,23,19,78,30,3353,"['Python', 'Jupyter Notebook']","[0.7000352305718468, 0.2999647694281532]",530789,Python,371571.0,Application
638,638,591.0,591.0,591.0,591.0,591.0,Neural Homomorphic Vocoder,https://scholar.google.com/scholar?q=Neural%20Homomorphic%20Vocoder,591,25.0,https://x-lance.sjtu.edu.cn/papers/2020/zjl00-liu-is2020.pdf,https://github.com/google/REAPER,google/REAPER,373,96,0,37,9,96,30,3232,"['C++', 'CMake']","[0.9928283486369281, 0.007171651363071849]",196468,C++,195059.0,Application
639,639,108.0,108.0,108.0,108.0,108.0,"Deep learning techniques for music generation–a
survey",https://scholar.google.com/scholar?q=Deep%20learning%20techniques%20for%20music%20generation%E2%80%93a%0Asurvey,108,367.0,https://arxiv.org/pdf/1709.01620,https://github.com/feynmanliang/bachbot,feynmanliang/bachbot,372,54,0,13,20,54,30,2726,"['Python', 'Lua', 'Shell', 'Dockerfile']","[0.6955760340755395, 0.23934130044209379, 0.03568544724145588, 0.029397218240910836]",198148,Python,137827.0,Model
640,640,205.0,205.0,205.0,205.0,205.0,"Automatic stylistic composition of bach
chorales with deep lstm",https://scholar.google.com/scholar?q=Automatic%20stylistic%20composition%20of%20bach%0Achorales%20with%20deep%20lstm,205,101.0,https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/156_Paper.pdf,https://github.com/feynmanliang/bachbot,feynmanliang/bachbot,372,54,0,13,20,54,30,2726,"['Python', 'Lua', 'Shell', 'Dockerfile']","[0.6955760340755395, 0.23934130044209379, 0.03568544724145588, 0.029397218240910836]",198148,Python,137827.0,Model
641,641,236.0,236.0,236.0,236.0,236.0,Music generation using deep learning,https://scholar.google.com/scholar?q=Music%20generation%20using%20deep%20learning,236,367.0,https://arxiv.org/pdf/1709.01620,https://github.com/feynmanliang/bachbot,feynmanliang/bachbot,372,54,0,13,20,54,30,2726,"['Python', 'Lua', 'Shell', 'Dockerfile']","[0.6955760340755395, 0.23934130044209379, 0.03568544724145588, 0.029397218240910836]",198148,Python,137827.0,Model
642,642,463.0,463.0,463.0,463.0,463.0,"Blind arbitrary reverb matching,",https://scholar.google.com/scholar?q=Blind%20arbitrary%20reverb%20matching%2C,463,11.0,https://www.dafx.de/paper-archive/2020/proceedings/papers/DAFx2020_paper_5.pdf,"https://github.com/fedden/RenderMan,https://github.com/iZotope/max_vst_renderer",fedden/RenderMan,336,44,0,15,22,44,30,2440,"['C++', 'C', 'Jupyter Notebook', 'Objective-C++', 'Java', 'Objective-C', 'Makefile', 'CMake', 'R', 'Ruby']","[0.6103558096042689, 0.2687596802668775, 0.076187581458656, 0.033826011531476886, 0.007003548979893927, 0.0027538892559417433, 0.0004864503341384427, 0.00042326910510107193, 0.00018038240890169363, 2.3377054743827195e-05]",22158480,C++,13524557.000000002,Tool
643,643,651.0,651.0,651.0,651.0,651.0,MIDI-DDSP: Detailed control of musical performance via hierarchical modeling,https://scholar.google.com/scholar?q=MIDI-DDSP%3A%20Detailed%20control%20of%20musical%20performance%20via%20hierarchical%20modeling,651,27.0,https://arxiv.org/pdf/2112.09312,"https://github.com/magenta/midi-ddsp,https://github.com/rodrigo-castellon/midi2params",magenta/midi-ddsp,282,16,0,11,8,16,30,705,"['Python', 'Jupyter Notebook', 'Shell']","[0.8882382150990565, 0.10174055238780158, 0.010021232513141898]",233604,Python,207496.0,Dataset
644,644,640.0,640.0,640.0,640.0,640.0,One Billion Audio Sounds from  GPU-enabled Modular Synthesis,https://scholar.google.com/scholar?q=One%20Billion%20Audio%20Sounds%20from%20%20GPU-enabled%20Modular%20Synthesis,640,17.0,https://arxiv.org/pdf/2104.12922,https://github.com/torchsynth/torchsynth,torchsynth/torchsynth,272,10,0,13,118,10,30,1044,['Python'],[1.0],140216,Python,140216.0,Dataset
645,645,526.0,526.0,526.0,526.0,526.0,NoiseBandNet: Controllable Time-Varying Neural Synthesis  of Sound Effects Using Filterbanks,https://scholar.google.com/scholar?q=NoiseBandNet%3A%20Controllable%20Time-Varying%20Neural%20Synthesis%20%20of%20Sound%20Effects%20Using%20Filterbanks,526,1.0,https://arxiv.org/pdf/2307.08007,"https://github.com/YatingMusic/ddsp-singing-vocoders,https://github.com/gudgud96/frechet-audio-distance",YatingMusic/ddsp-singing-vocoders,230,33,0,9,4,33,30,458,"['Python', 'Jupyter Notebook']","[0.9417240873772175, 0.05827591262278258]",95494,Python,89929.0,Model
646,646,504.0,504.0,504.0,504.0,504.0,"Layer normalization,",https://scholar.google.com/scholar?q=Layer%20normalization%2C,504,9245.0,https://arxiv.org/pdf/1607.06450.pdf%EF%BC%89%E4%B8%AD%EF%BC%8C%E4%BD%9C%E8%80%85%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E7%A7%8D%E7%B1%BB%E4%BC%BC%E4%B8%8E,"https://github.com/ivendrov/order-embedding,https://github.com/ryankiros/skip-thoughts,https://github.com/NVIDIA/cnmem",ivendrov/order-embedding,182,64,0,16,2,64,30,2928,"['JavaScript', 'Python', 'HTML', 'CSS']","[0.637074769523871, 0.3067633240227974, 0.053925763275586486, 0.002236143177745128]",123874,JavaScript,78917.0,Model
647,647,258.0,258.0,258.0,258.0,258.0,"The jazz transformer on the front line: Exploring the shortcomings of ai-composed
music through quantitative measures",https://scholar.google.com/scholar?q=The%20jazz%20transformer%20on%20the%20front%20line%3A%20Exploring%20the%20shortcomings%20of%20ai-composed%0Amusic%20through%20quantitative%20measures,258,69.0,https://arxiv.org/pdf/2008.01307,https://github.com/slSeanWU/jazz_transformer,slSeanWU/jazz_transformer,113,15,0,3,3,15,30,1194,"['Python', 'Shell']","[0.9935421975097228, 0.006457802490277191]",96937,Python,96311.0,Model
648,648,340.0,340.0,340.0,340.0,340.0,"Performancenet: Score-to-audio music generation with multi-band convolutional
residual network",https://scholar.google.com/scholar?q=Performancenet%3A%20Score-to-audio%20music%20generation%20with%20multi-band%20convolutional%0Aresidual%20network,340,29.0,https://ojs.aaai.org/index.php/AAAI/article/download/3911/3789,"https://github.com/bwang514/PerformanceNet,https://github.com/salu133445/pypianoroll",bwang514/PerformanceNet,105,11,0,4,3,11,30,1814,"['Python', 'Shell']","[0.964695033592412, 0.035304966407587936]",22773,Python,21969.0,Model
649,649,479.0,479.0,479.0,479.0,479.0,"Automatic multitrack mixing with a differentiable mixing console of neural audio effects,",https://scholar.google.com/scholar?q=Automatic%20multitrack%20mixing%20with%20a%20differentiable%20mixing%20console%20of%20neural%20audio%20effects%2C,479,33.0,https://arxiv.org/pdf/2010.10291,https://github.com/csteinmetz1/pymixconsole,csteinmetz1/pymixconsole,101,12,0,4,13,12,30,1517,['Python'],[1.0],79463,Python,79463.0,Library
650,650,634.0,634.0,634.0,634.0,634.0,Automatic Multitrack Mixing With A Differentiable Mixing Console Of Neural Audio Effects,https://scholar.google.com/scholar?q=Automatic%20Multitrack%20Mixing%20With%20A%20Differentiable%20Mixing%20Console%20Of%20Neural%20Audio%20Effects,634,33.0,https://arxiv.org/pdf/2010.10291,https://github.com/csteinmetz1/pymixconsole,csteinmetz1/pymixconsole,101,12,0,4,13,12,30,1517,['Python'],[1.0],79463,Python,79463.0,Library
651,651,314.0,314.0,314.0,314.0,314.0,"Drumgan: Synthesis of drum sounds with timbral feature conditioning using
generative adversarial networks",https://scholar.google.com/scholar?q=Drumgan%3A%20Synthesis%20of%20drum%20sounds%20with%20timbral%20feature%20conditioning%20using%0Agenerative%20adversarial%20networks,314,58.0,https://arxiv.org/pdf/2008.12073,https://github.com/SonyCSLParis/DrumGAN,SonyCSLParis/DrumGAN,87,12,0,4,1,12,30,1200,"['Python', 'Shell']","[0.9940180012148059, 0.0059819987851940926]",325978,Python,324028.0,Model
652,652,597.0,597.0,597.0,597.0,597.0,Hierarchical Timbre-painting and Articulation Generation,https://scholar.google.com/scholar?q=Hierarchical%20Timbre-painting%20and%20Articulation%20Generation,597,9.0,https://arxiv.org/pdf/2008.13095,"https://github.com/mosheman5/timbre_painting,https://github.com/magenta/ddspfor",mosheman5/timbre_painting,77,8,0,2,0,8,30,1182,"['Python', 'HTML', 'Jupyter Notebook']","[0.81423494143462, 0.10043576351330469, 0.08532929505207529]",161785,Python,131731.0,Dataset
653,653,211.0,211.0,211.0,211.0,211.0,"Music sketchnet: Controllable music generation
via factorized representations of pitch and rhythm",https://scholar.google.com/scholar?q=Music%20sketchnet%3A%20Controllable%20music%20generation%0Avia%20factorized%20representations%20of%20pitch%20and%20rhythm,211,39.0,https://arxiv.org/pdf/2008.01291,https://github.com/RetroCirce/Music-SketchNet,RetroCirce/Music-SketchNet,75,10,0,3,5,10,30,1192,"['Jupyter Notebook', 'Python']","[0.990879340755171, 0.009120659244828917]",11937295,Jupyter Notebook,11828419.0,Model
654,654,264.0,264.0,264.0,264.0,264.0,"Vector quantized contrastive predictive coding for template-based music
generation",https://scholar.google.com/scholar?q=Vector%20quantized%20contrastive%20predictive%20coding%20for%20template-based%20music%0Ageneration,264,15.0,https://arxiv.org/pdf/2004.10120,https://github.com/SonyCSLParis/vqcpc-bach,SonyCSLParis/vqcpc-bach,75,13,0,8,5,13,30,1315,['Python'],[1.0],335369,Python,335369.0,Model
655,655,543.0,543.0,543.0,543.0,543.0,Direct design of biquad filter cascades with deep learning by sampling random polynomials,https://scholar.google.com/scholar?q=Direct%20design%20of%20biquad%20filter%20cascades%20with%20deep%20learning%20by%20sampling%20random%20polynomials,543,12.0,https://arxiv.org/pdf/2110.03691,https://github.com/csteinmetz1/IIRNet,csteinmetz1/IIRNet,63,10,0,4,1,10,30,1088,"['Python', 'Shell']","[0.9705448920376015, 0.029455107962398513]",109251,Python,106033.0,Model
656,656,383.0,383.0,383.0,383.0,383.0,Performing structured improvisations with pre-trained deep learning models,https://scholar.google.com/scholar?q=Performing%20structured%20improvisations%20with%20pre-trained%20deep%20learning%20models,383,14.0,https://arxiv.org/pdf/1904.13285,https://github.com/psc-g/Psc2,psc-g/Psc2,26,4,0,8,0,4,30,1880,"['Python', 'SuperCollider', 'HTML', 'CSS']","[0.779259213872124, 0.18194295517906928, 0.028902300787353327, 0.009895530161453387]",163205,Python,127179.0,Tool
657,657,431.0,431.0,431.0,431.0,431.0,dmelodies: A music dataset for disentanglement learning,https://scholar.google.com/scholar?q=dmelodies%3A%20A%20music%20dataset%20for%20disentanglement%20learning,431,13.0,https://arxiv.org/pdf/2007.15067,"https://github.com/ashispati/dmelodies_dataset,https://github.com/ashispati/dmelodies_benchmarkinglearning,https://github.com/deepmind/3d-shapes,https://github.com/deepmind/dsprites-",ashispati/dmelodies_dataset,23,5,0,4,0,5,30,1259,"['Python', 'Jupyter Notebook']","[0.5034703267566092, 0.49652967324339076]",76938,Python,38736.0,Dataset
658,658,221.0,221.0,221.0,221.0,221.0,Chordal: A chord-based approach for music generation using bi-lstms,https://scholar.google.com/scholar?q=Chordal%3A%20A%20chord-based%20approach%20for%20music%20generation%20using%20bi-lstms,221,15.0,https://www.computationalcreativity.net/iccc2019/assets/creative-submissions/iccc19-tan-chordal.pdf,https://github.com/gudgud96/ChordAL,gudgud96/ChordAL,17,2,0,1,0,2,30,1905,"['CSS', 'Python', 'JavaScript', 'SCSS', 'Less', 'HTML']","[0.31344660502316923, 0.23020249254141892, 0.1890043235084601, 0.11212768808672408, 0.11070579688679179, 0.04451309395343588]",708915,CSS,222207.00000000003,Tool
659,659,537.0,537.0,537.0,537.0,537.0,Towards realistic MIDI instrument synthesizers,https://scholar.google.com/scholar?q=Towards%20realistic%20MIDI%20instrument%20synthesizers,537,10.0,https://www-cs.stanford.edu/~rjcaste/research/realistic_midi.pdf,"https://github.com/rodrigo-castellon/midi2params,https://github.com/craffel/align_midi",rodrigo-castellon/midi2params,16,3,0,2,0,3,30,1117,"['Python', 'Jupyter Notebook', 'Shell']","[0.5887691906987512, 0.4033193940007814, 0.007911415300467378]",276436,Python,162757.0,Dataset
660,660,527.0,527.0,527.0,527.0,527.0,InverSynth: Deep Estimation of Synthesizer  Parameter Configurations from Audio Signals,https://scholar.google.com/scholar?q=InverSynth%3A%20Deep%20Estimation%20of%20Synthesizer%20%20Parameter%20Configurations%20from%20Audio%20Signals,527,19.0,https://arxiv.org/pdf/1812.06349,https://github.com/deepsynth/deepsynth,deepsynth/deepsynth,3,0,0,0,0,0,30,2038,[],[],0,0,0.0,
661,661,,,,,,,,670,,,https://github.com/sdatkinson/NeuralAmpModelerPlugin,sdatkinson/NeuralAmpModelerPlugin,1463,111,149903,51,69,111,30,320,"['Python', 'C++', 'Shell', 'Batchfile', 'Inno Setup', 'C', 'Makefile', 'Rich Text Format', 'Objective-C', 'Dockerfile', 'TeX']","[0.2793908645884368, 0.2659845408202516, 0.2164973499127435, 0.07556291578431651, 0.06600721536362275, 0.052754754568313134, 0.02337502480028181, 0.012770626748672931, 0.005980410814137578, 0.001125629117352909, 0.0005506674818704879]",246973,Python,69002.0,Plugin
662,662,,,,,,,,671,,,https://github.com/DamRsn/NeuralNote,DamRsn/NeuralNote,611,30,40825,26,11,30,30,240,"['C++', 'CMake', 'Python', 'Shell', 'C', 'Batchfile']","[0.9174833335630568, 0.03305720573573532, 0.018713272962008334, 0.012588845547729642, 0.011890486232673109, 0.006266855958796801]",217653,C++,199693.0,Model
663,663,,,,,,,,672,,,https://github.com/GuitarML/NeuralPi,GuitarML/NeuralPi,981,53,7824,31,7,53,30,890,"['C++', 'Shell', 'Inno Setup', 'CMake', 'Batchfile']","[0.7801287255500967, 0.08366323789117754, 0.06426233898844148, 0.058437047514626, 0.013508650055658317]",119479,C++,93209.0,Model
664,664,,,,,,,,673,,,https://github.com/WuYiming6526/HARD,WuYiming6526/HARD,163,6,1532,4,5,6,8,231,"['C++', 'C', 'Objective-C++']","[0.9358622655562742, 0.0534496445987905, 0.010688089844935342]",326251,C++,305326.0,Dataset
665,665,,,,,,,,674,,,https://github.com/Torsion-Audio/Scyclone,Torsion-Audio/Scyclone,259,6,1565,11,2,6,30,219,"['C++', 'CMake', 'C', 'GLSL']","[0.9277355445639417, 0.041189609331960045, 0.02111727226999449, 0.009957573834103811]",1335968,C++,1239425.0,Tool
666,666,,,,,,,,675,,,https://github.com/james34602/SpleeterRT,james34602/SpleeterRT,137,16,898,15,5,16,20,1210,"['C', 'C++', 'Objective-C++', 'R']","[0.9844266782100672, 0.011821979909111519, 0.0034116752123114157, 0.00033966666850981914]",1265947,C,1246232.0,Tool
667,667,,,,,,,,676,,,https://github.com/rodrigodzf/NeuralResonatorVST,rodrigodzf/NeuralResonatorVST,90,0,196,6,8,0,30,249,"['C++', 'TypeScript', 'CMake', 'Python', 'Shell', 'SCSS', 'Jupyter Notebook', 'HTML', 'C']","[0.7780123557201447, 0.11984962015678267, 0.0518196134078599, 0.024322079360409782, 0.009638846107255957, 0.007895374392164327, 0.005187152819838372, 0.0029418381296831466, 0.00033311990586117984]",231148,C++,179836.0,Plugin
668,668,,,,,,,,677,,,https://github.com/csteinmetz1/neural-2a,csteinmetz1/neural-2a,14,3,0,2,0,3,2,545,"['CMake', 'C++', 'C']","[0.6316059336268322, 0.3277285666295373, 0.04066549974363055]",56559,CMake,35723.0,Model
669,669,,,,,,,,678,,,https://github.com/magenta/ddsp-vst,magenta/ddsp-vst,561,56,0,35,25,56,30,537,"['C++', 'CMake', 'Shell']","[0.8994462742851688, 0.07510501694591627, 0.025448708768914984]",167592,C++,150740.0,Plugin
670,670,,,,,,,,679,,,https://github.com/csteinmetz1/steerable-nafx,csteinmetz1/steerable-nafx,194,16,0,8,0,16,30,778,"['Jupyter Notebook', 'Python']","[0.9535704049912916, 0.0464295950087083]",45359,Jupyter Notebook,43253.0,Model
671,671,,,,,,,,680,,,https://github.com/supercollider/supercollider,supercollider/supercollider,4958,711,237222,160,851,711,30,4195,"['C++', 'SuperCollider', 'C', 'Python', 'CMake', 'Objective-C++', 'JavaScript', 'HTML', 'CSS', 'Yacc', 'Shell', 'Scala', 'Lex', 'Makefile', 'NSIS', 'Objective-C', 'XSLT', 'Batchfile', 'Ruby', 'Perl', 'Rich Text Format']","[0.9146499217489178, 0.05054934677383412, 0.023107001110965024, 0.004012645715830417, 0.0037325246452174833, 0.0008464571296269116, 0.0007169713340115771, 0.0006225753525879096, 0.00047705817194176235, 0.0002860357555249231, 0.00016945472699029246, 0.00015684868136979003, 0.00015145178032214998, 0.0001287091198188107, 0.00012267494632274082, 9.64870316450778e-05, 6.496196021181502e-05, 4.0825266227535497e-05, 3.210259958965231e-05, 2.839845348315396e-05, 7.54769556109071e-06]",50214002,C++,45928233.0,Application
672,672,,,,,,,,681,,,https://github.com/fiebrink1/wekinator,fiebrink1/wekinator,275,50,53612,28,9,50,30,3095,"['HTML', 'Java', 'CSS']","[0.9475749390752398, 0.052145249390978476, 0.0002798115337816849]",39808938,HTML,37721952.0,Library
673,673,,,,,,,,682,,,https://github.com/tidalcycles/Tidal,tidalcycles/Tidal,1996,248,399,80,136,248,30,4798,"['C++', 'Haskell', 'XSLT', 'Makefile', 'Perl', 'CMake', 'C', 'HTML', 'Emacs Lisp', 'Python', 'M4', 'Shell', 'TeX', 'Nix', 'SuperCollider']","[0.8573044844888302, 0.10499940004535499, 0.009785977045046718, 0.008416861355617556, 0.007013994689921654, 0.0031232634191454725, 0.0024848168580595357, 0.0019893886479099283, 0.0016812275602570627, 0.0011716894101966102, 0.0006339313803144666, 0.0005343716443035407, 0.00037837214864923287, 0.0002935996296648731, 0.00018862167672818261]",8859003,C++,7594863.000000001,Application
674,674,,,,,,,,683,,,https://github.com/pure-data/pure-data,pure-data/pure-data,1342,224,0,74,504,224,30,3359,"['C', 'C++', 'Tcl', 'PostScript', 'Shell', 'M4', 'Makefile', 'NSIS', 'Gnuplot', 'Emacs Lisp', 'Awk']","[0.7775682212152, 0.09314532719886477, 0.08256733061713922, 0.02037753871138103, 0.008420284922751925, 0.006280354030665896, 0.00625071941918657, 0.0037690546652521077, 0.0015385042506950875, 4.632357689136669e-05, 3.634139197201494e-05]",6411422,C,4985318.0,Application
675,675,,,,,,,,684,,,https://github.com/mimic-sussex/sema,mimic-sussex/sema,143,50,0,11,97,50,30,1723,"['Svelte', 'JavaScript', 'Nearley', 'CSS', 'HCL', 'HTML']","[0.7000317203848079, 0.21485852212744655, 0.04742940975302706, 0.025854591773516452, 0.009326536580046887, 0.0024992193811551175]",807052,Svelte,564962.0,Tool
676,676,,,,,,,,685,,,https://github.com/jatinchowdhury18/RTNeural,jatinchowdhury18/RTNeural,440,42,0,17,18,42,30,1109,"['C++', 'NASL', 'CMake', 'C', 'Python', 'HTML', 'Makefile']","[0.9827203099173379, 0.006791944640169154, 0.004370715437163512, 0.003289217030498231, 0.002482781802406779, 0.00032507466558725205, 1.995650683708722e-05]",8318089,C++,8174355.0,Model
677,677,,,,,,,,686,,,https://github.com/MTG/essentia,MTG/essentia,2534,512,0,109,402,512,30,3800,"['C++', 'Python', 'MATLAB', 'NSIS', 'Shell', 'C', 'Csound Score']","[0.6922035711154534, 0.29730342711131746, 0.003562302877079802, 0.0030898341047607076, 0.0023581186537944706, 0.0012434182957231994, 0.00023932784187099445]",6626893,C++,4587159.0,Library
